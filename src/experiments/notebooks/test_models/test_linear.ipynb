{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4c1c768",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os, sys\n",
    "sys.path.append(\"../../../../\")\n",
    "\n",
    "from src.core.module import Module\n",
    "from src.core.losses import CrossEntropy, MSE, BCE\n",
    "from src.core.optim import AdamW, Standard\n",
    "from src.core.tensor import Tensor\n",
    "from src.utils.backend import xp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a30a4e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../../data.csv\")\n",
    "df['Quality'] = df['Quality'].apply(lambda x: 1 if x == \"Good\" else 0)\n",
    "x = np.array(df.drop('Quality', axis=1).values)\n",
    "y = np.array(df['Quality'].values).reshape((-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f5c4307",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.fc1 = self.linear(input_size, hidden_size, bias=False)\n",
    "        self.fc2 = self.linear(hidden_size, output_size, bias=False)\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "    \n",
    "    def train(self, x: Tensor, y: Tensor, optimizer, num_epochs=100):\n",
    "        for epoch in range(num_epochs):\n",
    "            y_hat = self.forward(x)\n",
    "            \n",
    "            loss = BCE(y_hat, y)\n",
    "            \n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            if epoch % 100 == 0:\n",
    "                print(f\"Epoch {epoch}, Loss: {loss.data}\")          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc7f1ce4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.8684522718552586\n",
      "Epoch 100, Loss: 0.8464026275243832\n",
      "Epoch 200, Loss: 0.8524561470133099\n",
      "Epoch 300, Loss: 0.8743869109518382\n",
      "Epoch 400, Loss: 0.9037543175717707\n",
      "Epoch 500, Loss: 0.9357752855061997\n",
      "Epoch 600, Loss: 0.9680390195527392\n",
      "Epoch 700, Loss: 0.9994278790775899\n",
      "Epoch 800, Loss: 1.0294772122652744\n",
      "Epoch 900, Loss: 1.0580402322558484\n",
      "Epoch 1000, Loss: 1.0851201894589877\n",
      "Epoch 1100, Loss: 1.1107874168153935\n",
      "Epoch 1200, Loss: 1.1351385431189434\n",
      "Epoch 1300, Loss: 1.1582766725773816\n",
      "Epoch 1400, Loss: 1.180302057405773\n",
      "Epoch 1500, Loss: 1.2013080425378992\n",
      "Epoch 1600, Loss: 1.2213796462177566\n",
      "Epoch 1700, Loss: 1.240593430356249\n",
      "Epoch 1800, Loss: 1.2590179690923262\n",
      "Epoch 1900, Loss: 1.2767145610902135\n",
      "Epoch 2000, Loss: 1.293738006703213\n",
      "Epoch 2100, Loss: 1.3101373631768958\n",
      "Epoch 2200, Loss: 1.325956639254902\n",
      "Epoch 2300, Loss: 1.3412354154828094\n",
      "Epoch 2400, Loss: 1.3560093890314553\n",
      "Epoch 2500, Loss: 1.3703108477726438\n",
      "Epoch 2600, Loss: 1.3841690807581415\n",
      "Epoch 2700, Loss: 1.3976107328674217\n",
      "Epoch 2800, Loss: 1.4106601111159685\n",
      "Epoch 2900, Loss: 1.4233394494460827\n",
      "Epoch 3000, Loss: 1.435669138015563\n",
      "Epoch 3100, Loss: 1.4476679221884847\n",
      "Epoch 3200, Loss: 1.459353075679129\n",
      "Epoch 3300, Loss: 1.4707405516299406\n",
      "Epoch 3400, Loss: 1.4818451148230078\n",
      "Epoch 3500, Loss: 1.4926804577277062\n",
      "Epoch 3600, Loss: 1.5032593026667356\n",
      "Epoch 3700, Loss: 1.5135934920289247\n",
      "Epoch 3800, Loss: 1.523694068160486\n",
      "Epoch 3900, Loss: 1.5335713443178518\n",
      "Epoch 4000, Loss: 1.543234967857186\n",
      "Epoch 4100, Loss: 1.552693976661352\n",
      "Epoch 4200, Loss: 1.5619568496589495\n",
      "Epoch 4300, Loss: 1.5710315521671914\n",
      "Epoch 4400, Loss: 1.5799255766869629\n",
      "Epoch 4500, Loss: 1.5886459796911783\n",
      "Epoch 4600, Loss: 1.597199414873704\n",
      "Epoch 4700, Loss: 1.6055921632634944\n",
      "Epoch 4800, Loss: 1.6138301605553655\n",
      "Epoch 4900, Loss: 1.6219190219634272\n",
      "Epoch 5000, Loss: 1.6298640648643432\n",
      "Epoch 5100, Loss: 1.6376703294643344\n",
      "Epoch 5200, Loss: 1.6453425976952085\n",
      "Epoch 5300, Loss: 1.65288541052003\n",
      "Epoch 5400, Loss: 1.6603030838077215\n",
      "Epoch 5500, Loss: 1.6675997229174335\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m net._build(x.shape)\n\u001b[32m      3\u001b[39m optimizer = Standard(net.parameters(), lr=\u001b[32m0.001\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[43mnet\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m50000\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 18\u001b[39m, in \u001b[36mNet.train\u001b[39m\u001b[34m(self, x, y, optimizer, num_epochs)\u001b[39m\n\u001b[32m     14\u001b[39m y_hat = \u001b[38;5;28mself\u001b[39m.forward(x)\n\u001b[32m     16\u001b[39m loss = BCE(y_hat, y)\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m optimizer.step()\n\u001b[32m     21\u001b[39m optimizer.zero_grad()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ml-projects\\transformer\\src\\experiments\\notebooks\\test_models\\../../../..\\src\\core\\tensor.py:385\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, grad, _visited)\u001b[39m\n\u001b[32m    382\u001b[39m \u001b[38;5;28mself\u001b[39m.parents = ()\n\u001b[32m    384\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m parent, g \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(parents_local, grads):\n\u001b[32m--> \u001b[39m\u001b[32m385\u001b[39m     \u001b[43mparent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_visited\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ml-projects\\transformer\\src\\experiments\\notebooks\\test_models\\../../../..\\src\\core\\tensor.py:385\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, grad, _visited)\u001b[39m\n\u001b[32m    382\u001b[39m \u001b[38;5;28mself\u001b[39m.parents = ()\n\u001b[32m    384\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m parent, g \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(parents_local, grads):\n\u001b[32m--> \u001b[39m\u001b[32m385\u001b[39m     \u001b[43mparent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_visited\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[31m[... skipping similar frames: Tensor.backward at line 385 (1 times)]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ml-projects\\transformer\\src\\experiments\\notebooks\\test_models\\../../../..\\src\\core\\tensor.py:385\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, grad, _visited)\u001b[39m\n\u001b[32m    382\u001b[39m \u001b[38;5;28mself\u001b[39m.parents = ()\n\u001b[32m    384\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m parent, g \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(parents_local, grads):\n\u001b[32m--> \u001b[39m\u001b[32m385\u001b[39m     \u001b[43mparent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_visited\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ml-projects\\transformer\\src\\experiments\\notebooks\\test_models\\../../../..\\src\\core\\tensor.py:378\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, grad, _visited)\u001b[39m\n\u001b[32m    374\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.grad_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    375\u001b[39m     \u001b[38;5;66;03m# Save a local copy, then immediately sever the links so the\u001b[39;00m\n\u001b[32m    376\u001b[39m     \u001b[38;5;66;03m# graph from this Tensor backward is eligible for GC.\u001b[39;00m\n\u001b[32m    377\u001b[39m     parents_local = \u001b[38;5;28mself\u001b[39m.parents\n\u001b[32m--> \u001b[39m\u001b[32m378\u001b[39m     grads         = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgrad_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    380\u001b[39m     \u001b[38;5;66;03m# *******  CRUCIAL: break reference cycles  *******\u001b[39;00m\n\u001b[32m    381\u001b[39m     \u001b[38;5;28mself\u001b[39m.grad_fn = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ml-projects\\transformer\\src\\experiments\\notebooks\\test_models\\../../../..\\src\\core\\tensor.py:270\u001b[39m, in \u001b[36mTensor.log.<locals>.grad_fn\u001b[39m\u001b[34m(grad)\u001b[39m\n\u001b[32m    268\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgrad_fn\u001b[39m(grad):\n\u001b[32m    269\u001b[39m     safe_self = xp.maximum(\u001b[38;5;28mself\u001b[39m.data, \u001b[38;5;28mself\u001b[39m.eps)\n\u001b[32m--> \u001b[39m\u001b[32m270\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[43mgrad\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[43mTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43msafe_self\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequires_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m,)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ml-projects\\transformer\\src\\experiments\\notebooks\\test_models\\../../../..\\src\\core\\tensor.py:247\u001b[39m, in \u001b[36mTensor.__truediv__\u001b[39m\u001b[34m(self, other)\u001b[39m\n\u001b[32m    246\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__truediv__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other):\n\u001b[32m--> \u001b[39m\u001b[32m247\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__div__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ml-projects\\transformer\\src\\experiments\\notebooks\\test_models\\../../../..\\src\\core\\tensor.py:238\u001b[39m, in \u001b[36mTensor.__div__\u001b[39m\u001b[34m(self, other)\u001b[39m\n\u001b[32m    235\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, Tensor):\n\u001b[32m    236\u001b[39m     \u001b[38;5;28mself\u001b[39m = Tensor(\u001b[38;5;28mself\u001b[39m, requires_grad=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m238\u001b[39m out = \u001b[43mTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdivide\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequires_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequires_grad\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequires_grad\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m out.requires_grad:\n\u001b[32m    240\u001b[39m     out.parents = (\u001b[38;5;28mself\u001b[39m, other)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ml-projects\\transformer\\src\\experiments\\notebooks\\test_models\\../../../..\\src\\core\\tensor.py:5\u001b[39m, in \u001b[36mTensor.__init__\u001b[39m\u001b[34m(self, data, requires_grad, requires_mask, name, eps)\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, data, requires_grad=\u001b[38;5;28;01mTrue\u001b[39;00m, requires_mask=\u001b[38;5;28;01mFalse\u001b[39;00m, name=\u001b[38;5;28;01mNone\u001b[39;00m, eps=\u001b[32m1e-4\u001b[39m):\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m     \u001b[38;5;28mself\u001b[39m.data = \u001b[43mxp\u001b[49m\u001b[43m.\u001b[49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m.astype(xp.float64)\n\u001b[32m      6\u001b[39m     \u001b[38;5;28mself\u001b[39m.requires_grad = requires_grad\n\u001b[32m      7\u001b[39m     \u001b[38;5;28mself\u001b[39m.parents = ()\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "net = Net(7,4,1)\n",
    "net._build(x.shape)\n",
    "optimizer = Standard(net.parameters(), lr=0.001)\n",
    "net.train(Tensor(x), Tensor(y), optimizer, 50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6fe72d55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0 loss 0.8076825604811081\n",
      "step 100 loss 0.7622999420352848\n",
      "step 200 loss 0.736710288784231\n",
      "step 300 loss 0.7218319802758465\n",
      "step 400 loss 0.7128380315685587\n",
      "step 500 loss 0.7071656087227384\n",
      "step 600 loss 0.7034308205133043\n",
      "step 700 loss 0.7008667903180094\n",
      "step 800 loss 0.699035743618891\n",
      "step 900 loss 0.6976799498494384\n",
      "step 1000 loss 0.69664295388873\n",
      "step 1100 loss 0.6958269009008714\n",
      "step 1200 loss 0.695168796062181\n",
      "step 1300 loss 0.6946269431904931\n",
      "step 1400 loss 0.6941729955782154\n",
      "step 1500 loss 0.6937871799086663\n",
      "step 1600 loss 0.693455358478106\n",
      "step 1700 loss 0.6931671815583634\n",
      "step 1800 loss 0.6929149006823363\n",
      "step 1900 loss 0.6926925910738568\n",
      "step 2000 loss 0.6924956323783472\n",
      "step 2100 loss 0.6923203555053383\n",
      "step 2200 loss 0.692163798178007\n",
      "step 2300 loss 0.6920235328123987\n",
      "step 2400 loss 0.6918975432916381\n",
      "step 2500 loss 0.6917841353023197\n",
      "step 2600 loss 0.691681870052673\n",
      "step 2700 loss 0.6915895145175944\n",
      "step 2800 loss 0.691506003532092\n",
      "step 2900 loss 0.691430410497989\n",
      "step 3000 loss 0.6913619244379732\n",
      "step 3100 loss 0.6912998317898497\n",
      "step 3200 loss 0.6912435017869081\n",
      "step 3300 loss 0.6911923745854813\n",
      "step 3400 loss 0.6911459515224972\n",
      "step 3500 loss 0.6911037870435757\n",
      "step 3600 loss 0.691065481955694\n",
      "step 3700 loss 0.691030677740968\n",
      "step 3800 loss 0.6909990517287697\n",
      "step 3900 loss 0.6909703129684684\n",
      "step 4000 loss 0.6909441986789456\n",
      "step 4100 loss 0.6909204711766826\n",
      "step 4200 loss 0.6908989152038985\n",
      "step 4300 loss 0.6908793355934042\n",
      "step 4400 loss 0.6908615552187021\n",
      "step 4500 loss 0.6908454131871828\n",
      "step 4600 loss 0.6908307632416814\n",
      "step 4700 loss 0.6908174723415503\n",
      "step 4800 loss 0.690805419399183\n",
      "step 4900 loss 0.6907944941517611\n"
     ]
    }
   ],
   "source": [
    "# Inputs\n",
    "\n",
    "num_samples = 1000\n",
    "x = Tensor(xp.random.randn(num_samples, 7), requires_grad=False)\n",
    "y = Tensor((xp.random.rand(num_samples, 1) > 0.5).astype(xp.float32), requires_grad=False)\n",
    "\n",
    "# Model\n",
    "model = Net(7, 4, 1)\n",
    "optimizer = Standard(model.parameters(), lr=0.01)  # Use a big LR for fast check\n",
    "\n",
    "# Train for like 500 steps\n",
    "for step in range(5000):\n",
    "    y_hat = model.forward(x)\n",
    "    loss = BCE(y_hat, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if step % 100 == 0:\n",
    "        print(\"step\", step, \"loss\", loss.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b66d44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be98f112",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2222f462",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21f80306",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\luequ\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dfb7c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "def split_text(text, max_tok, tokenizer):\n",
    "    sentences = sent_tokenize(text)\n",
    "    chunks = []\n",
    "    curr = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        tokens = tokenizer.encode(sentence)\n",
    "\n",
    "        if len(tokens) > max_tok:\n",
    "            continue\n",
    "\n",
    "        if len(tokens) + len(curr) > max_tok:\n",
    "            chunks.append(curr)\n",
    "            curr = []\n",
    "\n",
    "        curr.extend(tokens)\n",
    "\n",
    "    if curr:\n",
    "        chunks.append(curr)\n",
    "\n",
    "    return chunks\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
