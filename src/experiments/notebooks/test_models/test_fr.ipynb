{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94517d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "\n",
    "import os, sys\n",
    "sys.path.append(\"../../../../\")\n",
    "\n",
    "from src.core.module import Module\n",
    "from src.core.losses import CrossEntropy\n",
    "from src.core.optim import AdamW, Standard\n",
    "from src.core.tensor import Tensor\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6609a20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class T(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, vocab_size, max_seq_len, pad_idx):\n",
    "        super().__init__()\n",
    "        self.pad_idx = pad_idx\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model, padding_idx=pad_idx)\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, max_seq_len, d_model))\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=n_heads,\n",
    "            dim_feedforward=4 * d_model,\n",
    "            dropout=0.0,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=1)\n",
    "\n",
    "        self.project = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, idx):\n",
    "        # idx: (B, T)\n",
    "        padding_mask = (idx == self.pad_idx)  # (B, T)\n",
    "        x = self.embedding(idx) + self.pos_embedding[:, :idx.size(1)]\n",
    "        x = self.encoder(x, src_key_padding_mask=padding_mask)\n",
    "        logits = self.project(x)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bc5e5604",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(Module):\n",
    "    def __init__(self, d_model, n_heads, vocab_size, max_seq_len, pad_idx):\n",
    "        super().__init__()\n",
    "        self.e = self.embedding(vocab_size, d_model, max_seq_len, pad_idx, name=\"Embedding\")\n",
    "\n",
    "        self.head1 = self.transformer(d_model=d_model, n_heads=n_heads)\n",
    "        self.project = self.linear(d_model, vocab_size, name=\"project\")\n",
    "    \n",
    "    def forward(self, idx):\n",
    "        x, padding_mask = self.e.get_sentence_embedding(idx)\n",
    "        x = self.head1(x, padding_mask)\n",
    "        x = self.project(x)\n",
    "        return x\n",
    "\n",
    "    def train(self, x, y, epochs, optimizer):\n",
    "        for epoch in range(epochs):\n",
    "            y_hat = self.forward(x)\n",
    "            loss = CrossEntropy(y_hat, y, axis=-1)\n",
    "    \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            if epoch % 1 == 0:\n",
    "                print(f\"Epoch {epoch}, Loss: {loss.data}\")\n",
    "                \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c0871236",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | Loss: 4.3274\n",
      "Epoch 1 | Loss: 4.2448\n",
      "Epoch 2 | Loss: 4.1745\n",
      "Epoch 3 | Loss: 4.1144\n",
      "Epoch 4 | Loss: 4.0627\n",
      "Epoch 5 | Loss: 4.0178\n",
      "Epoch 6 | Loss: 3.9782\n",
      "Epoch 7 | Loss: 3.9423\n",
      "Epoch 8 | Loss: 3.9085\n",
      "Epoch 9 | Loss: 3.8756\n",
      "Epoch 10 | Loss: 3.8425\n",
      "Epoch 11 | Loss: 3.8081\n",
      "Epoch 12 | Loss: 3.7718\n",
      "Epoch 13 | Loss: 3.7333\n",
      "Epoch 14 | Loss: 3.6929\n",
      "Epoch 15 | Loss: 3.6508\n",
      "Epoch 16 | Loss: 3.6072\n",
      "Epoch 17 | Loss: 3.5623\n",
      "Epoch 18 | Loss: 3.5152\n",
      "Epoch 19 | Loss: 3.4657\n",
      "Epoch 20 | Loss: 3.4138\n",
      "Epoch 21 | Loss: 3.3594\n",
      "Epoch 22 | Loss: 3.3029\n",
      "Epoch 23 | Loss: 3.2447\n",
      "Epoch 24 | Loss: 3.1858\n",
      "Epoch 25 | Loss: 3.1261\n",
      "Epoch 26 | Loss: 3.0644\n",
      "Epoch 27 | Loss: 3.0016\n",
      "Epoch 28 | Loss: 2.9376\n",
      "Epoch 29 | Loss: 2.8727\n",
      "Epoch 30 | Loss: 2.8073\n",
      "Epoch 31 | Loss: 2.7412\n",
      "Epoch 32 | Loss: 2.6746\n",
      "Epoch 33 | Loss: 2.6055\n",
      "Epoch 34 | Loss: 2.5346\n",
      "Epoch 35 | Loss: 2.4629\n",
      "Epoch 36 | Loss: 2.3918\n",
      "Epoch 37 | Loss: 2.3209\n",
      "Epoch 38 | Loss: 2.2503\n",
      "Epoch 39 | Loss: 2.1791\n",
      "Epoch 40 | Loss: 2.1084\n",
      "Epoch 41 | Loss: 2.0389\n",
      "Epoch 42 | Loss: 1.9738\n",
      "Epoch 43 | Loss: 1.9039\n",
      "Epoch 44 | Loss: 1.8282\n",
      "Epoch 45 | Loss: 1.7641\n",
      "Epoch 46 | Loss: 1.7010\n",
      "Epoch 47 | Loss: 1.6324\n",
      "Epoch 48 | Loss: 1.5705\n",
      "Epoch 49 | Loss: 1.5115\n",
      "Epoch 50 | Loss: 1.4500\n",
      "Epoch 51 | Loss: 1.3888\n",
      "Epoch 52 | Loss: 1.3352\n",
      "Epoch 53 | Loss: 1.2798\n",
      "Epoch 54 | Loss: 1.2256\n",
      "Epoch 55 | Loss: 1.1737\n",
      "Epoch 56 | Loss: 1.1236\n",
      "Epoch 57 | Loss: 1.0726\n",
      "Epoch 58 | Loss: 1.0247\n",
      "Epoch 59 | Loss: 0.9785\n",
      "Epoch 60 | Loss: 0.9324\n",
      "Epoch 61 | Loss: 0.8886\n",
      "Epoch 62 | Loss: 0.8477\n",
      "Epoch 63 | Loss: 0.8055\n",
      "Epoch 64 | Loss: 0.7654\n",
      "Epoch 65 | Loss: 0.7279\n",
      "Epoch 66 | Loss: 0.6924\n",
      "Epoch 67 | Loss: 0.6583\n",
      "Epoch 68 | Loss: 0.6256\n",
      "Epoch 69 | Loss: 0.5933\n",
      "Epoch 70 | Loss: 0.5661\n",
      "Epoch 71 | Loss: 0.5341\n",
      "Epoch 72 | Loss: 0.5041\n",
      "Epoch 73 | Loss: 0.4762\n",
      "Epoch 74 | Loss: 0.4499\n",
      "Epoch 75 | Loss: 0.4236\n",
      "Epoch 76 | Loss: 0.4011\n",
      "Epoch 77 | Loss: 0.3782\n",
      "Epoch 78 | Loss: 0.3536\n",
      "Epoch 79 | Loss: 0.3336\n",
      "Epoch 80 | Loss: 0.3137\n",
      "Epoch 81 | Loss: 0.2953\n",
      "Epoch 82 | Loss: 0.2772\n",
      "Epoch 83 | Loss: 0.2604\n",
      "Epoch 84 | Loss: 0.2445\n",
      "Epoch 85 | Loss: 0.2288\n",
      "Epoch 86 | Loss: 0.2149\n",
      "Epoch 87 | Loss: 0.2026\n",
      "Epoch 88 | Loss: 0.1898\n",
      "Epoch 89 | Loss: 0.1788\n",
      "Epoch 90 | Loss: 0.1686\n",
      "Epoch 91 | Loss: 0.1584\n",
      "Epoch 92 | Loss: 0.1500\n",
      "Epoch 93 | Loss: 0.1415\n",
      "Epoch 94 | Loss: 0.1336\n",
      "Epoch 95 | Loss: 0.1263\n",
      "Epoch 96 | Loss: 0.1193\n",
      "Epoch 97 | Loss: 0.1131\n",
      "Epoch 98 | Loss: 0.1071\n",
      "Epoch 99 | Loss: 0.1018\n",
      "Epoch 100 | Loss: 0.0968\n",
      "Epoch 101 | Loss: 0.0920\n",
      "Epoch 102 | Loss: 0.0876\n",
      "Epoch 103 | Loss: 0.0834\n",
      "Epoch 104 | Loss: 0.0797\n",
      "Epoch 105 | Loss: 0.0761\n",
      "Epoch 106 | Loss: 0.0728\n",
      "Epoch 107 | Loss: 0.0697\n",
      "Epoch 108 | Loss: 0.0668\n",
      "Epoch 109 | Loss: 0.0641\n",
      "Epoch 110 | Loss: 0.0616\n",
      "Epoch 111 | Loss: 0.0593\n",
      "Epoch 112 | Loss: 0.0571\n",
      "Epoch 113 | Loss: 0.0550\n",
      "Epoch 114 | Loss: 0.0530\n",
      "Epoch 115 | Loss: 0.0512\n",
      "Epoch 116 | Loss: 0.0494\n",
      "Epoch 117 | Loss: 0.0477\n",
      "Epoch 118 | Loss: 0.0462\n",
      "Epoch 119 | Loss: 0.0447\n",
      "Epoch 120 | Loss: 0.0433\n",
      "Epoch 121 | Loss: 0.0420\n",
      "Epoch 122 | Loss: 0.0407\n",
      "Epoch 123 | Loss: 0.0395\n",
      "Epoch 124 | Loss: 0.0384\n",
      "Epoch 125 | Loss: 0.0373\n",
      "Epoch 126 | Loss: 0.0363\n",
      "Epoch 127 | Loss: 0.0354\n",
      "Epoch 128 | Loss: 0.0344\n",
      "Epoch 129 | Loss: 0.0336\n",
      "Epoch 130 | Loss: 0.0327\n",
      "Epoch 131 | Loss: 0.0319\n",
      "Epoch 132 | Loss: 0.0312\n",
      "Epoch 133 | Loss: 0.0304\n",
      "Epoch 134 | Loss: 0.0297\n",
      "Epoch 135 | Loss: 0.0291\n",
      "Epoch 136 | Loss: 0.0284\n",
      "Epoch 137 | Loss: 0.0278\n",
      "Epoch 138 | Loss: 0.0272\n",
      "Epoch 139 | Loss: 0.0267\n",
      "Epoch 140 | Loss: 0.0261\n",
      "Epoch 141 | Loss: 0.0256\n",
      "Epoch 142 | Loss: 0.0251\n",
      "Epoch 143 | Loss: 0.0246\n",
      "Epoch 144 | Loss: 0.0242\n",
      "Epoch 145 | Loss: 0.0237\n",
      "Epoch 146 | Loss: 0.0233\n",
      "Epoch 147 | Loss: 0.0228\n",
      "Epoch 148 | Loss: 0.0224\n",
      "Epoch 149 | Loss: 0.0220\n",
      "Epoch 150 | Loss: 0.0217\n",
      "Epoch 151 | Loss: 0.0213\n",
      "Epoch 152 | Loss: 0.0209\n",
      "Epoch 153 | Loss: 0.0206\n",
      "Epoch 154 | Loss: 0.0203\n",
      "Epoch 155 | Loss: 0.0199\n",
      "Epoch 156 | Loss: 0.0196\n",
      "Epoch 157 | Loss: 0.0193\n",
      "Epoch 158 | Loss: 0.0190\n",
      "Epoch 159 | Loss: 0.0187\n",
      "Epoch 160 | Loss: 0.0184\n",
      "Epoch 161 | Loss: 0.0182\n",
      "Epoch 162 | Loss: 0.0179\n",
      "Epoch 163 | Loss: 0.0176\n",
      "Epoch 164 | Loss: 0.0174\n",
      "Epoch 165 | Loss: 0.0171\n",
      "Epoch 166 | Loss: 0.0169\n",
      "Epoch 167 | Loss: 0.0167\n",
      "Epoch 168 | Loss: 0.0164\n",
      "Epoch 169 | Loss: 0.0162\n",
      "Epoch 170 | Loss: 0.0160\n",
      "Epoch 171 | Loss: 0.0158\n",
      "Epoch 172 | Loss: 0.0156\n",
      "Epoch 173 | Loss: 0.0154\n",
      "Epoch 174 | Loss: 0.0152\n",
      "Epoch 175 | Loss: 0.0150\n",
      "Epoch 176 | Loss: 0.0148\n",
      "Epoch 177 | Loss: 0.0146\n",
      "Epoch 178 | Loss: 0.0144\n",
      "Epoch 179 | Loss: 0.0142\n",
      "Epoch 180 | Loss: 0.0141\n",
      "Epoch 181 | Loss: 0.0139\n",
      "Epoch 182 | Loss: 0.0137\n",
      "Epoch 183 | Loss: 0.0136\n",
      "Epoch 184 | Loss: 0.0134\n",
      "Epoch 185 | Loss: 0.0133\n",
      "Epoch 186 | Loss: 0.0131\n",
      "Epoch 187 | Loss: 0.0130\n",
      "Epoch 188 | Loss: 0.0128\n",
      "Epoch 189 | Loss: 0.0127\n",
      "Epoch 190 | Loss: 0.0125\n",
      "Epoch 191 | Loss: 0.0124\n",
      "Epoch 192 | Loss: 0.0122\n",
      "Epoch 193 | Loss: 0.0121\n",
      "Epoch 194 | Loss: 0.0120\n",
      "Epoch 195 | Loss: 0.0118\n",
      "Epoch 196 | Loss: 0.0117\n",
      "Epoch 197 | Loss: 0.0116\n",
      "Epoch 198 | Loss: 0.0115\n",
      "Epoch 199 | Loss: 0.0114\n",
      "Epoch 200 | Loss: 0.0112\n",
      "Epoch 201 | Loss: 0.0111\n",
      "Epoch 202 | Loss: 0.0110\n",
      "Epoch 203 | Loss: 0.0109\n",
      "Epoch 204 | Loss: 0.0108\n",
      "Epoch 205 | Loss: 0.0107\n",
      "Epoch 206 | Loss: 0.0106\n",
      "Epoch 207 | Loss: 0.0105\n",
      "Epoch 208 | Loss: 0.0104\n",
      "Epoch 209 | Loss: 0.0103\n",
      "Epoch 210 | Loss: 0.0102\n",
      "Epoch 211 | Loss: 0.0101\n",
      "Epoch 212 | Loss: 0.0100\n",
      "Epoch 213 | Loss: 0.0099\n",
      "Epoch 214 | Loss: 0.0098\n",
      "Epoch 215 | Loss: 0.0097\n",
      "Epoch 216 | Loss: 0.0097\n",
      "Epoch 217 | Loss: 0.0096\n",
      "Epoch 218 | Loss: 0.0095\n",
      "Epoch 219 | Loss: 0.0094\n",
      "Epoch 220 | Loss: 0.0093\n",
      "Epoch 221 | Loss: 0.0092\n",
      "Epoch 222 | Loss: 0.0092\n",
      "Epoch 223 | Loss: 0.0091\n",
      "Epoch 224 | Loss: 0.0090\n",
      "Epoch 225 | Loss: 0.0089\n",
      "Epoch 226 | Loss: 0.0089\n",
      "Epoch 227 | Loss: 0.0088\n",
      "Epoch 228 | Loss: 0.0087\n",
      "Epoch 229 | Loss: 0.0086\n",
      "Epoch 230 | Loss: 0.0086\n",
      "Epoch 231 | Loss: 0.0085\n",
      "Epoch 232 | Loss: 0.0084\n",
      "Epoch 233 | Loss: 0.0084\n",
      "Epoch 234 | Loss: 0.0083\n",
      "Epoch 235 | Loss: 0.0082\n",
      "Epoch 236 | Loss: 0.0082\n",
      "Epoch 237 | Loss: 0.0081\n",
      "Epoch 238 | Loss: 0.0080\n",
      "Epoch 239 | Loss: 0.0080\n",
      "Epoch 240 | Loss: 0.0079\n",
      "Epoch 241 | Loss: 0.0079\n",
      "Epoch 242 | Loss: 0.0078\n",
      "Epoch 243 | Loss: 0.0077\n",
      "Epoch 244 | Loss: 0.0077\n",
      "Epoch 245 | Loss: 0.0076\n",
      "Epoch 246 | Loss: 0.0076\n",
      "Epoch 247 | Loss: 0.0075\n",
      "Epoch 248 | Loss: 0.0074\n",
      "Epoch 249 | Loss: 0.0074\n",
      "Epoch 250 | Loss: 0.0073\n",
      "Epoch 251 | Loss: 0.0073\n",
      "Epoch 252 | Loss: 0.0072\n",
      "Epoch 253 | Loss: 0.0072\n",
      "Epoch 254 | Loss: 0.0071\n",
      "Epoch 255 | Loss: 0.0071\n",
      "Epoch 256 | Loss: 0.0070\n",
      "Epoch 257 | Loss: 0.0070\n",
      "Epoch 258 | Loss: 0.0069\n",
      "Epoch 259 | Loss: 0.0069\n",
      "Epoch 260 | Loss: 0.0068\n",
      "Epoch 261 | Loss: 0.0068\n",
      "Epoch 262 | Loss: 0.0067\n",
      "Epoch 263 | Loss: 0.0067\n",
      "Epoch 264 | Loss: 0.0066\n",
      "Epoch 265 | Loss: 0.0066\n",
      "Epoch 266 | Loss: 0.0065\n",
      "Epoch 267 | Loss: 0.0065\n",
      "Epoch 268 | Loss: 0.0064\n",
      "Epoch 269 | Loss: 0.0064\n",
      "Epoch 270 | Loss: 0.0064\n",
      "Epoch 271 | Loss: 0.0063\n",
      "Epoch 272 | Loss: 0.0063\n",
      "Epoch 273 | Loss: 0.0062\n",
      "Epoch 274 | Loss: 0.0062\n",
      "Epoch 275 | Loss: 0.0062\n",
      "Epoch 276 | Loss: 0.0061\n",
      "Epoch 277 | Loss: 0.0061\n",
      "Epoch 278 | Loss: 0.0060\n",
      "Epoch 279 | Loss: 0.0060\n",
      "Epoch 280 | Loss: 0.0060\n",
      "Epoch 281 | Loss: 0.0059\n",
      "Epoch 282 | Loss: 0.0059\n",
      "Epoch 283 | Loss: 0.0058\n",
      "Epoch 284 | Loss: 0.0058\n",
      "Epoch 285 | Loss: 0.0058\n",
      "Epoch 286 | Loss: 0.0057\n",
      "Epoch 287 | Loss: 0.0057\n",
      "Epoch 288 | Loss: 0.0057\n",
      "Epoch 289 | Loss: 0.0056\n",
      "Epoch 290 | Loss: 0.0056\n",
      "Epoch 291 | Loss: 0.0056\n",
      "Epoch 292 | Loss: 0.0055\n",
      "Epoch 293 | Loss: 0.0055\n",
      "Epoch 294 | Loss: 0.0055\n",
      "Epoch 295 | Loss: 0.0054\n",
      "Epoch 296 | Loss: 0.0054\n",
      "Epoch 297 | Loss: 0.0054\n",
      "Epoch 298 | Loss: 0.0053\n",
      "Epoch 299 | Loss: 0.0053\n",
      "Epoch 300 | Loss: 0.0053\n",
      "Epoch 301 | Loss: 0.0052\n",
      "Epoch 302 | Loss: 0.0052\n",
      "Epoch 303 | Loss: 0.0052\n",
      "Epoch 304 | Loss: 0.0052\n",
      "Epoch 305 | Loss: 0.0051\n",
      "Epoch 306 | Loss: 0.0051\n",
      "Epoch 307 | Loss: 0.0051\n",
      "Epoch 308 | Loss: 0.0050\n",
      "Epoch 309 | Loss: 0.0050\n",
      "Epoch 310 | Loss: 0.0050\n",
      "Epoch 311 | Loss: 0.0049\n",
      "Epoch 312 | Loss: 0.0049\n",
      "Epoch 313 | Loss: 0.0049\n",
      "Epoch 314 | Loss: 0.0049\n",
      "Epoch 315 | Loss: 0.0048\n",
      "Epoch 316 | Loss: 0.0048\n",
      "Epoch 317 | Loss: 0.0048\n",
      "Epoch 318 | Loss: 0.0048\n",
      "Epoch 319 | Loss: 0.0047\n",
      "Epoch 320 | Loss: 0.0047\n",
      "Epoch 321 | Loss: 0.0047\n",
      "Epoch 322 | Loss: 0.0047\n",
      "Epoch 323 | Loss: 0.0046\n",
      "Epoch 324 | Loss: 0.0046\n",
      "Epoch 325 | Loss: 0.0046\n",
      "Epoch 326 | Loss: 0.0046\n",
      "Epoch 327 | Loss: 0.0045\n",
      "Epoch 328 | Loss: 0.0045\n",
      "Epoch 329 | Loss: 0.0045\n",
      "Epoch 330 | Loss: 0.0045\n",
      "Epoch 331 | Loss: 0.0044\n",
      "Epoch 332 | Loss: 0.0044\n",
      "Epoch 333 | Loss: 0.0044\n",
      "Epoch 334 | Loss: 0.0044\n",
      "Epoch 335 | Loss: 0.0044\n",
      "Epoch 336 | Loss: 0.0043\n",
      "Epoch 337 | Loss: 0.0043\n",
      "Epoch 338 | Loss: 0.0043\n",
      "Epoch 339 | Loss: 0.0043\n",
      "Epoch 340 | Loss: 0.0043\n",
      "Epoch 341 | Loss: 0.0042\n",
      "Epoch 342 | Loss: 0.0042\n",
      "Epoch 343 | Loss: 0.0042\n",
      "Epoch 344 | Loss: 0.0042\n",
      "Epoch 345 | Loss: 0.0042\n",
      "Epoch 346 | Loss: 0.0041\n",
      "Epoch 347 | Loss: 0.0041\n",
      "Epoch 348 | Loss: 0.0041\n",
      "Epoch 349 | Loss: 0.0041\n",
      "Epoch 350 | Loss: 0.0041\n",
      "Epoch 351 | Loss: 0.0040\n",
      "Epoch 352 | Loss: 0.0040\n",
      "Epoch 353 | Loss: 0.0040\n",
      "Epoch 354 | Loss: 0.0040\n",
      "Epoch 355 | Loss: 0.0040\n",
      "Epoch 356 | Loss: 0.0039\n",
      "Epoch 357 | Loss: 0.0039\n",
      "Epoch 358 | Loss: 0.0039\n",
      "Epoch 359 | Loss: 0.0039\n",
      "Epoch 360 | Loss: 0.0039\n",
      "Epoch 361 | Loss: 0.0039\n",
      "Epoch 362 | Loss: 0.0038\n",
      "Epoch 363 | Loss: 0.0038\n",
      "Epoch 364 | Loss: 0.0038\n",
      "Epoch 365 | Loss: 0.0038\n",
      "Epoch 366 | Loss: 0.0038\n",
      "Epoch 367 | Loss: 0.0038\n",
      "Epoch 368 | Loss: 0.0037\n",
      "Epoch 369 | Loss: 0.0037\n",
      "Epoch 370 | Loss: 0.0037\n",
      "Epoch 371 | Loss: 0.0037\n",
      "Epoch 372 | Loss: 0.0037\n",
      "Epoch 373 | Loss: 0.0037\n",
      "Epoch 374 | Loss: 0.0036\n",
      "Epoch 375 | Loss: 0.0036\n",
      "Epoch 376 | Loss: 0.0036\n",
      "Epoch 377 | Loss: 0.0036\n",
      "Epoch 378 | Loss: 0.0036\n",
      "Epoch 379 | Loss: 0.0036\n",
      "Epoch 380 | Loss: 0.0035\n",
      "Epoch 381 | Loss: 0.0035\n",
      "Epoch 382 | Loss: 0.0035\n",
      "Epoch 383 | Loss: 0.0035\n",
      "Epoch 384 | Loss: 0.0035\n",
      "Epoch 385 | Loss: 0.0035\n",
      "Epoch 386 | Loss: 0.0035\n",
      "Epoch 387 | Loss: 0.0034\n",
      "Epoch 388 | Loss: 0.0034\n",
      "Epoch 389 | Loss: 0.0034\n",
      "Epoch 390 | Loss: 0.0034\n",
      "Epoch 391 | Loss: 0.0034\n",
      "Epoch 392 | Loss: 0.0034\n",
      "Epoch 393 | Loss: 0.0034\n",
      "Epoch 394 | Loss: 0.0033\n",
      "Epoch 395 | Loss: 0.0033\n",
      "Epoch 396 | Loss: 0.0033\n",
      "Epoch 397 | Loss: 0.0033\n",
      "Epoch 398 | Loss: 0.0033\n",
      "Epoch 399 | Loss: 0.0033\n",
      "Epoch 400 | Loss: 0.0033\n",
      "Epoch 401 | Loss: 0.0033\n",
      "Epoch 402 | Loss: 0.0032\n",
      "Epoch 403 | Loss: 0.0032\n",
      "Epoch 404 | Loss: 0.0032\n",
      "Epoch 405 | Loss: 0.0032\n",
      "Epoch 406 | Loss: 0.0032\n",
      "Epoch 407 | Loss: 0.0032\n",
      "Epoch 408 | Loss: 0.0032\n",
      "Epoch 409 | Loss: 0.0032\n",
      "Epoch 410 | Loss: 0.0031\n",
      "Epoch 411 | Loss: 0.0031\n",
      "Epoch 412 | Loss: 0.0031\n",
      "Epoch 413 | Loss: 0.0031\n",
      "Epoch 414 | Loss: 0.0031\n",
      "Epoch 415 | Loss: 0.0031\n",
      "Epoch 416 | Loss: 0.0031\n",
      "Epoch 417 | Loss: 0.0031\n",
      "Epoch 418 | Loss: 0.0030\n",
      "Epoch 419 | Loss: 0.0030\n",
      "Epoch 420 | Loss: 0.0030\n",
      "Epoch 421 | Loss: 0.0030\n",
      "Epoch 422 | Loss: 0.0030\n",
      "Epoch 423 | Loss: 0.0030\n",
      "Epoch 424 | Loss: 0.0030\n",
      "Epoch 425 | Loss: 0.0030\n",
      "Epoch 426 | Loss: 0.0030\n",
      "Epoch 427 | Loss: 0.0029\n",
      "Epoch 428 | Loss: 0.0029\n",
      "Epoch 429 | Loss: 0.0029\n",
      "Epoch 430 | Loss: 0.0029\n",
      "Epoch 431 | Loss: 0.0029\n",
      "Epoch 432 | Loss: 0.0029\n",
      "Epoch 433 | Loss: 0.0029\n",
      "Epoch 434 | Loss: 0.0029\n",
      "Epoch 435 | Loss: 0.0029\n",
      "Epoch 436 | Loss: 0.0029\n",
      "Epoch 437 | Loss: 0.0028\n",
      "Epoch 438 | Loss: 0.0028\n",
      "Epoch 439 | Loss: 0.0028\n",
      "Epoch 440 | Loss: 0.0028\n",
      "Epoch 441 | Loss: 0.0028\n",
      "Epoch 442 | Loss: 0.0028\n",
      "Epoch 443 | Loss: 0.0028\n",
      "Epoch 444 | Loss: 0.0028\n",
      "Epoch 445 | Loss: 0.0028\n",
      "Epoch 446 | Loss: 0.0028\n",
      "Epoch 447 | Loss: 0.0027\n",
      "Epoch 448 | Loss: 0.0027\n",
      "Epoch 449 | Loss: 0.0027\n",
      "Epoch 450 | Loss: 0.0027\n",
      "Epoch 451 | Loss: 0.0027\n",
      "Epoch 452 | Loss: 0.0027\n",
      "Epoch 453 | Loss: 0.0027\n",
      "Epoch 454 | Loss: 0.0027\n",
      "Epoch 455 | Loss: 0.0027\n",
      "Epoch 456 | Loss: 0.0027\n",
      "Epoch 457 | Loss: 0.0027\n",
      "Epoch 458 | Loss: 0.0026\n",
      "Epoch 459 | Loss: 0.0026\n",
      "Epoch 460 | Loss: 0.0026\n",
      "Epoch 461 | Loss: 0.0026\n",
      "Epoch 462 | Loss: 0.0026\n",
      "Epoch 463 | Loss: 0.0026\n",
      "Epoch 464 | Loss: 0.0026\n",
      "Epoch 465 | Loss: 0.0026\n",
      "Epoch 466 | Loss: 0.0026\n",
      "Epoch 467 | Loss: 0.0026\n",
      "Epoch 468 | Loss: 0.0026\n",
      "Epoch 469 | Loss: 0.0026\n",
      "Epoch 470 | Loss: 0.0025\n",
      "Epoch 471 | Loss: 0.0025\n",
      "Epoch 472 | Loss: 0.0025\n",
      "Epoch 473 | Loss: 0.0025\n",
      "Epoch 474 | Loss: 0.0025\n",
      "Epoch 475 | Loss: 0.0025\n",
      "Epoch 476 | Loss: 0.0025\n",
      "Epoch 477 | Loss: 0.0025\n",
      "Epoch 478 | Loss: 0.0025\n",
      "Epoch 479 | Loss: 0.0025\n",
      "Epoch 480 | Loss: 0.0025\n",
      "Epoch 481 | Loss: 0.0025\n",
      "Epoch 482 | Loss: 0.0024\n",
      "Epoch 483 | Loss: 0.0024\n",
      "Epoch 484 | Loss: 0.0024\n",
      "Epoch 485 | Loss: 0.0024\n",
      "Epoch 486 | Loss: 0.0024\n",
      "Epoch 487 | Loss: 0.0024\n",
      "Epoch 488 | Loss: 0.0024\n",
      "Epoch 489 | Loss: 0.0024\n",
      "Epoch 490 | Loss: 0.0024\n",
      "Epoch 491 | Loss: 0.0024\n",
      "Epoch 492 | Loss: 0.0024\n",
      "Epoch 493 | Loss: 0.0024\n",
      "Epoch 494 | Loss: 0.0024\n",
      "Epoch 495 | Loss: 0.0024\n",
      "Epoch 496 | Loss: 0.0023\n",
      "Epoch 497 | Loss: 0.0023\n",
      "Epoch 498 | Loss: 0.0023\n",
      "Epoch 499 | Loss: 0.0023\n",
      "Epoch 500 | Loss: 0.0023\n",
      "Epoch 501 | Loss: 0.0023\n",
      "Epoch 502 | Loss: 0.0023\n",
      "Epoch 503 | Loss: 0.0023\n",
      "Epoch 504 | Loss: 0.0023\n",
      "Epoch 505 | Loss: 0.0023\n",
      "Epoch 506 | Loss: 0.0023\n",
      "Epoch 507 | Loss: 0.0023\n",
      "Epoch 508 | Loss: 0.0023\n",
      "Epoch 509 | Loss: 0.0023\n",
      "Epoch 510 | Loss: 0.0022\n",
      "Epoch 511 | Loss: 0.0022\n",
      "Epoch 512 | Loss: 0.0022\n",
      "Epoch 513 | Loss: 0.0022\n",
      "Epoch 514 | Loss: 0.0022\n",
      "Epoch 515 | Loss: 0.0022\n",
      "Epoch 516 | Loss: 0.0022\n",
      "Epoch 517 | Loss: 0.0022\n",
      "Epoch 518 | Loss: 0.0022\n",
      "Epoch 519 | Loss: 0.0022\n",
      "Epoch 520 | Loss: 0.0022\n",
      "Epoch 521 | Loss: 0.0022\n",
      "Epoch 522 | Loss: 0.0022\n",
      "Epoch 523 | Loss: 0.0022\n",
      "Epoch 524 | Loss: 0.0022\n",
      "Epoch 525 | Loss: 0.0022\n",
      "Epoch 526 | Loss: 0.0021\n",
      "Epoch 527 | Loss: 0.0021\n",
      "Epoch 528 | Loss: 0.0021\n",
      "Epoch 529 | Loss: 0.0021\n",
      "Epoch 530 | Loss: 0.0021\n",
      "Epoch 531 | Loss: 0.0021\n",
      "Epoch 532 | Loss: 0.0021\n",
      "Epoch 533 | Loss: 0.0021\n",
      "Epoch 534 | Loss: 0.0021\n",
      "Epoch 535 | Loss: 0.0021\n",
      "Epoch 536 | Loss: 0.0021\n",
      "Epoch 537 | Loss: 0.0021\n",
      "Epoch 538 | Loss: 0.0021\n",
      "Epoch 539 | Loss: 0.0021\n",
      "Epoch 540 | Loss: 0.0021\n",
      "Epoch 541 | Loss: 0.0021\n",
      "Epoch 542 | Loss: 0.0021\n",
      "Epoch 543 | Loss: 0.0020\n",
      "Epoch 544 | Loss: 0.0020\n",
      "Epoch 545 | Loss: 0.0020\n",
      "Epoch 546 | Loss: 0.0020\n",
      "Epoch 547 | Loss: 0.0020\n",
      "Epoch 548 | Loss: 0.0020\n",
      "Epoch 549 | Loss: 0.0020\n",
      "Epoch 550 | Loss: 0.0020\n",
      "Epoch 551 | Loss: 0.0020\n",
      "Epoch 552 | Loss: 0.0020\n",
      "Epoch 553 | Loss: 0.0020\n",
      "Epoch 554 | Loss: 0.0020\n",
      "Epoch 555 | Loss: 0.0020\n",
      "Epoch 556 | Loss: 0.0020\n",
      "Epoch 557 | Loss: 0.0020\n",
      "Epoch 558 | Loss: 0.0020\n",
      "Epoch 559 | Loss: 0.0020\n",
      "Epoch 560 | Loss: 0.0020\n",
      "Epoch 561 | Loss: 0.0020\n",
      "Epoch 562 | Loss: 0.0019\n",
      "Epoch 563 | Loss: 0.0019\n",
      "Epoch 564 | Loss: 0.0019\n",
      "Epoch 565 | Loss: 0.0019\n",
      "Epoch 566 | Loss: 0.0019\n",
      "Epoch 567 | Loss: 0.0019\n",
      "Epoch 568 | Loss: 0.0019\n",
      "Epoch 569 | Loss: 0.0019\n",
      "Epoch 570 | Loss: 0.0019\n",
      "Epoch 571 | Loss: 0.0019\n",
      "Epoch 572 | Loss: 0.0019\n",
      "Epoch 573 | Loss: 0.0019\n",
      "Epoch 574 | Loss: 0.0019\n",
      "Epoch 575 | Loss: 0.0019\n",
      "Epoch 576 | Loss: 0.0019\n",
      "Epoch 577 | Loss: 0.0019\n",
      "Epoch 578 | Loss: 0.0019\n",
      "Epoch 579 | Loss: 0.0019\n",
      "Epoch 580 | Loss: 0.0019\n",
      "Epoch 581 | Loss: 0.0019\n",
      "Epoch 582 | Loss: 0.0019\n",
      "Epoch 583 | Loss: 0.0018\n",
      "Epoch 584 | Loss: 0.0018\n",
      "Epoch 585 | Loss: 0.0018\n",
      "Epoch 586 | Loss: 0.0018\n",
      "Epoch 587 | Loss: 0.0018\n",
      "Epoch 588 | Loss: 0.0018\n",
      "Epoch 589 | Loss: 0.0018\n",
      "Epoch 590 | Loss: 0.0018\n",
      "Epoch 591 | Loss: 0.0018\n",
      "Epoch 592 | Loss: 0.0018\n",
      "Epoch 593 | Loss: 0.0018\n",
      "Epoch 594 | Loss: 0.0018\n",
      "Epoch 595 | Loss: 0.0018\n",
      "Epoch 596 | Loss: 0.0018\n",
      "Epoch 597 | Loss: 0.0018\n",
      "Epoch 598 | Loss: 0.0018\n",
      "Epoch 599 | Loss: 0.0018\n",
      "Epoch 600 | Loss: 0.0018\n",
      "Epoch 601 | Loss: 0.0018\n",
      "Epoch 602 | Loss: 0.0018\n",
      "Epoch 603 | Loss: 0.0018\n",
      "Epoch 604 | Loss: 0.0018\n",
      "Epoch 605 | Loss: 0.0018\n",
      "Epoch 606 | Loss: 0.0017\n",
      "Epoch 607 | Loss: 0.0017\n",
      "Epoch 608 | Loss: 0.0017\n",
      "Epoch 609 | Loss: 0.0017\n",
      "Epoch 610 | Loss: 0.0017\n",
      "Epoch 611 | Loss: 0.0017\n",
      "Epoch 612 | Loss: 0.0017\n",
      "Epoch 613 | Loss: 0.0017\n",
      "Epoch 614 | Loss: 0.0017\n",
      "Epoch 615 | Loss: 0.0017\n",
      "Epoch 616 | Loss: 0.0017\n",
      "Epoch 617 | Loss: 0.0017\n",
      "Epoch 618 | Loss: 0.0017\n",
      "Epoch 619 | Loss: 0.0017\n",
      "Epoch 620 | Loss: 0.0017\n",
      "Epoch 621 | Loss: 0.0017\n",
      "Epoch 622 | Loss: 0.0017\n",
      "Epoch 623 | Loss: 0.0017\n",
      "Epoch 624 | Loss: 0.0017\n",
      "Epoch 625 | Loss: 0.0017\n",
      "Epoch 626 | Loss: 0.0017\n",
      "Epoch 627 | Loss: 0.0017\n",
      "Epoch 628 | Loss: 0.0017\n",
      "Epoch 629 | Loss: 0.0017\n",
      "Epoch 630 | Loss: 0.0017\n",
      "Epoch 631 | Loss: 0.0016\n",
      "Epoch 632 | Loss: 0.0016\n",
      "Epoch 633 | Loss: 0.0016\n",
      "Epoch 634 | Loss: 0.0016\n",
      "Epoch 635 | Loss: 0.0016\n",
      "Epoch 636 | Loss: 0.0016\n",
      "Epoch 637 | Loss: 0.0016\n",
      "Epoch 638 | Loss: 0.0016\n",
      "Epoch 639 | Loss: 0.0016\n",
      "Epoch 640 | Loss: 0.0016\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m     18\u001b[39m x, y = create_dummy_data(BATCH_SIZE, MAX_SEQ_LEN, VOCAB_SIZE)\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1000\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m     logits = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m     loss = loss_fn(logits.view(-\u001b[32m1\u001b[39m, VOCAB_SIZE), y.reshape(-\u001b[32m1\u001b[39m))\n\u001b[32m     23\u001b[39m     optimizer.zero_grad()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\luequ\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\luequ\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 27\u001b[39m, in \u001b[36mT.forward\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m     25\u001b[39m padding_mask = (idx == \u001b[38;5;28mself\u001b[39m.pad_idx)  \u001b[38;5;66;03m# (B, T)\u001b[39;00m\n\u001b[32m     26\u001b[39m x = \u001b[38;5;28mself\u001b[39m.embedding(idx) + \u001b[38;5;28mself\u001b[39m.pos_embedding[:, :idx.size(\u001b[32m1\u001b[39m)]\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpadding_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     28\u001b[39m logits = \u001b[38;5;28mself\u001b[39m.project(x)\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m logits\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\luequ\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\luequ\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\luequ\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:514\u001b[39m, in \u001b[36mTransformerEncoder.forward\u001b[39m\u001b[34m(self, src, mask, src_key_padding_mask, is_causal)\u001b[39m\n\u001b[32m    511\u001b[39m is_causal = _detect_is_causal_mask(mask, is_causal, seq_len)\n\u001b[32m    513\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m mod \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.layers:\n\u001b[32m--> \u001b[39m\u001b[32m514\u001b[39m     output = \u001b[43mmod\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    515\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    516\u001b[39m \u001b[43m        \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    517\u001b[39m \u001b[43m        \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_causal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    518\u001b[39m \u001b[43m        \u001b[49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43msrc_key_padding_mask_for_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    519\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    521\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m convert_to_nested:\n\u001b[32m    522\u001b[39m     output = output.to_padded_tensor(\u001b[32m0.0\u001b[39m, src.size())\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\luequ\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\luequ\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\luequ\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:914\u001b[39m, in \u001b[36mTransformerEncoderLayer.forward\u001b[39m\u001b[34m(self, src, src_mask, src_key_padding_mask, is_causal)\u001b[39m\n\u001b[32m    910\u001b[39m     x = x + \u001b[38;5;28mself\u001b[39m._ff_block(\u001b[38;5;28mself\u001b[39m.norm2(x))\n\u001b[32m    911\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    912\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.norm1(\n\u001b[32m    913\u001b[39m         x\n\u001b[32m--> \u001b[39m\u001b[32m914\u001b[39m         + \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sa_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_causal\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    915\u001b[39m     )\n\u001b[32m    916\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.norm2(x + \u001b[38;5;28mself\u001b[39m._ff_block(x))\n\u001b[32m    918\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\luequ\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:928\u001b[39m, in \u001b[36mTransformerEncoderLayer._sa_block\u001b[39m\u001b[34m(self, x, attn_mask, key_padding_mask, is_causal)\u001b[39m\n\u001b[32m    921\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_sa_block\u001b[39m(\n\u001b[32m    922\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    923\u001b[39m     x: Tensor,\n\u001b[32m   (...)\u001b[39m\u001b[32m    926\u001b[39m     is_causal: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    927\u001b[39m ) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m928\u001b[39m     x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    929\u001b[39m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    931\u001b[39m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    933\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    934\u001b[39m \u001b[43m        \u001b[49m\u001b[43mneed_weights\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    935\u001b[39m \u001b[43m        \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_causal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    936\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m]\n\u001b[32m    937\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dropout1(x)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\luequ\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\luequ\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\luequ\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:1373\u001b[39m, in \u001b[36mMultiheadAttention.forward\u001b[39m\u001b[34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)\u001b[39m\n\u001b[32m   1347\u001b[39m     attn_output, attn_output_weights = F.multi_head_attention_forward(\n\u001b[32m   1348\u001b[39m         query,\n\u001b[32m   1349\u001b[39m         key,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1370\u001b[39m         is_causal=is_causal,\n\u001b[32m   1371\u001b[39m     )\n\u001b[32m   1372\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1373\u001b[39m     attn_output, attn_output_weights = \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmulti_head_attention_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1374\u001b[39m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1375\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1376\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1377\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43membed_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1378\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1379\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43min_proj_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1380\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43min_proj_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1381\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1382\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias_v\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1383\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43madd_zero_attn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1384\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1385\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mout_proj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1386\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mout_proj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1387\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1388\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1389\u001b[39m \u001b[43m        \u001b[49m\u001b[43mneed_weights\u001b[49m\u001b[43m=\u001b[49m\u001b[43mneed_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1390\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1391\u001b[39m \u001b[43m        \u001b[49m\u001b[43maverage_attn_weights\u001b[49m\u001b[43m=\u001b[49m\u001b[43maverage_attn_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1392\u001b[39m \u001b[43m        \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_causal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1393\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1394\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.batch_first \u001b[38;5;129;01mand\u001b[39;00m is_batched:\n\u001b[32m   1395\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m attn_output.transpose(\u001b[32m1\u001b[39m, \u001b[32m0\u001b[39m), attn_output_weights\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\luequ\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\functional.py:6410\u001b[39m, in \u001b[36mmulti_head_attention_forward\u001b[39m\u001b[34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights, is_causal)\u001b[39m\n\u001b[32m   6407\u001b[39m k = k.view(bsz, num_heads, src_len, head_dim)\n\u001b[32m   6408\u001b[39m v = v.view(bsz, num_heads, src_len, head_dim)\n\u001b[32m-> \u001b[39m\u001b[32m6410\u001b[39m attn_output = \u001b[43mscaled_dot_product_attention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   6411\u001b[39m \u001b[43m    \u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout_p\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_causal\u001b[49m\n\u001b[32m   6412\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   6413\u001b[39m attn_output = (\n\u001b[32m   6414\u001b[39m     attn_output.permute(\u001b[32m2\u001b[39m, \u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m3\u001b[39m).contiguous().view(bsz * tgt_len, embed_dim)\n\u001b[32m   6415\u001b[39m )\n\u001b[32m   6417\u001b[39m attn_output = linear(attn_output, out_proj_weight, out_proj_bias)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "def create_dummy_data(batch_size, seq_len, vocab_size):\n",
    "    src = torch.randint(2, vocab_size - 1, (batch_size, seq_len))\n",
    "    x = src[:, :-1]\n",
    "    y = src[:, 1:]\n",
    "    return x, y\n",
    "\n",
    "D_MODEL = 16\n",
    "N_HEADS = 4\n",
    "VOCAB_SIZE = 64\n",
    "MAX_SEQ_LEN = 16\n",
    "PAD_IDX = 0\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "model = T(D_MODEL, N_HEADS, VOCAB_SIZE, MAX_SEQ_LEN, PAD_IDX)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.01)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "x, y = create_dummy_data(BATCH_SIZE, MAX_SEQ_LEN, VOCAB_SIZE)\n",
    "\n",
    "for epoch in range(1000):\n",
    "    logits = model(x)\n",
    "    loss = loss_fn(logits.view(-1, VOCAB_SIZE), y.reshape(-1))\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if epoch % 1 == 0:\n",
    "        print(f\"Epoch {epoch} | Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e934961a",
   "metadata": {},
   "outputs": [],
   "source": [
    "D_MODEL = 16\n",
    "N_HEADS = 4\n",
    "VOCAB_SIZE = 64\n",
    "MAX_SEQ_LEN = 16\n",
    "PAD_IDX = 0\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5b15fe42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dummy_data():\n",
    "    src = np.random.randint(2, VOCAB_SIZE - 1, (BATCH_SIZE, MAX_SEQ_LEN))\n",
    "    x = src[:, :-1]\n",
    "    y = src[:, -1:]\n",
    "    return x, y\n",
    "\n",
    "x, y = create_dummy_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "337e2672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Architecture:\n",
      "  embedding_0 (embedding):\n",
      "    Embedding (embedding):\n",
      "      embedding_1_embedding_1_embed: shape=(64, 64), dtype=float16\n",
      "      embedding_1_embedding_1_pe: shape=(64, 64), dtype=float16\n",
      "  transformer_0 (transformer):\n",
      "    q (linear):\n",
      "      transformer_1_linear_1_q_weight: shape=(64, 64), dtype=float16\n",
      "      transformer_1_linear_1_q_bias: shape=(64,), dtype=float16\n",
      "    k (linear):\n",
      "      transformer_1_linear_2_k_weight: shape=(64, 64), dtype=float16\n",
      "      transformer_1_linear_2_k_bias: shape=(64,), dtype=float16\n",
      "    v (linear):\n",
      "      transformer_1_linear_3_v_weight: shape=(64, 64), dtype=float16\n",
      "      transformer_1_linear_3_v_bias: shape=(64,), dtype=float16\n",
      "    o (linear):\n",
      "      transformer_1_linear_4_o_weight: shape=(64, 64), dtype=float16\n",
      "      transformer_1_linear_4_o_bias: shape=(64,), dtype=float16\n",
      "    proj_up (linear):\n",
      "      transformer_1_linear_5_proj_up_weight: shape=(64, 256), dtype=float16\n",
      "      transformer_1_linear_5_proj_up_bias: shape=(256,), dtype=float16\n",
      "    proj_down (linear):\n",
      "      transformer_1_linear_6_proj_down_weight: shape=(256, 64), dtype=float16\n",
      "      transformer_1_linear_6_proj_down_bias: shape=(64,), dtype=float16\n",
      "    ln1 (layernorm):\n",
      "      transformer_1_layernorm_2_gamma: shape=(64,), dtype=float16\n",
      "      transformer_1_layernorm_2_beta: shape=(64,), dtype=float16\n",
      "    ln2 (layernorm):\n",
      "      transformer_1_layernorm_2_gamma: shape=(64,), dtype=float16\n",
      "      transformer_1_layernorm_2_beta: shape=(64,), dtype=float16\n",
      "  linear_0 (linear):\n",
      "    project (linear):\n",
      "      linear_1_linear_1_project_weight: shape=(64, 64), dtype=float16\n",
      "      linear_1_linear_1_project_bias: shape=(64,), dtype=float16\n",
      "  embedding_1 (embedding):\n",
      "    Embedding (embedding):\n",
      "      embedding_2_embedding_1_embed: shape=(16, 16), dtype=float16\n",
      "      embedding_2_embedding_1_pe: shape=(16, 16), dtype=float16\n",
      "  transformer_1 (transformer):\n",
      "    q (linear):\n",
      "      transformer_2_linear_1_q_weight: shape=(16, 16), dtype=float16\n",
      "      transformer_2_linear_1_q_bias: shape=(16,), dtype=float16\n",
      "    k (linear):\n",
      "      transformer_2_linear_2_k_weight: shape=(16, 16), dtype=float16\n",
      "      transformer_2_linear_2_k_bias: shape=(16,), dtype=float16\n",
      "    v (linear):\n",
      "      transformer_2_linear_3_v_weight: shape=(16, 16), dtype=float16\n",
      "      transformer_2_linear_3_v_bias: shape=(16,), dtype=float16\n",
      "    o (linear):\n",
      "      transformer_2_linear_4_o_weight: shape=(16, 16), dtype=float16\n",
      "      transformer_2_linear_4_o_bias: shape=(16,), dtype=float16\n",
      "    proj_up (linear):\n",
      "      transformer_2_linear_5_proj_up_weight: shape=(16, 64), dtype=float16\n",
      "      transformer_2_linear_5_proj_up_bias: shape=(64,), dtype=float16\n",
      "    proj_down (linear):\n",
      "      transformer_2_linear_6_proj_down_weight: shape=(64, 16), dtype=float16\n",
      "      transformer_2_linear_6_proj_down_bias: shape=(16,), dtype=float16\n",
      "    ln1 (layernorm):\n",
      "      transformer_2_layernorm_2_gamma: shape=(16,), dtype=float16\n",
      "      transformer_2_layernorm_2_beta: shape=(16,), dtype=float16\n",
      "    ln2 (layernorm):\n",
      "      transformer_2_layernorm_2_gamma: shape=(16,), dtype=float16\n",
      "      transformer_2_layernorm_2_beta: shape=(16,), dtype=float16\n",
      "  linear_1 (linear):\n",
      "    project (linear):\n",
      "      linear_2_linear_1_project_weight: shape=(16, 16), dtype=float16\n",
      "      linear_2_linear_1_project_bias: shape=(16,), dtype=float16\n",
      "  embedding_2 (embedding):\n",
      "    Embedding (embedding):\n",
      "      embedding_3_embedding_1_embed: shape=(64, 16), dtype=float16\n",
      "      embedding_3_embedding_1_pe: shape=(16, 16), dtype=float16\n",
      "  transformer_2 (transformer):\n",
      "    q (linear):\n",
      "      transformer_3_linear_1_q_weight: shape=(16, 16), dtype=float16\n",
      "      transformer_3_linear_1_q_bias: shape=(16,), dtype=float16\n",
      "    k (linear):\n",
      "      transformer_3_linear_2_k_weight: shape=(16, 16), dtype=float16\n",
      "      transformer_3_linear_2_k_bias: shape=(16,), dtype=float16\n",
      "    v (linear):\n",
      "      transformer_3_linear_3_v_weight: shape=(16, 16), dtype=float16\n",
      "      transformer_3_linear_3_v_bias: shape=(16,), dtype=float16\n",
      "    o (linear):\n",
      "      transformer_3_linear_4_o_weight: shape=(16, 16), dtype=float16\n",
      "      transformer_3_linear_4_o_bias: shape=(16,), dtype=float16\n",
      "    proj_up (linear):\n",
      "      transformer_3_linear_5_proj_up_weight: shape=(16, 64), dtype=float16\n",
      "      transformer_3_linear_5_proj_up_bias: shape=(64,), dtype=float16\n",
      "    proj_down (linear):\n",
      "      transformer_3_linear_6_proj_down_weight: shape=(64, 16), dtype=float16\n",
      "      transformer_3_linear_6_proj_down_bias: shape=(16,), dtype=float16\n",
      "    ln1 (layernorm):\n",
      "      transformer_3_layernorm_2_gamma: shape=(16,), dtype=float16\n",
      "      transformer_3_layernorm_2_beta: shape=(16,), dtype=float16\n",
      "    ln2 (layernorm):\n",
      "      transformer_3_layernorm_2_gamma: shape=(16,), dtype=float16\n",
      "      transformer_3_layernorm_2_beta: shape=(16,), dtype=float16\n",
      "  linear_2 (linear):\n",
      "    project (linear):\n",
      "      linear_3_linear_1_project_weight: shape=(16, 64), dtype=float16\n",
      "      linear_3_linear_1_project_bias: shape=(64,), dtype=float16\n",
      "  embedding_3 (embedding):\n",
      "    Embedding (embedding):\n",
      "      embedding_4_embedding_1_embed: shape=(64, 16), dtype=float16\n",
      "      embedding_4_embedding_1_pe: shape=(16, 16), dtype=float16\n",
      "  transformer_3 (transformer):\n",
      "    q (linear):\n",
      "      transformer_4_linear_1_q_weight: shape=(16, 16), dtype=float16\n",
      "      transformer_4_linear_1_q_bias: shape=(16,), dtype=float16\n",
      "    k (linear):\n",
      "      transformer_4_linear_2_k_weight: shape=(16, 16), dtype=float16\n",
      "      transformer_4_linear_2_k_bias: shape=(16,), dtype=float16\n",
      "    v (linear):\n",
      "      transformer_4_linear_3_v_weight: shape=(16, 16), dtype=float16\n",
      "      transformer_4_linear_3_v_bias: shape=(16,), dtype=float16\n",
      "    o (linear):\n",
      "      transformer_4_linear_4_o_weight: shape=(16, 16), dtype=float16\n",
      "      transformer_4_linear_4_o_bias: shape=(16,), dtype=float16\n",
      "    proj_up (linear):\n",
      "      transformer_4_linear_5_proj_up_weight: shape=(16, 64), dtype=float16\n",
      "      transformer_4_linear_5_proj_up_bias: shape=(64,), dtype=float16\n",
      "    proj_down (linear):\n",
      "      transformer_4_linear_6_proj_down_weight: shape=(64, 16), dtype=float16\n",
      "      transformer_4_linear_6_proj_down_bias: shape=(16,), dtype=float16\n",
      "    ln1 (layernorm):\n",
      "      transformer_4_layernorm_2_gamma: shape=(16,), dtype=float16\n",
      "      transformer_4_layernorm_2_beta: shape=(16,), dtype=float16\n",
      "    ln2 (layernorm):\n",
      "      transformer_4_layernorm_2_gamma: shape=(16,), dtype=float16\n",
      "      transformer_4_layernorm_2_beta: shape=(16,), dtype=float16\n",
      "  linear_3 (linear):\n",
      "    project (linear):\n",
      "      linear_4_linear_1_project_weight: shape=(16, 64), dtype=float16\n",
      "      linear_4_linear_1_project_bias: shape=(64,), dtype=float16\n",
      "  embedding_4 (embedding):\n",
      "    Embedding (embedding):\n",
      "      embedding_5_embedding_1_embed: shape=(64, 16), dtype=float16\n",
      "      embedding_5_embedding_1_pe: shape=(16, 16), dtype=float16\n",
      "  transformer_4 (transformer):\n",
      "    q (linear):\n",
      "      transformer_5_linear_1_q_weight: shape=(16, 16), dtype=float16\n",
      "      transformer_5_linear_1_q_bias: shape=(16,), dtype=float16\n",
      "    k (linear):\n",
      "      transformer_5_linear_2_k_weight: shape=(16, 16), dtype=float16\n",
      "      transformer_5_linear_2_k_bias: shape=(16,), dtype=float16\n",
      "    v (linear):\n",
      "      transformer_5_linear_3_v_weight: shape=(16, 16), dtype=float16\n",
      "      transformer_5_linear_3_v_bias: shape=(16,), dtype=float16\n",
      "    o (linear):\n",
      "      transformer_5_linear_4_o_weight: shape=(16, 16), dtype=float16\n",
      "      transformer_5_linear_4_o_bias: shape=(16,), dtype=float16\n",
      "    proj_up (linear):\n",
      "      transformer_5_linear_5_proj_up_weight: shape=(16, 64), dtype=float16\n",
      "      transformer_5_linear_5_proj_up_bias: shape=(64,), dtype=float16\n",
      "    proj_down (linear):\n",
      "      transformer_5_linear_6_proj_down_weight: shape=(64, 16), dtype=float16\n",
      "      transformer_5_linear_6_proj_down_bias: shape=(16,), dtype=float16\n",
      "    ln1 (layernorm):\n",
      "      transformer_5_layernorm_2_gamma: shape=(16,), dtype=float16\n",
      "      transformer_5_layernorm_2_beta: shape=(16,), dtype=float16\n",
      "    ln2 (layernorm):\n",
      "      transformer_5_layernorm_2_gamma: shape=(16,), dtype=float16\n",
      "      transformer_5_layernorm_2_beta: shape=(16,), dtype=float16\n",
      "  linear_4 (linear):\n",
      "    project (linear):\n",
      "      linear_5_linear_1_project_weight: shape=(16, 64), dtype=float16\n",
      "      linear_5_linear_1_project_bias: shape=(64,), dtype=float16\n",
      "  embedding_5 (embedding):\n",
      "    Embedding (embedding):\n",
      "      embedding_6_embedding_1_embed: shape=(64, 16), dtype=float16\n",
      "      embedding_6_embedding_1_pe: shape=(16, 16), dtype=float16\n",
      "  transformer_5 (transformer):\n",
      "    q (linear):\n",
      "      transformer_6_linear_1_q_weight: shape=(16, 16), dtype=float16\n",
      "      transformer_6_linear_1_q_bias: shape=(16,), dtype=float16\n",
      "    k (linear):\n",
      "      transformer_6_linear_2_k_weight: shape=(16, 16), dtype=float16\n",
      "      transformer_6_linear_2_k_bias: shape=(16,), dtype=float16\n",
      "    v (linear):\n",
      "      transformer_6_linear_3_v_weight: shape=(16, 16), dtype=float16\n",
      "      transformer_6_linear_3_v_bias: shape=(16,), dtype=float16\n",
      "    o (linear):\n",
      "      transformer_6_linear_4_o_weight: shape=(16, 16), dtype=float16\n",
      "      transformer_6_linear_4_o_bias: shape=(16,), dtype=float16\n",
      "    proj_up (linear):\n",
      "      transformer_6_linear_5_proj_up_weight: shape=(16, 64), dtype=float16\n",
      "      transformer_6_linear_5_proj_up_bias: shape=(64,), dtype=float16\n",
      "    proj_down (linear):\n",
      "      transformer_6_linear_6_proj_down_weight: shape=(64, 16), dtype=float16\n",
      "      transformer_6_linear_6_proj_down_bias: shape=(16,), dtype=float16\n",
      "    ln1 (layernorm):\n",
      "      transformer_6_layernorm_2_gamma: shape=(16,), dtype=float16\n",
      "      transformer_6_layernorm_2_beta: shape=(16,), dtype=float16\n",
      "    ln2 (layernorm):\n",
      "      transformer_6_layernorm_2_gamma: shape=(16,), dtype=float16\n",
      "      transformer_6_layernorm_2_beta: shape=(16,), dtype=float16\n",
      "  linear_5 (linear):\n",
      "    project (linear):\n",
      "      linear_6_linear_1_project_weight: shape=(16, 64), dtype=float16\n",
      "      linear_6_linear_1_project_bias: shape=(64,), dtype=float16\n",
      "  embedding_6 (embedding):\n",
      "    Embedding (embedding):\n",
      "      embedding_7_embedding_1_embed: shape=(64, 16), dtype=float16\n",
      "      embedding_7_embedding_1_pe: shape=(16, 16), dtype=float16\n",
      "  transformer_6 (transformer):\n",
      "    q (linear):\n",
      "      transformer_7_linear_1_q_weight: shape=(16, 16), dtype=float16\n",
      "      transformer_7_linear_1_q_bias: shape=(16,), dtype=float16\n",
      "    k (linear):\n",
      "      transformer_7_linear_2_k_weight: shape=(16, 16), dtype=float16\n",
      "      transformer_7_linear_2_k_bias: shape=(16,), dtype=float16\n",
      "    v (linear):\n",
      "      transformer_7_linear_3_v_weight: shape=(16, 16), dtype=float16\n",
      "      transformer_7_linear_3_v_bias: shape=(16,), dtype=float16\n",
      "    o (linear):\n",
      "      transformer_7_linear_4_o_weight: shape=(16, 16), dtype=float16\n",
      "      transformer_7_linear_4_o_bias: shape=(16,), dtype=float16\n",
      "    proj_up (linear):\n",
      "      transformer_7_linear_5_proj_up_weight: shape=(16, 64), dtype=float16\n",
      "      transformer_7_linear_5_proj_up_bias: shape=(64,), dtype=float16\n",
      "    proj_down (linear):\n",
      "      transformer_7_linear_6_proj_down_weight: shape=(64, 16), dtype=float16\n",
      "      transformer_7_linear_6_proj_down_bias: shape=(16,), dtype=float16\n",
      "    ln1 (layernorm):\n",
      "      transformer_7_layernorm_2_gamma: shape=(16,), dtype=float16\n",
      "      transformer_7_layernorm_2_beta: shape=(16,), dtype=float16\n",
      "    ln2 (layernorm):\n",
      "      transformer_7_layernorm_2_gamma: shape=(16,), dtype=float16\n",
      "      transformer_7_layernorm_2_beta: shape=(16,), dtype=float16\n",
      "  linear_6 (linear):\n",
      "    project (linear):\n",
      "      linear_7_linear_1_project_weight: shape=(16, 64), dtype=float16\n",
      "      linear_7_linear_1_project_bias: shape=(64,), dtype=float16\n",
      "Epoch 0, Loss: 4.16015625\n",
      "Epoch 1, Loss: 4.15625\n",
      "Epoch 2, Loss: 4.15234375\n",
      "Epoch 3, Loss: 4.14453125\n",
      "Epoch 4, Loss: 4.140625\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(net)\n\u001b[32m      4\u001b[39m optimizer = Standard(net.parameters(), lr=\u001b[32m0.01\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[43mnet\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 21\u001b[39m, in \u001b[36mNet.train\u001b[39m\u001b[34m(self, x, y, epochs, optimizer)\u001b[39m\n\u001b[32m     18\u001b[39m loss = CrossEntropy(y_hat, y, axis=-\u001b[32m1\u001b[39m)\n\u001b[32m     20\u001b[39m loss.backward()\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m \u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m optimizer.zero_grad()\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m epoch % \u001b[32m1\u001b[39m == \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ml-projects\\transformer\\src\\experiments\\notebooks\\../../..\\src\\core\\optim.py:162\u001b[39m, in \u001b[36mStandard.step\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    159\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m param[\u001b[33m'\u001b[39m\u001b[33mparam\u001b[39m\u001b[33m'\u001b[39m].grad \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    160\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m162\u001b[39m param[\u001b[33m'\u001b[39m\u001b[33mparam\u001b[39m\u001b[33m'\u001b[39m].grad = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mreduce_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mparam\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mparam\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    163\u001b[39m param[\u001b[33m'\u001b[39m\u001b[33mparam\u001b[39m\u001b[33m'\u001b[39m].data -= \u001b[38;5;28mself\u001b[39m.get_lr(\u001b[38;5;28mself\u001b[39m.t) * param[\u001b[33m'\u001b[39m\u001b[33mparam\u001b[39m\u001b[33m'\u001b[39m].grad.data\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ml-projects\\transformer\\src\\experiments\\notebooks\\../../..\\src\\core\\optim.py:55\u001b[39m, in \u001b[36mOptimizer.reduce_like\u001b[39m\u001b[34m(self, grad, target_shape)\u001b[39m\n\u001b[32m     52\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCannot broadcast grad shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m to target \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget_shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     54\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m axis \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mreversed\u001b[39m(axes_to_sum):\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m     grad = Tensor(\u001b[43mgrad\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m.\u001b[49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m)\n\u001b[32m     57\u001b[39m grad = Tensor(grad.data.reshape(target_shape))\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m grad\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\luequ\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\_core\\_methods.py:51\u001b[39m, in \u001b[36m_sum\u001b[39m\u001b[34m(a, axis, dtype, out, keepdims, initial, where)\u001b[39m\n\u001b[32m     49\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_sum\u001b[39m(a, axis=\u001b[38;5;28;01mNone\u001b[39;00m, dtype=\u001b[38;5;28;01mNone\u001b[39;00m, out=\u001b[38;5;28;01mNone\u001b[39;00m, keepdims=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m     50\u001b[39m          initial=_NoValue, where=\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mumr_sum\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "net = Net(D_MODEL, N_HEADS, VOCAB_SIZE, MAX_SEQ_LEN, PAD_IDX)\n",
    "net._build(x.shape)\n",
    "print(net)\n",
    "optimizer = Standard(net.parameters(), lr=0.01)\n",
    "\n",
    "\n",
    "net.train(Tensor(x), Tensor(y), 100, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3744ed6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
