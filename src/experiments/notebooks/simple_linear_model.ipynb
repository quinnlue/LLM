{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "948e45d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.append(\"../../../\")\n",
    "\n",
    "from src.core.module import Module, Linear, LayerNorm\n",
    "from src.core.losses import CrossEntropy, BCE\n",
    "from src.core.optim import Standard, AdamW\n",
    "from src.core.tensor import Tensor\n",
    "from src.utils.lr_scheduler import LRScheduler\n",
    "import numpy as np\n",
    "import time\n",
    "from typing import List\n",
    "# from src.tokenizer.tokenizer import Tokenizer\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a785bb0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== INITIAL PARAMETERS ===\n",
      "ours  weight:\n",
      " (15, 2)\n",
      "torch weight:\n",
      " [[ 0.17037155  0.22215534 -0.08031394  0.54166553 -0.16102839 -0.15895096\n",
      "   0.08299237 -0.59164194 -0.34739821 -0.31144969  0.50271338  0.02316198\n",
      "  -0.18672173 -0.39478754 -0.20601737]\n",
      " [-0.04742426  0.52239493 -0.08030831  0.26322794  0.18609656 -0.15974399\n",
      "  -0.65624971 -0.19286303  0.10778595 -0.48441617 -0.07744063 -0.4886846\n",
      "   0.03804613  0.12886336 -0.10005013]]\n",
      "max |Δ| : 0.0\n",
      "ours  bias : [-0.60170661  1.85227818]\n",
      "torch bias : [-0.60170661  1.85227818]\n",
      "max |Δ| : 0.0\n",
      "\n",
      "=== FORWARD PASS ===\n",
      "logits  max |Δ| : 0.0\n",
      "probs   max |Δ| : 0.0\n",
      "loss    ours: 0.7621513321543442   torch: 0.7621513321543439   |Δ|: 2.220446049250313e-16\n",
      "\n",
      "=== GRADIENTS ===\n",
      "grad weight  ours:\n",
      " [[-0.04202402 -0.02189836]\n",
      " [-0.00390502  0.00443687]\n",
      " [-0.00327928  0.00983601]\n",
      " [-0.04325104 -0.00781277]\n",
      " [ 0.11511354  0.04874803]\n",
      " [ 0.04046855  0.00406869]\n",
      " [-0.08025032 -0.04046177]\n",
      " [-0.07088411 -0.00238526]\n",
      " [-0.04862195  0.03461024]\n",
      " [ 0.04059041  0.00369065]\n",
      " [ 0.04582923  0.00800723]\n",
      " [-0.04038237 -0.00298981]\n",
      " [-0.02125911 -0.01764848]\n",
      " [ 0.0152997   0.0299284 ]\n",
      " [-0.17393904 -0.03444142]]\n",
      "grad weight  torch:\n",
      " [[-0.00171714  0.00844855]\n",
      " [ 0.06321    -0.05032762]\n",
      " [ 0.02920674  0.03916767]\n",
      " [-0.05862255 -0.04352591]\n",
      " [ 0.07236832  0.05530001]\n",
      " [ 0.03869073  0.10599669]\n",
      " [-0.07263114 -0.02149049]\n",
      " [-0.16953635  0.05084137]\n",
      " [-0.10650434  0.06076275]\n",
      " [ 0.04440126 -0.04744273]\n",
      " [ 0.14250927 -0.05935715]\n",
      " [-0.13510073  0.04114326]\n",
      " [-0.02916856 -0.03162429]\n",
      " [-0.03756912  0.05311114]\n",
      " [-0.21351203 -0.08076971]]\n",
      "max |Δ| : 0.10192799504480624\n",
      "grad bias    ours: [-0.14025205 -0.06350895]\n",
      "grad bias    torch: [-0.01601192  0.03803952]\n",
      "max |Δ| : 0.1242401283087105\n",
      "\n",
      "=== AFTER SGD STEP ===\n",
      "updated weight ours:\n",
      " [[ 0.17037155 -0.04742426]\n",
      " [ 0.22215534  0.52239493]\n",
      " [-0.08031394 -0.08030831]\n",
      " [ 0.54166553  0.26322794]\n",
      " [-0.16102839  0.18609656]\n",
      " [-0.15895096 -0.15974399]\n",
      " [ 0.08299237 -0.65624971]\n",
      " [-0.59164194 -0.19286303]\n",
      " [-0.34739821  0.10778595]\n",
      " [-0.31144969 -0.48441617]\n",
      " [ 0.50271338 -0.07744063]\n",
      " [ 0.02316198 -0.4886846 ]\n",
      " [-0.18672173  0.03804613]\n",
      " [-0.39478754  0.12886336]\n",
      " [-0.20601737 -0.10005013]]\n",
      "updated weight torch:\n",
      " [[ 0.17208869 -0.05587282]\n",
      " [ 0.15894534  0.57272256]\n",
      " [-0.10952069 -0.11947598]\n",
      " [ 0.60028807  0.30675385]\n",
      " [-0.23339671  0.13079655]\n",
      " [-0.19764168 -0.26574067]\n",
      " [ 0.15562351 -0.63475922]\n",
      " [-0.42210558 -0.2437044 ]\n",
      " [-0.24089387  0.0470232 ]\n",
      " [-0.35585095 -0.43697345]\n",
      " [ 0.36020411 -0.01808348]\n",
      " [ 0.15826272 -0.52982785]\n",
      " [-0.15755318  0.06967042]\n",
      " [-0.35721842  0.07575222]\n",
      " [ 0.00749465 -0.01928042]]\n",
      "max |Δ| : 0.21351202532509564\n",
      "updated bias   ours: [-0.60170661  1.85227818]\n",
      "updated bias   torch: [-0.58569469  1.81423866]\n",
      "max |Δ| : 0.03803952062657601\n"
     ]
    }
   ],
   "source": [
    "import numpy as np, torch\n",
    "from pprint import pprint\n",
    "\n",
    "from src.core.module import Module\n",
    "from src.core.tensor  import Tensor\n",
    "from src.core.losses  import BCE\n",
    "from src.core.optim   import Standard\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "# 0.  Setup ─ reproducibility\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "SEED     = 42\n",
    "LR       = 1\n",
    "CLIP_NORM= 1e-9              # effectively no clipping so we can see full grads\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "# 1.  Build matching Linear layers\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "mdl  = Module()\n",
    "lin  = mdl.linear(in_features=15, out_features=2, seed=SEED)\n",
    "\n",
    "torch_lin = torch.nn.Linear(15, 2, bias=True, dtype=torch.float64)\n",
    "torch_lin.weight.data = torch.tensor(lin.weight.data.T)  # transpose!\n",
    "torch_lin.bias.data   = torch.tensor(lin.bias.data)\n",
    "\n",
    "print(\"=== INITIAL PARAMETERS ===\")\n",
    "print(\"ours  weight:\\n\", lin.weight.data.shape)\n",
    "print(\"torch weight:\\n\", torch_lin.weight.data.numpy())\n",
    "print(\"max |Δ| :\", np.abs(lin.weight.data - torch_lin.weight.data.T.numpy()).max())\n",
    "print(\"ours  bias :\", lin.bias.data)\n",
    "print(\"torch bias :\", torch_lin.bias.data.numpy())\n",
    "print(\"max |Δ| :\", np.abs(lin.bias.data - torch_lin.bias.data.numpy()).max())\n",
    "print()\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "# 2.  Dummy data\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "x_np = np.random.randn(15, 15)\n",
    "y_np = np.random.randint(0, 2, size=(15, 2))\n",
    "\n",
    "x_my  = Tensor(x_np, requires_grad=False)\n",
    "y_my  = Tensor(y_np,  requires_grad=False)\n",
    "x_pt  = torch.tensor(x_np, dtype=torch.float64)\n",
    "y_pt  = torch.tensor(y_np, dtype=torch.float64)\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "# 3.  Forward pass\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "logits_my = lin(x_my)\n",
    "logits_pt = torch_lin(x_pt)\n",
    "\n",
    "probs_my  = mdl.sigmoid(logits_my)\n",
    "probs_pt  = torch.sigmoid(logits_pt)\n",
    "\n",
    "loss_my   = BCE(probs_my, y_my)\n",
    "loss_pt   = torch.nn.functional.binary_cross_entropy(probs_pt, y_pt)\n",
    "\n",
    "print(\"=== FORWARD PASS ===\")\n",
    "print(\"logits  max |Δ| :\", np.abs(logits_my.data - logits_pt.detach().numpy()).max())\n",
    "print(\"probs   max |Δ| :\", np.abs(probs_my.data  - probs_pt.detach().numpy()).max())\n",
    "print(\"loss    ours:\", loss_my.data, \"  torch:\", loss_pt.item(),\n",
    "      \"  |Δ|:\", abs(loss_my.data - loss_pt.item()))\n",
    "print()\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "# 4.  Backward pass\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "loss_my.backward()\n",
    "loss_pt.backward()\n",
    "\n",
    "print(\"=== GRADIENTS ===\")\n",
    "print(\"grad weight  ours:\\n\", lin.weight.grad.data)\n",
    "print(\"grad weight  torch:\\n\", torch_lin.weight.grad.T.numpy())   # transpose back\n",
    "print(\"max |Δ| :\", np.abs(lin.weight.grad.data - torch_lin.weight.grad.T.numpy()).max())\n",
    "print(\"grad bias    ours:\", lin.bias.grad.data)\n",
    "print(\"grad bias    torch:\", torch_lin.bias.grad.numpy())\n",
    "print(\"max |Δ| :\", np.abs(lin.bias.grad.data - torch_lin.bias.grad.numpy()).max())\n",
    "print()\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "# 5.  Optimiser step (SGD)\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "opt_my = Standard(mdl.parameters(), lr=LR, clip_norm=CLIP_NORM)\n",
    "opt_pt = torch.optim.SGD(torch_lin.parameters(), lr=LR)\n",
    "\n",
    "opt_my.step()\n",
    "opt_pt.step()\n",
    "\n",
    "print(\"=== AFTER SGD STEP ===\")\n",
    "print(\"updated weight ours:\\n\", lin.weight.data)\n",
    "print(\"updated weight torch:\\n\", torch_lin.weight.data.numpy().T)  # transpose back\n",
    "print(\"max |Δ| :\", np.abs(lin.weight.data - torch_lin.weight.data.numpy().T).max())\n",
    "print(\"updated bias   ours:\", lin.bias.data)\n",
    "print(\"updated bias   torch:\", torch_lin.bias.data.numpy())\n",
    "print(\"max |Δ| :\", np.abs(lin.bias.data - torch_lin.bias.data.numpy()).max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11bbef6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = self.linear(7, 1, name=\"fc1\")\n",
    "        self.ln = self.layer_norm(axis=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.dropout(x, p=0.1)\n",
    "        x = self.ln(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "    \n",
    "    def train(self, x: Tensor, y: Tensor, optimizer, num_epochs=100):\n",
    "        for epoch in range(num_epochs):\n",
    "            y_hat = self.forward(x)\n",
    "            \n",
    "            loss = BCE(y_hat, y)\n",
    "            \n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            if epoch % 10 == 0:\n",
    "\n",
    "                print(f\"Epoch {epoch}, Loss: {loss.data}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"../../../src/experiments/data.csv\")\n",
    "df['Quality'] = df['Quality'].apply(lambda x: 1 if x == \"Good\" else 0)\n",
    "X = Tensor(np.array(df.drop('Quality', axis=1).values))[:128]\n",
    "y = Tensor(np.array(df['Quality'].values).reshape((-1, 1)))[:128]\n",
    "\n",
    "X_test = Tensor(np.array(df.drop('Quality', axis=1).values))[128:]\n",
    "y_test = Tensor(np.array(df['Quality'].values).reshape((-1, 1)))[128:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1e432c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.6931471805599453\n",
      "Epoch 10, Loss: 0.6931471805599453\n",
      "Epoch 20, Loss: 0.6931471805599453\n",
      "Epoch 30, Loss: 0.6931471805599453\n",
      "Epoch 40, Loss: 0.6931471805599453\n",
      "Epoch 50, Loss: 0.6931471805599453\n",
      "Epoch 60, Loss: 0.6931471805599453\n",
      "Epoch 70, Loss: 0.6931471805599453\n",
      "Epoch 80, Loss: 0.6931471805599453\n",
      "Epoch 90, Loss: 0.6931471805599453\n",
      "Epoch 100, Loss: 0.6931471805599453\n",
      "Epoch 110, Loss: 0.6931471805599453\n",
      "Epoch 120, Loss: 0.6931471805599453\n",
      "Epoch 130, Loss: 0.6931471805599453\n",
      "Epoch 140, Loss: 0.6931471805599453\n",
      "Epoch 150, Loss: 0.6931471805599453\n",
      "Epoch 160, Loss: 0.6931471805599453\n",
      "Epoch 170, Loss: 0.6931471805599453\n",
      "Epoch 180, Loss: 0.6931471805599453\n",
      "Epoch 190, Loss: 0.6931471805599453\n",
      "Epoch 200, Loss: 0.6931471805599453\n",
      "Epoch 210, Loss: 0.6931471805599453\n",
      "Epoch 220, Loss: 0.6931471805599453\n",
      "Epoch 230, Loss: 0.6931471805599453\n",
      "Epoch 240, Loss: 0.6931471805599453\n",
      "Epoch 250, Loss: 0.6931471805599453\n",
      "Epoch 260, Loss: 0.6931471805599453\n",
      "Epoch 270, Loss: 0.6931471805599453\n",
      "Epoch 280, Loss: 0.6931471805599453\n",
      "Epoch 290, Loss: 0.6931471805599453\n",
      "Epoch 300, Loss: 0.6931471805599453\n",
      "Epoch 310, Loss: 0.6931471805599453\n",
      "Epoch 320, Loss: 0.6931471805599453\n",
      "Epoch 330, Loss: 0.6931471805599453\n",
      "Epoch 340, Loss: 0.6931471805599453\n",
      "Epoch 350, Loss: 0.6931471805599453\n",
      "Epoch 360, Loss: 0.6931471805599453\n",
      "Epoch 370, Loss: 0.6931471805599453\n",
      "Epoch 380, Loss: 0.6931471805599453\n",
      "Epoch 390, Loss: 0.6931471805599453\n",
      "Epoch 400, Loss: 0.6931471805599453\n",
      "Epoch 410, Loss: 0.6931471805599453\n",
      "Epoch 420, Loss: 0.6931471805599453\n",
      "Epoch 430, Loss: 0.6931471805599453\n",
      "Epoch 440, Loss: 0.6931471805599453\n",
      "Epoch 450, Loss: 0.6931471805599453\n",
      "Epoch 460, Loss: 0.6931471805599453\n",
      "Epoch 470, Loss: 0.6931471805599453\n",
      "Epoch 480, Loss: 0.6931471805599453\n",
      "Epoch 490, Loss: 0.6931471805599453\n",
      "Epoch 500, Loss: 0.6931471805599453\n",
      "Epoch 510, Loss: 0.6931471805599453\n",
      "Epoch 520, Loss: 0.6931471805599453\n",
      "Epoch 530, Loss: 0.6931471805599453\n",
      "Epoch 540, Loss: 0.6931471805599453\n",
      "Epoch 550, Loss: 0.6931471805599453\n",
      "Epoch 560, Loss: 0.6931471805599453\n",
      "Epoch 570, Loss: 0.6931471805599453\n",
      "Epoch 580, Loss: 0.6931471805599453\n",
      "Epoch 590, Loss: 0.6931471805599453\n",
      "Epoch 600, Loss: 0.6931471805599453\n",
      "Epoch 610, Loss: 0.6931471805599453\n",
      "Epoch 620, Loss: 0.6931471805599453\n",
      "Epoch 630, Loss: 0.6931471805599453\n",
      "Epoch 640, Loss: 0.6931471805599453\n",
      "Epoch 650, Loss: 0.6931471805599453\n",
      "Epoch 660, Loss: 0.6931471805599453\n",
      "Epoch 670, Loss: 0.6931471805599453\n",
      "Epoch 680, Loss: 0.6931471805599453\n",
      "Epoch 690, Loss: 0.6931471805599453\n",
      "Epoch 700, Loss: 0.6931471805599453\n",
      "Epoch 710, Loss: 0.6931471805599453\n",
      "Epoch 720, Loss: 0.6931471805599453\n",
      "Epoch 730, Loss: 0.6931471805599453\n",
      "Epoch 740, Loss: 0.6931471805599453\n",
      "Epoch 750, Loss: 0.6931471805599453\n",
      "Epoch 760, Loss: 0.6931471805599453\n",
      "Epoch 770, Loss: 0.6931471805599453\n",
      "Epoch 780, Loss: 0.6931471805599453\n",
      "Epoch 790, Loss: 0.6931471805599453\n",
      "Epoch 800, Loss: 0.6931471805599453\n",
      "Epoch 810, Loss: 0.6931471805599453\n",
      "Epoch 820, Loss: 0.6931471805599453\n",
      "Epoch 830, Loss: 0.6931471805599453\n",
      "Epoch 840, Loss: 0.6931471805599453\n",
      "Epoch 850, Loss: 0.6931471805599453\n",
      "Epoch 860, Loss: 0.6931471805599453\n",
      "Epoch 870, Loss: 0.6931471805599453\n",
      "Epoch 880, Loss: 0.6931471805599453\n",
      "Epoch 890, Loss: 0.6931471805599453\n",
      "Epoch 900, Loss: 0.6931471805599453\n",
      "Epoch 910, Loss: 0.6931471805599453\n",
      "Epoch 920, Loss: 0.6931471805599453\n",
      "Epoch 930, Loss: 0.6931471805599453\n",
      "Epoch 940, Loss: 0.6931471805599453\n",
      "Epoch 950, Loss: 0.6931471805599453\n",
      "Epoch 960, Loss: 0.6931471805599453\n",
      "Epoch 970, Loss: 0.6931471805599453\n",
      "Epoch 980, Loss: 0.6931471805599453\n",
      "Epoch 990, Loss: 0.6931471805599453\n",
      "Epoch 1000, Loss: 0.6931471805599453\n",
      "Epoch 1010, Loss: 0.6931471805599453\n",
      "Epoch 1020, Loss: 0.6931471805599453\n",
      "Epoch 1030, Loss: 0.6931471805599453\n",
      "Epoch 1040, Loss: 0.6931471805599453\n",
      "Epoch 1050, Loss: 0.6931471805599453\n",
      "Epoch 1060, Loss: 0.6931471805599453\n",
      "Epoch 1070, Loss: 0.6931471805599453\n",
      "Epoch 1080, Loss: 0.6931471805599453\n",
      "Epoch 1090, Loss: 0.6931471805599453\n",
      "Epoch 1100, Loss: 0.6931471805599453\n",
      "Epoch 1110, Loss: 0.6931471805599453\n",
      "Epoch 1120, Loss: 0.6931471805599453\n",
      "Epoch 1130, Loss: 0.6931471805599453\n",
      "Epoch 1140, Loss: 0.6931471805599453\n",
      "Epoch 1150, Loss: 0.6931471805599453\n",
      "Epoch 1160, Loss: 0.6931471805599453\n",
      "Epoch 1170, Loss: 0.6931471805599453\n",
      "Epoch 1180, Loss: 0.6931471805599453\n",
      "Epoch 1190, Loss: 0.6931471805599453\n",
      "Epoch 1200, Loss: 0.6931471805599453\n",
      "Epoch 1210, Loss: 0.6931471805599453\n",
      "Epoch 1220, Loss: 0.6931471805599453\n",
      "Epoch 1230, Loss: 0.6931471805599453\n",
      "Epoch 1240, Loss: 0.6931471805599453\n",
      "Epoch 1250, Loss: 0.6931471805599453\n",
      "Epoch 1260, Loss: 0.6931471805599453\n",
      "Epoch 1270, Loss: 0.6931471805599453\n",
      "Epoch 1280, Loss: 0.6931471805599453\n",
      "Epoch 1290, Loss: 0.6931471805599453\n",
      "Epoch 1300, Loss: 0.6931471805599453\n",
      "Epoch 1310, Loss: 0.6931471805599453\n",
      "Epoch 1320, Loss: 0.6931471805599453\n",
      "Epoch 1330, Loss: 0.6931471805599453\n",
      "Epoch 1340, Loss: 0.6931471805599453\n",
      "Epoch 1350, Loss: 0.6931471805599453\n",
      "Epoch 1360, Loss: 0.6931471805599453\n",
      "Epoch 1370, Loss: 0.6931471805599453\n",
      "Epoch 1380, Loss: 0.6931471805599453\n",
      "Epoch 1390, Loss: 0.6931471805599453\n",
      "Epoch 1400, Loss: 0.6931471805599453\n",
      "Epoch 1410, Loss: 0.6931471805599453\n",
      "Epoch 1420, Loss: 0.6931471805599453\n",
      "Epoch 1430, Loss: 0.6931471805599453\n",
      "Epoch 1440, Loss: 0.6931471805599453\n",
      "Epoch 1450, Loss: 0.6931471805599453\n",
      "Epoch 1460, Loss: 0.6931471805599453\n",
      "Epoch 1470, Loss: 0.6931471805599453\n",
      "Epoch 1480, Loss: 0.6931471805599453\n",
      "Epoch 1490, Loss: 0.6931471805599453\n",
      "Epoch 1500, Loss: 0.6931471805599453\n",
      "Epoch 1510, Loss: 0.6931471805599453\n",
      "Epoch 1520, Loss: 0.6931471805599453\n",
      "Epoch 1530, Loss: 0.6931471805599453\n",
      "Epoch 1540, Loss: 0.6931471805599453\n",
      "Epoch 1550, Loss: 0.6931471805599453\n",
      "Epoch 1560, Loss: 0.6931471805599453\n",
      "Epoch 1570, Loss: 0.6931471805599453\n",
      "Epoch 1580, Loss: 0.6931471805599453\n",
      "Epoch 1590, Loss: 0.6931471805599453\n",
      "Epoch 1600, Loss: 0.6931471805599453\n",
      "Epoch 1610, Loss: 0.6931471805599453\n",
      "Epoch 1620, Loss: 0.6931471805599453\n",
      "Epoch 1630, Loss: 0.6931471805599453\n",
      "Epoch 1640, Loss: 0.6931471805599453\n",
      "Epoch 1650, Loss: 0.6931471805599453\n",
      "Epoch 1660, Loss: 0.6931471805599453\n",
      "Epoch 1670, Loss: 0.6931471805599453\n",
      "Epoch 1680, Loss: 0.6931471805599453\n",
      "Epoch 1690, Loss: 0.6931471805599453\n",
      "Epoch 1700, Loss: 0.6931471805599453\n",
      "Epoch 1710, Loss: 0.6931471805599453\n",
      "Epoch 1720, Loss: 0.6931471805599453\n",
      "Epoch 1730, Loss: 0.6931471805599453\n",
      "Epoch 1740, Loss: 0.6931471805599453\n",
      "Epoch 1750, Loss: 0.6931471805599453\n",
      "Epoch 1760, Loss: 0.6931471805599453\n",
      "Epoch 1770, Loss: 0.6931471805599453\n",
      "Epoch 1780, Loss: 0.6931471805599453\n",
      "Epoch 1790, Loss: 0.6931471805599453\n",
      "Epoch 1800, Loss: 0.6931471805599453\n",
      "Epoch 1810, Loss: 0.6931471805599453\n",
      "Epoch 1820, Loss: 0.6931471805599453\n",
      "Epoch 1830, Loss: 0.6931471805599453\n",
      "Epoch 1840, Loss: 0.6931471805599453\n",
      "Epoch 1850, Loss: 0.6931471805599453\n",
      "Epoch 1860, Loss: 0.6931471805599453\n",
      "Epoch 1870, Loss: 0.6931471805599453\n",
      "Epoch 1880, Loss: 0.6931471805599453\n",
      "Epoch 1890, Loss: 0.6931471805599453\n",
      "Epoch 1900, Loss: 0.6931471805599453\n",
      "Epoch 1910, Loss: 0.6931471805599453\n",
      "Epoch 1920, Loss: 0.6931471805599453\n",
      "Epoch 1930, Loss: 0.6931471805599453\n",
      "Epoch 1940, Loss: 0.6931471805599453\n",
      "Epoch 1950, Loss: 0.6931471805599453\n",
      "Epoch 1960, Loss: 0.6931471805599453\n",
      "Epoch 1970, Loss: 0.6931471805599453\n",
      "Epoch 1980, Loss: 0.6931471805599453\n",
      "Epoch 1990, Loss: 0.6931471805599453\n",
      "Epoch 2000, Loss: 0.6931471805599453\n",
      "Epoch 2010, Loss: 0.6931471805599453\n",
      "Epoch 2020, Loss: 0.6931471805599453\n",
      "Epoch 2030, Loss: 0.6931471805599453\n",
      "Epoch 2040, Loss: 0.6931471805599453\n",
      "Epoch 2050, Loss: 0.6931471805599453\n",
      "Epoch 2060, Loss: 0.6931471805599453\n",
      "Epoch 2070, Loss: 0.6931471805599453\n",
      "Epoch 2080, Loss: 0.6931471805599453\n",
      "Epoch 2090, Loss: 0.6931471805599453\n",
      "Epoch 2100, Loss: 0.6931471805599453\n",
      "Epoch 2110, Loss: 0.6931471805599453\n",
      "Epoch 2120, Loss: 0.6931471805599453\n",
      "Epoch 2130, Loss: 0.6931471805599453\n",
      "Epoch 2140, Loss: 0.6931471805599453\n",
      "Epoch 2150, Loss: 0.6931471805599453\n",
      "Epoch 2160, Loss: 0.6931471805599453\n",
      "Epoch 2170, Loss: 0.6931471805599453\n",
      "Epoch 2180, Loss: 0.6931471805599453\n",
      "Epoch 2190, Loss: 0.6931471805599453\n",
      "Epoch 2200, Loss: 0.6931471805599453\n",
      "Epoch 2210, Loss: 0.6931471805599453\n",
      "Epoch 2220, Loss: 0.6931471805599453\n",
      "Epoch 2230, Loss: 0.6931471805599453\n",
      "Epoch 2240, Loss: 0.6931471805599453\n",
      "Epoch 2250, Loss: 0.6931471805599453\n",
      "Epoch 2260, Loss: 0.6931471805599453\n",
      "Epoch 2270, Loss: 0.6931471805599453\n",
      "Epoch 2280, Loss: 0.6931471805599453\n",
      "Epoch 2290, Loss: 0.6931471805599453\n",
      "Epoch 2300, Loss: 0.6931471805599453\n",
      "Epoch 2310, Loss: 0.6931471805599453\n",
      "Epoch 2320, Loss: 0.6931471805599453\n",
      "Epoch 2330, Loss: 0.6931471805599453\n",
      "Epoch 2340, Loss: 0.6931471805599453\n",
      "Epoch 2350, Loss: 0.6931471805599453\n",
      "Epoch 2360, Loss: 0.6931471805599453\n",
      "Epoch 2370, Loss: 0.6931471805599453\n",
      "Epoch 2380, Loss: 0.6931471805599453\n",
      "Epoch 2390, Loss: 0.6931471805599453\n",
      "Epoch 2400, Loss: 0.6931471805599453\n",
      "Epoch 2410, Loss: 0.6931471805599453\n",
      "Epoch 2420, Loss: 0.6931471805599453\n",
      "Epoch 2430, Loss: 0.6931471805599453\n",
      "Epoch 2440, Loss: 0.6931471805599453\n",
      "Epoch 2450, Loss: 0.6931471805599453\n",
      "Epoch 2460, Loss: 0.6931471805599453\n",
      "Epoch 2470, Loss: 0.6931471805599453\n",
      "Epoch 2480, Loss: 0.6931471805599453\n",
      "Epoch 2490, Loss: 0.6931471805599453\n",
      "Epoch 2500, Loss: 0.6931471805599453\n",
      "Epoch 2510, Loss: 0.6931471805599453\n",
      "Epoch 2520, Loss: 0.6931471805599453\n",
      "Epoch 2530, Loss: 0.6931471805599453\n",
      "Epoch 2540, Loss: 0.6931471805599453\n",
      "Epoch 2550, Loss: 0.6931471805599453\n",
      "Epoch 2560, Loss: 0.6931471805599453\n",
      "Epoch 2570, Loss: 0.6931471805599453\n",
      "Epoch 2580, Loss: 0.6931471805599453\n",
      "Epoch 2590, Loss: 0.6931471805599453\n",
      "Epoch 2600, Loss: 0.6931471805599453\n",
      "Epoch 2610, Loss: 0.6931471805599453\n",
      "Epoch 2620, Loss: 0.6931471805599453\n",
      "Epoch 2630, Loss: 0.6931471805599453\n",
      "Epoch 2640, Loss: 0.6931471805599453\n",
      "Epoch 2650, Loss: 0.6931471805599453\n",
      "Epoch 2660, Loss: 0.6931471805599453\n",
      "Epoch 2670, Loss: 0.6931471805599453\n",
      "Epoch 2680, Loss: 0.6931471805599453\n",
      "Epoch 2690, Loss: 0.6931471805599453\n",
      "Epoch 2700, Loss: 0.6931471805599453\n",
      "Epoch 2710, Loss: 0.6931471805599453\n",
      "Epoch 2720, Loss: 0.6931471805599453\n",
      "Epoch 2730, Loss: 0.6931471805599453\n",
      "Epoch 2740, Loss: 0.6931471805599453\n",
      "Epoch 2750, Loss: 0.6931471805599453\n",
      "Epoch 2760, Loss: 0.6931471805599453\n",
      "Epoch 2770, Loss: 0.6931471805599453\n",
      "Epoch 2780, Loss: 0.6931471805599453\n",
      "Epoch 2790, Loss: 0.6931471805599453\n",
      "Epoch 2800, Loss: 0.6931471805599453\n",
      "Epoch 2810, Loss: 0.6931471805599453\n",
      "Epoch 2820, Loss: 0.6931471805599453\n",
      "Epoch 2830, Loss: 0.6931471805599453\n",
      "Epoch 2840, Loss: 0.6931471805599453\n",
      "Epoch 2850, Loss: 0.6931471805599453\n",
      "Epoch 2860, Loss: 0.6931471805599453\n",
      "Epoch 2870, Loss: 0.6931471805599453\n",
      "Epoch 2880, Loss: 0.6931471805599453\n",
      "Epoch 2890, Loss: 0.6931471805599453\n",
      "Epoch 2900, Loss: 0.6931471805599453\n",
      "Epoch 2910, Loss: 0.6931471805599453\n",
      "Epoch 2920, Loss: 0.6931471805599453\n",
      "Epoch 2930, Loss: 0.6931471805599453\n",
      "Epoch 2940, Loss: 0.6931471805599453\n",
      "Epoch 2950, Loss: 0.6931471805599453\n",
      "Epoch 2960, Loss: 0.6931471805599453\n",
      "Epoch 2970, Loss: 0.6931471805599453\n",
      "Epoch 2980, Loss: 0.6931471805599453\n",
      "Epoch 2990, Loss: 0.6931471805599453\n",
      "Epoch 3000, Loss: 0.6931471805599453\n",
      "Epoch 3010, Loss: 0.6931471805599453\n",
      "Epoch 3020, Loss: 0.6931471805599453\n",
      "Epoch 3030, Loss: 0.6931471805599453\n",
      "Epoch 3040, Loss: 0.6931471805599453\n",
      "Epoch 3050, Loss: 0.6931471805599453\n",
      "Epoch 3060, Loss: 0.6931471805599453\n",
      "Epoch 3070, Loss: 0.6931471805599453\n",
      "Epoch 3080, Loss: 0.6931471805599453\n",
      "Epoch 3090, Loss: 0.6931471805599453\n",
      "Epoch 3100, Loss: 0.6931471805599453\n",
      "Epoch 3110, Loss: 0.6931471805599453\n",
      "Epoch 3120, Loss: 0.6931471805599453\n",
      "Epoch 3130, Loss: 0.6931471805599453\n",
      "Epoch 3140, Loss: 0.6931471805599453\n",
      "Epoch 3150, Loss: 0.6931471805599453\n",
      "Epoch 3160, Loss: 0.6931471805599453\n",
      "Epoch 3170, Loss: 0.6931471805599453\n",
      "Epoch 3180, Loss: 0.6931471805599453\n",
      "Epoch 3190, Loss: 0.6931471805599453\n",
      "Epoch 3200, Loss: 0.6931471805599453\n",
      "Epoch 3210, Loss: 0.6931471805599453\n",
      "Epoch 3220, Loss: 0.6931471805599453\n",
      "Epoch 3230, Loss: 0.6931471805599453\n",
      "Epoch 3240, Loss: 0.6931471805599453\n",
      "Epoch 3250, Loss: 0.6931471805599453\n",
      "Epoch 3260, Loss: 0.6931471805599453\n",
      "Epoch 3270, Loss: 0.6931471805599453\n",
      "Epoch 3280, Loss: 0.6931471805599453\n",
      "Epoch 3290, Loss: 0.6931471805599453\n",
      "Epoch 3300, Loss: 0.6931471805599453\n",
      "Epoch 3310, Loss: 0.6931471805599453\n",
      "Epoch 3320, Loss: 0.6931471805599453\n",
      "Epoch 3330, Loss: 0.6931471805599453\n",
      "Epoch 3340, Loss: 0.6931471805599453\n",
      "Epoch 3350, Loss: 0.6931471805599453\n",
      "Epoch 3360, Loss: 0.6931471805599453\n",
      "Epoch 3370, Loss: 0.6931471805599453\n",
      "Epoch 3380, Loss: 0.6931471805599453\n",
      "Epoch 3390, Loss: 0.6931471805599453\n",
      "Epoch 3400, Loss: 0.6931471805599453\n",
      "Epoch 3410, Loss: 0.6931471805599453\n",
      "Epoch 3420, Loss: 0.6931471805599453\n",
      "Epoch 3430, Loss: 0.6931471805599453\n",
      "Epoch 3440, Loss: 0.6931471805599453\n",
      "Epoch 3450, Loss: 0.6931471805599453\n",
      "Epoch 3460, Loss: 0.6931471805599453\n",
      "Epoch 3470, Loss: 0.6931471805599453\n",
      "Epoch 3480, Loss: 0.6931471805599453\n",
      "Epoch 3490, Loss: 0.6931471805599453\n",
      "Epoch 3500, Loss: 0.6931471805599453\n",
      "Epoch 3510, Loss: 0.6931471805599453\n",
      "Epoch 3520, Loss: 0.6931471805599453\n",
      "Epoch 3530, Loss: 0.6931471805599453\n",
      "Epoch 3540, Loss: 0.6931471805599453\n",
      "Epoch 3550, Loss: 0.6931471805599453\n",
      "Epoch 3560, Loss: 0.6931471805599453\n",
      "Epoch 3570, Loss: 0.6931471805599453\n",
      "Epoch 3580, Loss: 0.6931471805599453\n",
      "Epoch 3590, Loss: 0.6931471805599453\n",
      "Epoch 3600, Loss: 0.6931471805599453\n",
      "Epoch 3610, Loss: 0.6931471805599453\n",
      "Epoch 3620, Loss: 0.6931471805599453\n",
      "Epoch 3630, Loss: 0.6931471805599453\n",
      "Epoch 3640, Loss: 0.6931471805599453\n",
      "Epoch 3650, Loss: 0.6931471805599453\n",
      "Epoch 3660, Loss: 0.6931471805599453\n",
      "Epoch 3670, Loss: 0.6931471805599453\n",
      "Epoch 3680, Loss: 0.6931471805599453\n",
      "Epoch 3690, Loss: 0.6931471805599453\n",
      "Epoch 3700, Loss: 0.6931471805599453\n",
      "Epoch 3710, Loss: 0.6931471805599453\n",
      "Epoch 3720, Loss: 0.6931471805599453\n",
      "Epoch 3730, Loss: 0.6931471805599453\n",
      "Epoch 3740, Loss: 0.6931471805599453\n",
      "Epoch 3750, Loss: 0.6931471805599453\n",
      "Epoch 3760, Loss: 0.6931471805599453\n",
      "Epoch 3770, Loss: 0.6931471805599453\n",
      "Epoch 3780, Loss: 0.6931471805599453\n",
      "Epoch 3790, Loss: 0.6931471805599453\n",
      "Epoch 3800, Loss: 0.6931471805599453\n",
      "Epoch 3810, Loss: 0.6931471805599453\n",
      "Epoch 3820, Loss: 0.6931471805599453\n",
      "Epoch 3830, Loss: 0.6931471805599453\n",
      "Epoch 3840, Loss: 0.6931471805599453\n",
      "Epoch 3850, Loss: 0.6931471805599453\n",
      "Epoch 3860, Loss: 0.6931471805599453\n",
      "Epoch 3870, Loss: 0.6931471805599453\n",
      "Epoch 3880, Loss: 0.6931471805599453\n",
      "Epoch 3890, Loss: 0.6931471805599453\n",
      "Epoch 3900, Loss: 0.6931471805599453\n",
      "Epoch 3910, Loss: 0.6931471805599453\n",
      "Epoch 3920, Loss: 0.6931471805599453\n",
      "Epoch 3930, Loss: 0.6931471805599453\n",
      "Epoch 3940, Loss: 0.6931471805599453\n",
      "Epoch 3950, Loss: 0.6931471805599453\n",
      "Epoch 3960, Loss: 0.6931471805599453\n",
      "Epoch 3970, Loss: 0.6931471805599453\n",
      "Epoch 3980, Loss: 0.6931471805599453\n",
      "Epoch 3990, Loss: 0.6931471805599453\n",
      "Epoch 4000, Loss: 0.6931471805599453\n",
      "Epoch 4010, Loss: 0.6931471805599453\n",
      "Epoch 4020, Loss: 0.6931471805599453\n",
      "Epoch 4030, Loss: 0.6931471805599453\n",
      "Epoch 4040, Loss: 0.6931471805599453\n",
      "Epoch 4050, Loss: 0.6931471805599453\n",
      "Epoch 4060, Loss: 0.6931471805599453\n",
      "Epoch 4070, Loss: 0.6931471805599453\n",
      "Epoch 4080, Loss: 0.6931471805599453\n",
      "Epoch 4090, Loss: 0.6931471805599453\n",
      "Epoch 4100, Loss: 0.6931471805599453\n",
      "Epoch 4110, Loss: 0.6931471805599453\n",
      "Epoch 4120, Loss: 0.6931471805599453\n",
      "Epoch 4130, Loss: 0.6931471805599453\n",
      "Epoch 4140, Loss: 0.6931471805599453\n",
      "Epoch 4150, Loss: 0.6931471805599453\n",
      "Epoch 4160, Loss: 0.6931471805599453\n",
      "Epoch 4170, Loss: 0.6931471805599453\n",
      "Epoch 4180, Loss: 0.6931471805599453\n",
      "Epoch 4190, Loss: 0.6931471805599453\n",
      "Epoch 4200, Loss: 0.6931471805599453\n",
      "Epoch 4210, Loss: 0.6931471805599453\n",
      "Epoch 4220, Loss: 0.6931471805599453\n",
      "Epoch 4230, Loss: 0.6931471805599453\n",
      "Epoch 4240, Loss: 0.6931471805599453\n",
      "Epoch 4250, Loss: 0.6931471805599453\n",
      "Epoch 4260, Loss: 0.6931471805599453\n",
      "Epoch 4270, Loss: 0.6931471805599453\n",
      "Epoch 4280, Loss: 0.6931471805599453\n",
      "Epoch 4290, Loss: 0.6931471805599453\n",
      "Epoch 4300, Loss: 0.6931471805599453\n",
      "Epoch 4310, Loss: 0.6931471805599453\n",
      "Epoch 4320, Loss: 0.6931471805599453\n",
      "Epoch 4330, Loss: 0.6931471805599453\n",
      "Epoch 4340, Loss: 0.6931471805599453\n",
      "Epoch 4350, Loss: 0.6931471805599453\n",
      "Epoch 4360, Loss: 0.6931471805599453\n",
      "Epoch 4370, Loss: 0.6931471805599453\n",
      "Epoch 4380, Loss: 0.6931471805599453\n",
      "Epoch 4390, Loss: 0.6931471805599453\n",
      "Epoch 4400, Loss: 0.6931471805599453\n",
      "Epoch 4410, Loss: 0.6931471805599453\n",
      "Epoch 4420, Loss: 0.6931471805599453\n",
      "Epoch 4430, Loss: 0.6931471805599453\n",
      "Epoch 4440, Loss: 0.6931471805599453\n",
      "Epoch 4450, Loss: 0.6931471805599453\n",
      "Epoch 4460, Loss: 0.6931471805599453\n",
      "Epoch 4470, Loss: 0.6931471805599453\n",
      "Epoch 4480, Loss: 0.6931471805599453\n",
      "Epoch 4490, Loss: 0.6931471805599453\n",
      "Epoch 4500, Loss: 0.6931471805599453\n",
      "Epoch 4510, Loss: 0.6931471805599453\n",
      "Epoch 4520, Loss: 0.6931471805599453\n",
      "Epoch 4530, Loss: 0.6931471805599453\n",
      "Epoch 4540, Loss: 0.6931471805599453\n",
      "Epoch 4550, Loss: 0.6931471805599453\n",
      "Epoch 4560, Loss: 0.6931471805599453\n",
      "Epoch 4570, Loss: 0.6931471805599453\n",
      "Epoch 4580, Loss: 0.6931471805599453\n",
      "Epoch 4590, Loss: 0.6931471805599453\n",
      "Epoch 4600, Loss: 0.6931471805599453\n",
      "Epoch 4610, Loss: 0.6931471805599453\n",
      "Epoch 4620, Loss: 0.6931471805599453\n",
      "Epoch 4630, Loss: 0.6931471805599453\n",
      "Epoch 4640, Loss: 0.6931471805599453\n",
      "Epoch 4650, Loss: 0.6931471805599453\n",
      "Epoch 4660, Loss: 0.6931471805599453\n",
      "Epoch 4670, Loss: 0.6931471805599453\n",
      "Epoch 4680, Loss: 0.6931471805599453\n",
      "Epoch 4690, Loss: 0.6931471805599453\n",
      "Epoch 4700, Loss: 0.6931471805599453\n",
      "Epoch 4710, Loss: 0.6931471805599453\n",
      "Epoch 4720, Loss: 0.6931471805599453\n",
      "Epoch 4730, Loss: 0.6931471805599453\n",
      "Epoch 4740, Loss: 0.6931471805599453\n",
      "Epoch 4750, Loss: 0.6931471805599453\n",
      "Epoch 4760, Loss: 0.6931471805599453\n",
      "Epoch 4770, Loss: 0.6931471805599453\n",
      "Epoch 4780, Loss: 0.6931471805599453\n",
      "Epoch 4790, Loss: 0.6931471805599453\n",
      "Epoch 4800, Loss: 0.6931471805599453\n",
      "Epoch 4810, Loss: 0.6931471805599453\n",
      "Epoch 4820, Loss: 0.6931471805599453\n",
      "Epoch 4830, Loss: 0.6931471805599453\n",
      "Epoch 4840, Loss: 0.6931471805599453\n",
      "Epoch 4850, Loss: 0.6931471805599453\n",
      "Epoch 4860, Loss: 0.6931471805599453\n",
      "Epoch 4870, Loss: 0.6931471805599453\n",
      "Epoch 4880, Loss: 0.6931471805599453\n",
      "Epoch 4890, Loss: 0.6931471805599453\n",
      "Epoch 4900, Loss: 0.6931471805599453\n",
      "Epoch 4910, Loss: 0.6931471805599453\n",
      "Epoch 4920, Loss: 0.6931471805599453\n",
      "Epoch 4930, Loss: 0.6931471805599453\n",
      "Epoch 4940, Loss: 0.6931471805599453\n",
      "Epoch 4950, Loss: 0.6931471805599453\n",
      "Epoch 4960, Loss: 0.6931471805599453\n",
      "Epoch 4970, Loss: 0.6931471805599453\n",
      "Epoch 4980, Loss: 0.6931471805599453\n",
      "Epoch 4990, Loss: 0.6931471805599453\n",
      "Epoch 5000, Loss: 0.6931471805599453\n",
      "Epoch 5010, Loss: 0.6931471805599453\n",
      "Epoch 5020, Loss: 0.6931471805599453\n",
      "Epoch 5030, Loss: 0.6931471805599453\n",
      "Epoch 5040, Loss: 0.6931471805599453\n",
      "Epoch 5050, Loss: 0.6931471805599453\n",
      "Epoch 5060, Loss: 0.6931471805599453\n",
      "Epoch 5070, Loss: 0.6931471805599453\n",
      "Epoch 5080, Loss: 0.6931471805599453\n",
      "Epoch 5090, Loss: 0.6931471805599453\n",
      "Epoch 5100, Loss: 0.6931471805599453\n",
      "Epoch 5110, Loss: 0.6931471805599453\n",
      "Epoch 5120, Loss: 0.6931471805599453\n",
      "Epoch 5130, Loss: 0.6931471805599453\n",
      "Epoch 5140, Loss: 0.6931471805599453\n",
      "Epoch 5150, Loss: 0.6931471805599453\n",
      "Epoch 5160, Loss: 0.6931471805599453\n",
      "Epoch 5170, Loss: 0.6931471805599453\n",
      "Epoch 5180, Loss: 0.6931471805599453\n",
      "Epoch 5190, Loss: 0.6931471805599453\n",
      "Epoch 5200, Loss: 0.6931471805599453\n",
      "Epoch 5210, Loss: 0.6931471805599453\n",
      "Epoch 5220, Loss: 0.6931471805599453\n",
      "Epoch 5230, Loss: 0.6931471805599453\n",
      "Epoch 5240, Loss: 0.6931471805599453\n",
      "Epoch 5250, Loss: 0.6931471805599453\n",
      "Epoch 5260, Loss: 0.6931471805599453\n",
      "Epoch 5270, Loss: 0.6931471805599453\n",
      "Epoch 5280, Loss: 0.6931471805599453\n",
      "Epoch 5290, Loss: 0.6931471805599453\n",
      "Epoch 5300, Loss: 0.6931471805599453\n",
      "Epoch 5310, Loss: 0.6931471805599453\n",
      "Epoch 5320, Loss: 0.6931471805599453\n",
      "Epoch 5330, Loss: 0.6931471805599453\n",
      "Epoch 5340, Loss: 0.6931471805599453\n",
      "Epoch 5350, Loss: 0.6931471805599453\n",
      "Epoch 5360, Loss: 0.6931471805599453\n",
      "Epoch 5370, Loss: 0.6931471805599453\n",
      "Epoch 5380, Loss: 0.6931471805599453\n",
      "Epoch 5390, Loss: 0.6931471805599453\n",
      "Epoch 5400, Loss: 0.6931471805599453\n",
      "Epoch 5410, Loss: 0.6931471805599453\n",
      "Epoch 5420, Loss: 0.6931471805599453\n",
      "Epoch 5430, Loss: 0.6931471805599453\n",
      "Epoch 5440, Loss: 0.6931471805599453\n",
      "Epoch 5450, Loss: 0.6931471805599453\n",
      "Epoch 5460, Loss: 0.6931471805599453\n",
      "Epoch 5470, Loss: 0.6931471805599453\n",
      "Epoch 5480, Loss: 0.6931471805599453\n",
      "Epoch 5490, Loss: 0.6931471805599453\n",
      "Epoch 5500, Loss: 0.6931471805599453\n",
      "Epoch 5510, Loss: 0.6931471805599453\n",
      "Epoch 5520, Loss: 0.6931471805599453\n",
      "Epoch 5530, Loss: 0.6931471805599453\n",
      "Epoch 5540, Loss: 0.6931471805599453\n",
      "Epoch 5550, Loss: 0.6931471805599453\n",
      "Epoch 5560, Loss: 0.6931471805599453\n",
      "Epoch 5570, Loss: 0.6931471805599453\n",
      "Epoch 5580, Loss: 0.6931471805599453\n",
      "Epoch 5590, Loss: 0.6931471805599453\n",
      "Epoch 5600, Loss: 0.6931471805599453\n",
      "Epoch 5610, Loss: 0.6931471805599453\n",
      "Epoch 5620, Loss: 0.6931471805599453\n",
      "Epoch 5630, Loss: 0.6931471805599453\n",
      "Epoch 5640, Loss: 0.6931471805599453\n",
      "Epoch 5650, Loss: 0.6931471805599453\n",
      "Epoch 5660, Loss: 0.6931471805599453\n",
      "Epoch 5670, Loss: 0.6931471805599453\n",
      "Epoch 5680, Loss: 0.6931471805599453\n",
      "Epoch 5690, Loss: 0.6931471805599453\n",
      "Epoch 5700, Loss: 0.6931471805599453\n",
      "Epoch 5710, Loss: 0.6931471805599453\n",
      "Epoch 5720, Loss: 0.6931471805599453\n",
      "Epoch 5730, Loss: 0.6931471805599453\n",
      "Epoch 5740, Loss: 0.6931471805599453\n",
      "Epoch 5750, Loss: 0.6931471805599453\n",
      "Epoch 5760, Loss: 0.6931471805599453\n",
      "Epoch 5770, Loss: 0.6931471805599453\n",
      "Epoch 5780, Loss: 0.6931471805599453\n",
      "Epoch 5790, Loss: 0.6931471805599453\n",
      "Epoch 5800, Loss: 0.6931471805599453\n",
      "Epoch 5810, Loss: 0.6931471805599453\n",
      "Epoch 5820, Loss: 0.6931471805599453\n",
      "Epoch 5830, Loss: 0.6931471805599453\n",
      "Epoch 5840, Loss: 0.6931471805599453\n",
      "Epoch 5850, Loss: 0.6931471805599453\n",
      "Epoch 5860, Loss: 0.6931471805599453\n",
      "Epoch 5870, Loss: 0.6931471805599453\n",
      "Epoch 5880, Loss: 0.6931471805599453\n",
      "Epoch 5890, Loss: 0.6931471805599453\n",
      "Epoch 5900, Loss: 0.6931471805599453\n",
      "Epoch 5910, Loss: 0.6931471805599453\n",
      "Epoch 5920, Loss: 0.6931471805599453\n",
      "Epoch 5930, Loss: 0.6931471805599453\n",
      "Epoch 5940, Loss: 0.6931471805599453\n",
      "Epoch 5950, Loss: 0.6931471805599453\n",
      "Epoch 5960, Loss: 0.6931471805599453\n",
      "Epoch 5970, Loss: 0.6931471805599453\n",
      "Epoch 5980, Loss: 0.6931471805599453\n",
      "Epoch 5990, Loss: 0.6931471805599453\n",
      "Epoch 6000, Loss: 0.6931471805599453\n",
      "Epoch 6010, Loss: 0.6931471805599453\n",
      "Epoch 6020, Loss: 0.6931471805599453\n",
      "Epoch 6030, Loss: 0.6931471805599453\n",
      "Epoch 6040, Loss: 0.6931471805599453\n",
      "Epoch 6050, Loss: 0.6931471805599453\n",
      "Epoch 6060, Loss: 0.6931471805599453\n",
      "Epoch 6070, Loss: 0.6931471805599453\n",
      "Epoch 6080, Loss: 0.6931471805599453\n",
      "Epoch 6090, Loss: 0.6931471805599453\n",
      "Epoch 6100, Loss: 0.6931471805599453\n",
      "Epoch 6110, Loss: 0.6931471805599453\n",
      "Epoch 6120, Loss: 0.6931471805599453\n",
      "Epoch 6130, Loss: 0.6931471805599453\n",
      "Epoch 6140, Loss: 0.6931471805599453\n",
      "Epoch 6150, Loss: 0.6931471805599453\n",
      "Epoch 6160, Loss: 0.6931471805599453\n",
      "Epoch 6170, Loss: 0.6931471805599453\n",
      "Epoch 6180, Loss: 0.6931471805599453\n",
      "Epoch 6190, Loss: 0.6931471805599453\n",
      "Epoch 6200, Loss: 0.6931471805599453\n",
      "Epoch 6210, Loss: 0.6931471805599453\n",
      "Epoch 6220, Loss: 0.6931471805599453\n",
      "Epoch 6230, Loss: 0.6931471805599453\n",
      "Epoch 6240, Loss: 0.6931471805599453\n",
      "Epoch 6250, Loss: 0.6931471805599453\n",
      "Epoch 6260, Loss: 0.6931471805599453\n",
      "Epoch 6270, Loss: 0.6931471805599453\n",
      "Epoch 6280, Loss: 0.6931471805599453\n",
      "Epoch 6290, Loss: 0.6931471805599453\n",
      "Epoch 6300, Loss: 0.6931471805599453\n",
      "Epoch 6310, Loss: 0.6931471805599453\n",
      "Epoch 6320, Loss: 0.6931471805599453\n",
      "Epoch 6330, Loss: 0.6931471805599453\n",
      "Epoch 6340, Loss: 0.6931471805599453\n",
      "Epoch 6350, Loss: 0.6931471805599453\n",
      "Epoch 6360, Loss: 0.6931471805599453\n",
      "Epoch 6370, Loss: 0.6931471805599453\n",
      "Epoch 6380, Loss: 0.6931471805599453\n",
      "Epoch 6390, Loss: 0.6931471805599453\n",
      "Epoch 6400, Loss: 0.6931471805599453\n",
      "Epoch 6410, Loss: 0.6931471805599453\n",
      "Epoch 6420, Loss: 0.6931471805599453\n",
      "Epoch 6430, Loss: 0.6931471805599453\n",
      "Epoch 6440, Loss: 0.6931471805599453\n",
      "Epoch 6450, Loss: 0.6931471805599453\n",
      "Epoch 6460, Loss: 0.6931471805599453\n",
      "Epoch 6470, Loss: 0.6931471805599453\n",
      "Epoch 6480, Loss: 0.6931471805599453\n",
      "Epoch 6490, Loss: 0.6931471805599453\n",
      "Epoch 6500, Loss: 0.6931471805599453\n",
      "Epoch 6510, Loss: 0.6931471805599453\n",
      "Epoch 6520, Loss: 0.6931471805599453\n",
      "Epoch 6530, Loss: 0.6931471805599453\n",
      "Epoch 6540, Loss: 0.6931471805599453\n",
      "Epoch 6550, Loss: 0.6931471805599453\n",
      "Epoch 6560, Loss: 0.6931471805599453\n",
      "Epoch 6570, Loss: 0.6931471805599453\n",
      "Epoch 6580, Loss: 0.6931471805599453\n",
      "Epoch 6590, Loss: 0.6931471805599453\n",
      "Epoch 6600, Loss: 0.6931471805599453\n",
      "Epoch 6610, Loss: 0.6931471805599453\n",
      "Epoch 6620, Loss: 0.6931471805599453\n",
      "Epoch 6630, Loss: 0.6931471805599453\n",
      "Epoch 6640, Loss: 0.6931471805599453\n",
      "Epoch 6650, Loss: 0.6931471805599453\n",
      "Epoch 6660, Loss: 0.6931471805599453\n",
      "Epoch 6670, Loss: 0.6931471805599453\n",
      "Epoch 6680, Loss: 0.6931471805599453\n",
      "Epoch 6690, Loss: 0.6931471805599453\n",
      "Epoch 6700, Loss: 0.6931471805599453\n",
      "Epoch 6710, Loss: 0.6931471805599453\n",
      "Epoch 6720, Loss: 0.6931471805599453\n",
      "Epoch 6730, Loss: 0.6931471805599453\n",
      "Epoch 6740, Loss: 0.6931471805599453\n",
      "Epoch 6750, Loss: 0.6931471805599453\n",
      "Epoch 6760, Loss: 0.6931471805599453\n",
      "Epoch 6770, Loss: 0.6931471805599453\n",
      "Epoch 6780, Loss: 0.6931471805599453\n",
      "Epoch 6790, Loss: 0.6931471805599453\n",
      "Epoch 6800, Loss: 0.6931471805599453\n",
      "Epoch 6810, Loss: 0.6931471805599453\n",
      "Epoch 6820, Loss: 0.6931471805599453\n",
      "Epoch 6830, Loss: 0.6931471805599453\n",
      "Epoch 6840, Loss: 0.6931471805599453\n",
      "Epoch 6850, Loss: 0.6931471805599453\n",
      "Epoch 6860, Loss: 0.6931471805599453\n",
      "Epoch 6870, Loss: 0.6931471805599453\n",
      "Epoch 6880, Loss: 0.6931471805599453\n",
      "Epoch 6890, Loss: 0.6931471805599453\n",
      "Epoch 6900, Loss: 0.6931471805599453\n",
      "Epoch 6910, Loss: 0.6931471805599453\n",
      "Epoch 6920, Loss: 0.6931471805599453\n",
      "Epoch 6930, Loss: 0.6931471805599453\n",
      "Epoch 6940, Loss: 0.6931471805599453\n",
      "Epoch 6950, Loss: 0.6931471805599453\n",
      "Epoch 6960, Loss: 0.6931471805599453\n",
      "Epoch 6970, Loss: 0.6931471805599453\n",
      "Epoch 6980, Loss: 0.6931471805599453\n",
      "Epoch 6990, Loss: 0.6931471805599453\n",
      "Epoch 7000, Loss: 0.6931471805599453\n",
      "Epoch 7010, Loss: 0.6931471805599453\n",
      "Epoch 7020, Loss: 0.6931471805599453\n",
      "Epoch 7030, Loss: 0.6931471805599453\n",
      "Epoch 7040, Loss: 0.6931471805599453\n",
      "Epoch 7050, Loss: 0.6931471805599453\n",
      "Epoch 7060, Loss: 0.6931471805599453\n",
      "Epoch 7070, Loss: 0.6931471805599453\n",
      "Epoch 7080, Loss: 0.6931471805599453\n",
      "Epoch 7090, Loss: 0.6931471805599453\n",
      "Epoch 7100, Loss: 0.6931471805599453\n",
      "Epoch 7110, Loss: 0.6931471805599453\n",
      "Epoch 7120, Loss: 0.6931471805599453\n",
      "Epoch 7130, Loss: 0.6931471805599453\n",
      "Epoch 7140, Loss: 0.6931471805599453\n",
      "Epoch 7150, Loss: 0.6931471805599453\n",
      "Epoch 7160, Loss: 0.6931471805599453\n",
      "Epoch 7170, Loss: 0.6931471805599453\n",
      "Epoch 7180, Loss: 0.6931471805599453\n",
      "Epoch 7190, Loss: 0.6931471805599453\n",
      "Epoch 7200, Loss: 0.6931471805599453\n",
      "Epoch 7210, Loss: 0.6931471805599453\n",
      "Epoch 7220, Loss: 0.6931471805599453\n",
      "Epoch 7230, Loss: 0.6931471805599453\n",
      "Epoch 7240, Loss: 0.6931471805599453\n",
      "Epoch 7250, Loss: 0.6931471805599453\n",
      "Epoch 7260, Loss: 0.6931471805599453\n",
      "Epoch 7270, Loss: 0.6931471805599453\n",
      "Epoch 7280, Loss: 0.6931471805599453\n",
      "Epoch 7290, Loss: 0.6931471805599453\n",
      "Epoch 7300, Loss: 0.6931471805599453\n",
      "Epoch 7310, Loss: 0.6931471805599453\n",
      "Epoch 7320, Loss: 0.6931471805599453\n",
      "Epoch 7330, Loss: 0.6931471805599453\n",
      "Epoch 7340, Loss: 0.6931471805599453\n",
      "Epoch 7350, Loss: 0.6931471805599453\n",
      "Epoch 7360, Loss: 0.6931471805599453\n",
      "Epoch 7370, Loss: 0.6931471805599453\n",
      "Epoch 7380, Loss: 0.6931471805599453\n",
      "Epoch 7390, Loss: 0.6931471805599453\n",
      "Epoch 7400, Loss: 0.6931471805599453\n",
      "Epoch 7410, Loss: 0.6931471805599453\n",
      "Epoch 7420, Loss: 0.6931471805599453\n",
      "Epoch 7430, Loss: 0.6931471805599453\n",
      "Epoch 7440, Loss: 0.6931471805599453\n",
      "Epoch 7450, Loss: 0.6931471805599453\n",
      "Epoch 7460, Loss: 0.6931471805599453\n",
      "Epoch 7470, Loss: 0.6931471805599453\n",
      "Epoch 7480, Loss: 0.6931471805599453\n",
      "Epoch 7490, Loss: 0.6931471805599453\n",
      "Epoch 7500, Loss: 0.6931471805599453\n",
      "Epoch 7510, Loss: 0.6931471805599453\n",
      "Epoch 7520, Loss: 0.6931471805599453\n",
      "Epoch 7530, Loss: 0.6931471805599453\n",
      "Epoch 7540, Loss: 0.6931471805599453\n",
      "Epoch 7550, Loss: 0.6931471805599453\n",
      "Epoch 7560, Loss: 0.6931471805599453\n",
      "Epoch 7570, Loss: 0.6931471805599453\n",
      "Epoch 7580, Loss: 0.6931471805599453\n",
      "Epoch 7590, Loss: 0.6931471805599453\n",
      "Epoch 7600, Loss: 0.6931471805599453\n",
      "Epoch 7610, Loss: 0.6931471805599453\n",
      "Epoch 7620, Loss: 0.6931471805599453\n",
      "Epoch 7630, Loss: 0.6931471805599453\n",
      "Epoch 7640, Loss: 0.6931471805599453\n",
      "Epoch 7650, Loss: 0.6931471805599453\n",
      "Epoch 7660, Loss: 0.6931471805599453\n",
      "Epoch 7670, Loss: 0.6931471805599453\n",
      "Epoch 7680, Loss: 0.6931471805599453\n",
      "Epoch 7690, Loss: 0.6931471805599453\n",
      "Epoch 7700, Loss: 0.6931471805599453\n",
      "Epoch 7710, Loss: 0.6931471805599453\n",
      "Epoch 7720, Loss: 0.6931471805599453\n",
      "Epoch 7730, Loss: 0.6931471805599453\n",
      "Epoch 7740, Loss: 0.6931471805599453\n",
      "Epoch 7750, Loss: 0.6931471805599453\n",
      "Epoch 7760, Loss: 0.6931471805599453\n",
      "Epoch 7770, Loss: 0.6931471805599453\n",
      "Epoch 7780, Loss: 0.6931471805599453\n",
      "Epoch 7790, Loss: 0.6931471805599453\n",
      "Epoch 7800, Loss: 0.6931471805599453\n",
      "Epoch 7810, Loss: 0.6931471805599453\n",
      "Epoch 7820, Loss: 0.6931471805599453\n",
      "Epoch 7830, Loss: 0.6931471805599453\n",
      "Epoch 7840, Loss: 0.6931471805599453\n",
      "Epoch 7850, Loss: 0.6931471805599453\n",
      "Epoch 7860, Loss: 0.6931471805599453\n",
      "Epoch 7870, Loss: 0.6931471805599453\n",
      "Epoch 7880, Loss: 0.6931471805599453\n",
      "Epoch 7890, Loss: 0.6931471805599453\n",
      "Epoch 7900, Loss: 0.6931471805599453\n",
      "Epoch 7910, Loss: 0.6931471805599453\n",
      "Epoch 7920, Loss: 0.6931471805599453\n",
      "Epoch 7930, Loss: 0.6931471805599453\n",
      "Epoch 7940, Loss: 0.6931471805599453\n",
      "Epoch 7950, Loss: 0.6931471805599453\n",
      "Epoch 7960, Loss: 0.6931471805599453\n",
      "Epoch 7970, Loss: 0.6931471805599453\n",
      "Epoch 7980, Loss: 0.6931471805599453\n",
      "Epoch 7990, Loss: 0.6931471805599453\n",
      "Epoch 8000, Loss: 0.6931471805599453\n",
      "Epoch 8010, Loss: 0.6931471805599453\n",
      "Epoch 8020, Loss: 0.6931471805599453\n",
      "Epoch 8030, Loss: 0.6931471805599453\n",
      "Epoch 8040, Loss: 0.6931471805599453\n",
      "Epoch 8050, Loss: 0.6931471805599453\n",
      "Epoch 8060, Loss: 0.6931471805599453\n",
      "Epoch 8070, Loss: 0.6931471805599453\n",
      "Epoch 8080, Loss: 0.6931471805599453\n",
      "Epoch 8090, Loss: 0.6931471805599453\n",
      "Epoch 8100, Loss: 0.6931471805599453\n",
      "Epoch 8110, Loss: 0.6931471805599453\n",
      "Epoch 8120, Loss: 0.6931471805599453\n",
      "Epoch 8130, Loss: 0.6931471805599453\n",
      "Epoch 8140, Loss: 0.6931471805599453\n",
      "Epoch 8150, Loss: 0.6931471805599453\n",
      "Epoch 8160, Loss: 0.6931471805599453\n",
      "Epoch 8170, Loss: 0.6931471805599453\n",
      "Epoch 8180, Loss: 0.6931471805599453\n",
      "Epoch 8190, Loss: 0.6931471805599453\n",
      "Epoch 8200, Loss: 0.6931471805599453\n",
      "Epoch 8210, Loss: 0.6931471805599453\n",
      "Epoch 8220, Loss: 0.6931471805599453\n",
      "Epoch 8230, Loss: 0.6931471805599453\n",
      "Epoch 8240, Loss: 0.6931471805599453\n",
      "Epoch 8250, Loss: 0.6931471805599453\n",
      "Epoch 8260, Loss: 0.6931471805599453\n",
      "Epoch 8270, Loss: 0.6931471805599453\n",
      "Epoch 8280, Loss: 0.6931471805599453\n",
      "Epoch 8290, Loss: 0.6931471805599453\n",
      "Epoch 8300, Loss: 0.6931471805599453\n",
      "Epoch 8310, Loss: 0.6931471805599453\n",
      "Epoch 8320, Loss: 0.6931471805599453\n",
      "Epoch 8330, Loss: 0.6931471805599453\n",
      "Epoch 8340, Loss: 0.6931471805599453\n",
      "Epoch 8350, Loss: 0.6931471805599453\n",
      "Epoch 8360, Loss: 0.6931471805599453\n",
      "Epoch 8370, Loss: 0.6931471805599453\n",
      "Epoch 8380, Loss: 0.6931471805599453\n",
      "Epoch 8390, Loss: 0.6931471805599453\n",
      "Epoch 8400, Loss: 0.6931471805599453\n",
      "Epoch 8410, Loss: 0.6931471805599453\n",
      "Epoch 8420, Loss: 0.6931471805599453\n",
      "Epoch 8430, Loss: 0.6931471805599453\n",
      "Epoch 8440, Loss: 0.6931471805599453\n",
      "Epoch 8450, Loss: 0.6931471805599453\n",
      "Epoch 8460, Loss: 0.6931471805599453\n",
      "Epoch 8470, Loss: 0.6931471805599453\n",
      "Epoch 8480, Loss: 0.6931471805599453\n",
      "Epoch 8490, Loss: 0.6931471805599453\n",
      "Epoch 8500, Loss: 0.6931471805599453\n",
      "Epoch 8510, Loss: 0.6931471805599453\n",
      "Epoch 8520, Loss: 0.6931471805599453\n",
      "Epoch 8530, Loss: 0.6931471805599453\n",
      "Epoch 8540, Loss: 0.6931471805599453\n",
      "Epoch 8550, Loss: 0.6931471805599453\n",
      "Epoch 8560, Loss: 0.6931471805599453\n",
      "Epoch 8570, Loss: 0.6931471805599453\n",
      "Epoch 8580, Loss: 0.6931471805599453\n",
      "Epoch 8590, Loss: 0.6931471805599453\n",
      "Epoch 8600, Loss: 0.6931471805599453\n",
      "Epoch 8610, Loss: 0.6931471805599453\n",
      "Epoch 8620, Loss: 0.6931471805599453\n",
      "Epoch 8630, Loss: 0.6931471805599453\n",
      "Epoch 8640, Loss: 0.6931471805599453\n",
      "Epoch 8650, Loss: 0.6931471805599453\n",
      "Epoch 8660, Loss: 0.6931471805599453\n",
      "Epoch 8670, Loss: 0.6931471805599453\n",
      "Epoch 8680, Loss: 0.6931471805599453\n",
      "Epoch 8690, Loss: 0.6931471805599453\n",
      "Epoch 8700, Loss: 0.6931471805599453\n",
      "Epoch 8710, Loss: 0.6931471805599453\n",
      "Epoch 8720, Loss: 0.6931471805599453\n",
      "Epoch 8730, Loss: 0.6931471805599453\n",
      "Epoch 8740, Loss: 0.6931471805599453\n",
      "Epoch 8750, Loss: 0.6931471805599453\n",
      "Epoch 8760, Loss: 0.6931471805599453\n",
      "Epoch 8770, Loss: 0.6931471805599453\n",
      "Epoch 8780, Loss: 0.6931471805599453\n",
      "Epoch 8790, Loss: 0.6931471805599453\n",
      "Epoch 8800, Loss: 0.6931471805599453\n",
      "Epoch 8810, Loss: 0.6931471805599453\n",
      "Epoch 8820, Loss: 0.6931471805599453\n",
      "Epoch 8830, Loss: 0.6931471805599453\n",
      "Epoch 8840, Loss: 0.6931471805599453\n",
      "Epoch 8850, Loss: 0.6931471805599453\n",
      "Epoch 8860, Loss: 0.6931471805599453\n",
      "Epoch 8870, Loss: 0.6931471805599453\n",
      "Epoch 8880, Loss: 0.6931471805599453\n",
      "Epoch 8890, Loss: 0.6931471805599453\n",
      "Epoch 8900, Loss: 0.6931471805599453\n",
      "Epoch 8910, Loss: 0.6931471805599453\n",
      "Epoch 8920, Loss: 0.6931471805599453\n",
      "Epoch 8930, Loss: 0.6931471805599453\n",
      "Epoch 8940, Loss: 0.6931471805599453\n",
      "Epoch 8950, Loss: 0.6931471805599453\n",
      "Epoch 8960, Loss: 0.6931471805599453\n",
      "Epoch 8970, Loss: 0.6931471805599453\n",
      "Epoch 8980, Loss: 0.6931471805599453\n",
      "Epoch 8990, Loss: 0.6931471805599453\n",
      "Epoch 9000, Loss: 0.6931471805599453\n",
      "Epoch 9010, Loss: 0.6931471805599453\n",
      "Epoch 9020, Loss: 0.6931471805599453\n",
      "Epoch 9030, Loss: 0.6931471805599453\n",
      "Epoch 9040, Loss: 0.6931471805599453\n",
      "Epoch 9050, Loss: 0.6931471805599453\n",
      "Epoch 9060, Loss: 0.6931471805599453\n",
      "Epoch 9070, Loss: 0.6931471805599453\n",
      "Epoch 9080, Loss: 0.6931471805599453\n",
      "Epoch 9090, Loss: 0.6931471805599453\n",
      "Epoch 9100, Loss: 0.6931471805599453\n",
      "Epoch 9110, Loss: 0.6931471805599453\n",
      "Epoch 9120, Loss: 0.6931471805599453\n",
      "Epoch 9130, Loss: 0.6931471805599453\n",
      "Epoch 9140, Loss: 0.6931471805599453\n",
      "Epoch 9150, Loss: 0.6931471805599453\n",
      "Epoch 9160, Loss: 0.6931471805599453\n",
      "Epoch 9170, Loss: 0.6931471805599453\n",
      "Epoch 9180, Loss: 0.6931471805599453\n",
      "Epoch 9190, Loss: 0.6931471805599453\n",
      "Epoch 9200, Loss: 0.6931471805599453\n",
      "Epoch 9210, Loss: 0.6931471805599453\n",
      "Epoch 9220, Loss: 0.6931471805599453\n",
      "Epoch 9230, Loss: 0.6931471805599453\n",
      "Epoch 9240, Loss: 0.6931471805599453\n",
      "Epoch 9250, Loss: 0.6931471805599453\n",
      "Epoch 9260, Loss: 0.6931471805599453\n",
      "Epoch 9270, Loss: 0.6931471805599453\n",
      "Epoch 9280, Loss: 0.6931471805599453\n",
      "Epoch 9290, Loss: 0.6931471805599453\n",
      "Epoch 9300, Loss: 0.6931471805599453\n",
      "Epoch 9310, Loss: 0.6931471805599453\n",
      "Epoch 9320, Loss: 0.6931471805599453\n",
      "Epoch 9330, Loss: 0.6931471805599453\n",
      "Epoch 9340, Loss: 0.6931471805599453\n",
      "Epoch 9350, Loss: 0.6931471805599453\n",
      "Epoch 9360, Loss: 0.6931471805599453\n",
      "Epoch 9370, Loss: 0.6931471805599453\n",
      "Epoch 9380, Loss: 0.6931471805599453\n",
      "Epoch 9390, Loss: 0.6931471805599453\n",
      "Epoch 9400, Loss: 0.6931471805599453\n",
      "Epoch 9410, Loss: 0.6931471805599453\n",
      "Epoch 9420, Loss: 0.6931471805599453\n",
      "Epoch 9430, Loss: 0.6931471805599453\n",
      "Epoch 9440, Loss: 0.6931471805599453\n",
      "Epoch 9450, Loss: 0.6931471805599453\n",
      "Epoch 9460, Loss: 0.6931471805599453\n",
      "Epoch 9470, Loss: 0.6931471805599453\n",
      "Epoch 9480, Loss: 0.6931471805599453\n",
      "Epoch 9490, Loss: 0.6931471805599453\n",
      "Epoch 9500, Loss: 0.6931471805599453\n",
      "Epoch 9510, Loss: 0.6931471805599453\n",
      "Epoch 9520, Loss: 0.6931471805599453\n",
      "Epoch 9530, Loss: 0.6931471805599453\n",
      "Epoch 9540, Loss: 0.6931471805599453\n",
      "Epoch 9550, Loss: 0.6931471805599453\n",
      "Epoch 9560, Loss: 0.6931471805599453\n",
      "Epoch 9570, Loss: 0.6931471805599453\n",
      "Epoch 9580, Loss: 0.6931471805599453\n",
      "Epoch 9590, Loss: 0.6931471805599453\n",
      "Epoch 9600, Loss: 0.6931471805599453\n",
      "Epoch 9610, Loss: 0.6931471805599453\n",
      "Epoch 9620, Loss: 0.6931471805599453\n",
      "Epoch 9630, Loss: 0.6931471805599453\n",
      "Epoch 9640, Loss: 0.6931471805599453\n",
      "Epoch 9650, Loss: 0.6931471805599453\n",
      "Epoch 9660, Loss: 0.6931471805599453\n",
      "Epoch 9670, Loss: 0.6931471805599453\n",
      "Epoch 9680, Loss: 0.6931471805599453\n",
      "Epoch 9690, Loss: 0.6931471805599453\n",
      "Epoch 9700, Loss: 0.6931471805599453\n",
      "Epoch 9710, Loss: 0.6931471805599453\n",
      "Epoch 9720, Loss: 0.6931471805599453\n",
      "Epoch 9730, Loss: 0.6931471805599453\n",
      "Epoch 9740, Loss: 0.6931471805599453\n",
      "Epoch 9750, Loss: 0.6931471805599453\n",
      "Epoch 9760, Loss: 0.6931471805599453\n",
      "Epoch 9770, Loss: 0.6931471805599453\n",
      "Epoch 9780, Loss: 0.6931471805599453\n",
      "Epoch 9790, Loss: 0.6931471805599453\n",
      "Epoch 9800, Loss: 0.6931471805599453\n",
      "Epoch 9810, Loss: 0.6931471805599453\n",
      "Epoch 9820, Loss: 0.6931471805599453\n",
      "Epoch 9830, Loss: 0.6931471805599453\n",
      "Epoch 9840, Loss: 0.6931471805599453\n",
      "Epoch 9850, Loss: 0.6931471805599453\n",
      "Epoch 9860, Loss: 0.6931471805599453\n",
      "Epoch 9870, Loss: 0.6931471805599453\n",
      "Epoch 9880, Loss: 0.6931471805599453\n",
      "Epoch 9890, Loss: 0.6931471805599453\n",
      "Epoch 9900, Loss: 0.6931471805599453\n",
      "Epoch 9910, Loss: 0.6931471805599453\n",
      "Epoch 9920, Loss: 0.6931471805599453\n",
      "Epoch 9930, Loss: 0.6931471805599453\n",
      "Epoch 9940, Loss: 0.6931471805599453\n",
      "Epoch 9950, Loss: 0.6931471805599453\n",
      "Epoch 9960, Loss: 0.6931471805599453\n",
      "Epoch 9970, Loss: 0.6931471805599453\n",
      "Epoch 9980, Loss: 0.6931471805599453\n",
      "Epoch 9990, Loss: 0.6931471805599453\n",
      "{'linear_1_linear_1_weight': Tensor(data=[[ 0.17037155 -0.04742426]\n",
      " [ 0.22215534  0.52239493]\n",
      " [-0.08031394 -0.08030831]\n",
      " [ 0.54166553  0.26322794]\n",
      " [-0.16102839  0.18609656]\n",
      " [-0.15895096 -0.15974399]\n",
      " [ 0.08299237 -0.65624971]\n",
      " [-0.59164194 -0.19286303]\n",
      " [-0.34739821  0.10778595]\n",
      " [-0.31144969 -0.48441617]\n",
      " [ 0.50271338 -0.07744063]\n",
      " [ 0.02316198 -0.4886846 ]\n",
      " [-0.18672173  0.03804613]\n",
      " [-0.39478754  0.12886336]\n",
      " [-0.20601737 -0.10005013]], shape=(15, 2), dtype=float64), 'linear_1_linear_1_bias': Tensor(data=[-0.60170661  1.85227818], shape=(2,), dtype=float64), 'linear_2_linear_1_fc1_weight': Tensor(data=[[ 0.5444753 ]\n",
      " [-0.22224937]\n",
      " [-0.83456618]\n",
      " [-0.38590954]\n",
      " [ 0.83065087]\n",
      " [-0.16712355]\n",
      " [-0.15892345]], shape=(7, 1), dtype=float64), 'linear_2_linear_1_fc1_bias': Tensor(data=[0.31776703], shape=(1,), dtype=float64), 'layer_norm_1_layernorm_1_gamma': Tensor(data=[1.], shape=(1,), dtype=float64), 'layer_norm_1_layernorm_1_beta': Tensor(data=[0.], shape=(1,), dtype=float64)}\n"
     ]
    }
   ],
   "source": [
    "net = Net()\n",
    "\n",
    "net._build(X.shape)\n",
    "scheduler = LRScheduler(warmup_steps=1000, total_steps=10000, min_lr=1e-5, max_lr=3e-4, final_lr=1e-6)\n",
    "optimizer = AdamW(net.parameters(), lr=scheduler, clip_norm=100.0)\n",
    "\n",
    "net.train(X, y, optimizer, num_epochs=10000)\n",
    "print(net.parameters())\n",
    "\n",
    "net.save_checkpoint(optimizer, \"../../../checkpoints/simple_linear_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "563e2641",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Architecture:\n",
      "  linear_0 (linear):\n",
      "    fc1 (linear):\n",
      "      linear_1_linear_1_fc1_weight: shape=(7, 1), dtype=float64\n",
      "      linear_1_linear_1_fc1_bias: shape=(1,), dtype=float64\n",
      "  layer_norm_0 (layer_norm):\n",
      "    None (layernorm):\n",
      "      layer_norm_1_layernorm_1_gamma: shape=(1,), dtype=float64\n",
      "      layer_norm_1_layernorm_1_beta: shape=(1,), dtype=float64\n",
      "  linear_1 (linear):\n",
      "    fc1 (linear):\n",
      "      linear_2_linear_1_fc1_weight: shape=(7, 1), dtype=float64\n",
      "      linear_2_linear_1_fc1_bias: shape=(1,), dtype=float64\n",
      "  layer_norm_1 (layer_norm):\n",
      "    None (layernorm):\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../../../checkpoints/simple_linear_model/model/linear_2_linear_1_fc1_weight.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m scheduler = LRScheduler(warmup_steps=\u001b[32m1000\u001b[39m, total_steps=\u001b[32m10000\u001b[39m, min_lr=\u001b[32m1e-5\u001b[39m, max_lr=\u001b[32m3e-4\u001b[39m, final_lr=\u001b[32m1e-6\u001b[39m)\n\u001b[32m      5\u001b[39m optimizer = AdamW(new_net.parameters(), lr=scheduler, clip_norm=\u001b[32m100.0\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[43mnew_net\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m../../../checkpoints/simple_linear_model\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(new_net.parameters())\n\u001b[32m      8\u001b[39m new_net.train(X, y, optimizer, \u001b[32m1000\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ml-projects\\transformer\\src\\experiments\\notebooks\\../../..\\src\\core\\module.py:42\u001b[39m, in \u001b[36mModule.load_checkpoint\u001b[39m\u001b[34m(self, optimizer, path)\u001b[39m\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_checkpoint\u001b[39m(\u001b[38;5;28mself\u001b[39m, optimizer, path):\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m     \u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_load_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ml-projects\\transformer\\src\\experiments\\notebooks\\../../..\\src\\core\\optim.py:109\u001b[39m, in \u001b[36mAdamW._load_state\u001b[39m\u001b[34m(self, path)\u001b[39m\n\u001b[32m    108\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_load_state\u001b[39m(\u001b[38;5;28mself\u001b[39m, path):\n\u001b[32m--> \u001b[39m\u001b[32m109\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_load_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    110\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m name, param \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.params.items():\n\u001b[32m    111\u001b[39m         param[\u001b[33m'\u001b[39m\u001b[33mm_t\u001b[39m\u001b[33m'\u001b[39m] = xp.array(np.load(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/optim/m_t/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.npy\u001b[39m\u001b[33m\"\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ml-projects\\transformer\\src\\experiments\\notebooks\\../../..\\src\\core\\optim.py:77\u001b[39m, in \u001b[36mOptimizer._load_params\u001b[39m\u001b[34m(self, path)\u001b[39m\n\u001b[32m     74\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_load_params\u001b[39m(\u001b[38;5;28mself\u001b[39m, path):\n\u001b[32m     75\u001b[39m     \u001b[38;5;66;03m# ONLY CALL WITHIN THE SPECIFIC OPTIMIZER (AdamW, Standard, etc.)\u001b[39;00m\n\u001b[32m     76\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m name, param \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.params.items():\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m         param[\u001b[33m'\u001b[39m\u001b[33mparam\u001b[39m\u001b[33m'\u001b[39m].data = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mpath\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/model/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mname\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m.npy\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\luequ\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\lib\\_npyio_impl.py:454\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[39m\n\u001b[32m    452\u001b[39m     own_fid = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m454\u001b[39m     fid = stack.enter_context(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[32m    455\u001b[39m     own_fid = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    457\u001b[39m \u001b[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '../../../checkpoints/simple_linear_model/model/linear_2_linear_1_fc1_weight.npy'"
     ]
    }
   ],
   "source": [
    "new_net = Net()\n",
    "print(new_net)\n",
    "new_net._build(X.shape)\n",
    "scheduler = LRScheduler(warmup_steps=1000, total_steps=10000, min_lr=1e-5, max_lr=3e-4, final_lr=1e-6)\n",
    "optimizer = AdamW(new_net.parameters(), lr=scheduler, clip_norm=100.0)\n",
    "new_net.load_checkpoint(optimizer, \"../../../checkpoints/simple_linear_model\")\n",
    "print(new_net.parameters())\n",
    "new_net.train(X, y, optimizer, 1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62cf290c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
