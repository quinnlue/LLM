{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba1a9adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.append(\"../../../\")\n",
    "\n",
    "from src.core.module import Module\n",
    "from src.core.losses import CrossEntropyWithLogits\n",
    "from src.core.optim import AdamW\n",
    "from src.core.tensor import Tensor\n",
    "from src.utils.backend import xp\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "\n",
    "src = np.random.randint(low=0, high=16, size=(15, 15))\n",
    "x = src[:, :-1]\n",
    "y = src[:, 1:]\n",
    "\n",
    "x_mine = Tensor(x, requires_grad=False)\n",
    "y_mine = Tensor(y, requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49bfc84e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 5.42578125\n",
      "Epoch 5, Loss: 2.517578125\n",
      "Epoch 10, Loss: 2.068359375\n",
      "Epoch 15, Loss: 1.69140625\n",
      "Epoch 20, Loss: 1.30078125\n",
      "Epoch 25, Loss: 0.880859375\n",
      "Epoch 30, Loss: 0.56982421875\n",
      "Epoch 35, Loss: 0.412841796875\n",
      "Epoch 40, Loss: 0.340576171875\n",
      "Epoch 45, Loss: 0.274169921875\n",
      "Epoch 50, Loss: 0.201171875\n",
      "Epoch 55, Loss: 0.1375732421875\n",
      "Epoch 60, Loss: 0.158447265625\n",
      "Epoch 65, Loss: 0.1175537109375\n",
      "Epoch 70, Loss: 0.091796875\n",
      "Epoch 75, Loss: 0.08978271484375\n",
      "Epoch 80, Loss: 0.079833984375\n",
      "Epoch 85, Loss: 0.09771728515625\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 55\u001b[39m\n\u001b[32m     51\u001b[39m model._build((\u001b[32m15\u001b[39m, \u001b[32m15\u001b[39m))\n\u001b[32m     52\u001b[39m optimizer = AdamW(model.parameters(), lr=\u001b[32m0.01\u001b[39m, precision=(xp.float32, xp.float32), clip_norm=\u001b[32m1.0\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_mine\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_mine\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 37\u001b[39m, in \u001b[36mNet.train\u001b[39m\u001b[34m(self, x, y, epochs, optimizer)\u001b[39m\n\u001b[32m     34\u001b[39m \u001b[38;5;66;03m# print(y_hat.shape, y.shape)\u001b[39;00m\n\u001b[32m     35\u001b[39m loss = CrossEntropyWithLogits(y_hat, y, axis=-\u001b[32m1\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     38\u001b[39m optimizer.step()\n\u001b[32m     39\u001b[39m optimizer.zero_grad()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ml-projects\\transformer\\src\\experiments\\notebooks\\../../..\\src\\core\\tensor.py:423\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, grad, _visited)\u001b[39m\n\u001b[32m    420\u001b[39m \u001b[38;5;28mself\u001b[39m.parents = ()\n\u001b[32m    422\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m parent, g \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(parents_local, grads):\n\u001b[32m--> \u001b[39m\u001b[32m423\u001b[39m     \u001b[43mparent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_visited\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ml-projects\\transformer\\src\\experiments\\notebooks\\../../..\\src\\core\\tensor.py:423\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, grad, _visited)\u001b[39m\n\u001b[32m    420\u001b[39m \u001b[38;5;28mself\u001b[39m.parents = ()\n\u001b[32m    422\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m parent, g \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(parents_local, grads):\n\u001b[32m--> \u001b[39m\u001b[32m423\u001b[39m     \u001b[43mparent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_visited\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[31m[... skipping similar frames: Tensor.backward at line 423 (15 times)]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ml-projects\\transformer\\src\\experiments\\notebooks\\../../..\\src\\core\\tensor.py:423\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, grad, _visited)\u001b[39m\n\u001b[32m    420\u001b[39m \u001b[38;5;28mself\u001b[39m.parents = ()\n\u001b[32m    422\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m parent, g \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(parents_local, grads):\n\u001b[32m--> \u001b[39m\u001b[32m423\u001b[39m     \u001b[43mparent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_visited\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ml-projects\\transformer\\src\\experiments\\notebooks\\../../..\\src\\core\\tensor.py:417\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, grad, _visited)\u001b[39m\n\u001b[32m    415\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.grad_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    416\u001b[39m     parents_local = \u001b[38;5;28mself\u001b[39m.parents\n\u001b[32m--> \u001b[39m\u001b[32m417\u001b[39m     grads = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgrad_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    419\u001b[39m     \u001b[38;5;28mself\u001b[39m.grad_fn = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    420\u001b[39m     \u001b[38;5;28mself\u001b[39m.parents = ()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ml-projects\\transformer\\src\\experiments\\notebooks\\../../..\\src\\core\\tensor.py:182\u001b[39m, in \u001b[36mTensor.__matmul__.<locals>.grad_fn\u001b[39m\u001b[34m(grad)\u001b[39m\n\u001b[32m    177\u001b[39m \u001b[38;5;66;03m# For grad_other: self_T @ grad\u001b[39;00m\n\u001b[32m    178\u001b[39m \u001b[38;5;66;03m# Need to handle broadcasting by summing over batch dimensions\u001b[39;00m\n\u001b[32m    179\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.data.ndim > \u001b[32m2\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m other.data.ndim == \u001b[32m2\u001b[39m:\n\u001b[32m    180\u001b[39m     \u001b[38;5;66;03m# Case: (batch, ..., in) @ (in, out) -> (batch, ..., out)\u001b[39;00m\n\u001b[32m    181\u001b[39m     \u001b[38;5;66;03m# grad_other should be (in, out), so sum over batch dimensions\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m182\u001b[39m     grad_other = \u001b[43mxp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mself_T\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    183\u001b[39m     \u001b[38;5;66;03m# Sum over all dimensions except the last two\u001b[39;00m\n\u001b[32m    184\u001b[39m     sum_axes = \u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mrange\u001b[39m(grad_other.ndim - \u001b[32m2\u001b[39m))\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "class Net(Module):\n",
    "    def __init__(self, d_model, n_heads, vocab_size, max_seq_len, pad_idx=0):\n",
    "        super().__init__()\n",
    "\n",
    "        self.e = self.embedding(vocab_size, d_model, max_seq_len, pad_idx, name=\"Embedding\")\n",
    "\n",
    "        self.head1 = self.transformer(d_model=d_model, n_heads=n_heads)\n",
    "        self.head2 = self.transformer(d_model=d_model, n_heads=n_heads)\n",
    "        self.head3 = self.transformer(d_model=d_model, n_heads=n_heads)\n",
    "        self.head4 = self.transformer(d_model=d_model, n_heads=n_heads)\n",
    "        self.head5 = self.transformer(d_model=d_model, n_heads=n_heads)\n",
    "        self.head6 = self.transformer(d_model=d_model, n_heads=n_heads)\n",
    "        self.head7 = self.transformer(d_model=d_model, n_heads=n_heads)\n",
    "        self.head8 = self.transformer(d_model=d_model, n_heads=n_heads)\n",
    "        self.project = self.linear(d_model, vocab_size, name=\"project\")\n",
    "    \n",
    "    def forward(self, idx):\n",
    "        x, padding_mask = self.e.get_sentence_embedding(idx)\n",
    "        x = Tensor(x.data, requires_grad=False)\n",
    "        x = self.head1(x, padding_mask)\n",
    "        x = self.head2(x, padding_mask)\n",
    "        x = self.head3(x, padding_mask)\n",
    "        x = self.head4(x, padding_mask)\n",
    "        x = self.head5(x, padding_mask)\n",
    "        x = self.head6(x, padding_mask)\n",
    "        x = self.head7(x, padding_mask)\n",
    "        x = self.head8(x, padding_mask)\n",
    "        x = self.project(x)\n",
    "        return x\n",
    "\n",
    "    def train(self, x, y, epochs, optimizer):\n",
    "        for epoch in range(epochs):\n",
    "            y_hat = self.forward(x)\n",
    "            # print(y_hat.shape, y.shape)\n",
    "            loss = CrossEntropyWithLogits(y_hat, y, axis=-1)\n",
    "    \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            if epoch % 5 == 0:\n",
    "                print(f\"Epoch {epoch}, Loss: {loss.data}\")\n",
    "                \n",
    "if __name__ == \"__main__\":\n",
    "    D_MODEL = 16\n",
    "    VOCAB_SIZE = 20\n",
    "    N_HEADS = 2\n",
    "    MAX_SEQ_LEN = 16\n",
    "    PAD_IDX = 0\n",
    "\n",
    "    model = Net(d_model=D_MODEL, n_heads=N_HEADS, vocab_size=VOCAB_SIZE, max_seq_len=MAX_SEQ_LEN, pad_idx=PAD_IDX)\n",
    "    model._build((15, 15))\n",
    "    optimizer = AdamW(model.parameters(), lr=0.01, precision=(xp.float32, xp.float32), clip_norm=1.0)\n",
    "\n",
    "\n",
    "    model.train(x_mine, y_mine, epochs=1000, optimizer=optimizer)\n",
    "\n",
    "\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1945fa1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\luequ\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:382: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn was passed bias=False\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 3.0786\n",
      "Epoch 50, Loss: 3.0600\n",
      "Epoch 100, Loss: 3.0440\n",
      "Epoch 150, Loss: 3.0137\n",
      "Epoch 200, Loss: 2.9994\n",
      "Epoch 250, Loss: 2.9729\n",
      "Epoch 300, Loss: 2.9510\n",
      "Epoch 350, Loss: 2.9523\n",
      "Epoch 400, Loss: 2.9230\n",
      "Epoch 450, Loss: 2.9120\n",
      "Epoch 500, Loss: 2.9084\n",
      "Epoch 550, Loss: 2.8781\n",
      "Epoch 600, Loss: 2.8804\n",
      "Epoch 650, Loss: 2.8725\n",
      "Epoch 700, Loss: 2.8471\n",
      "Epoch 750, Loss: 2.8459\n",
      "Epoch 800, Loss: 2.8414\n",
      "Epoch 850, Loss: 2.8311\n",
      "Epoch 900, Loss: 2.8118\n",
      "Epoch 950, Loss: 2.8179\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, vocab_size, max_seq_len, num_layers=1, pad_idx=0):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model, padding_idx=pad_idx)\n",
    "        self.pos_embed = nn.Embedding(max_seq_len, d_model)\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, \n",
    "            nhead=n_heads, \n",
    "            dim_feedforward=d_model * 4, \n",
    "            batch_first=True,\n",
    "            bias=False,\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.project = nn.Linear(d_model, vocab_size, bias=False)\n",
    "\n",
    "    def forward(self, idx):\n",
    "        batch_size, seq_len = idx.size()\n",
    "        pos = torch.arange(seq_len, device=idx.device).unsqueeze(0).expand(batch_size, seq_len)\n",
    "        \n",
    "        x = self.embedding(idx) + self.pos_embed(pos)\n",
    "        padding_mask = (idx == 0)\n",
    "        x = self.encoder(x, src_key_padding_mask=padding_mask)\n",
    "        logits = self.project(x)\n",
    "        return logits\n",
    "\n",
    "    def train_model(self, x, y, epochs, optimizer, criterion):\n",
    "        for epoch in range(epochs):\n",
    "            optimizer.zero_grad()\n",
    "            logits = self.forward(x)\n",
    "            loss = criterion(logits.view(-1, logits.size(-1)), y.view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if epoch %  50== 0:\n",
    "                print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "# --- Config ---\n",
    "D_MODEL = 16\n",
    "VOCAB_SIZE = 20\n",
    "N_HEADS = 2\n",
    "MAX_SEQ_LEN = 16\n",
    "PAD_IDX = 0\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "# --- Model Training ---\n",
    "model = Net(D_MODEL, N_HEADS, VOCAB_SIZE, MAX_SEQ_LEN, num_layers=8, pad_idx=PAD_IDX)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "\n",
    "x_pt = torch.tensor(x).long()\n",
    "y_pt = torch.tensor(y).long()\n",
    "\n",
    "model.train_model(x_pt, y_pt, epochs=1000, optimizer=optimizer, criterion=criterion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7b1b2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
