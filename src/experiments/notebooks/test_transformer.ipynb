{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba1a9adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.append(\"../../../\")\n",
    "\n",
    "from src.core.module import Module\n",
    "from src.core.losses import CrossEntropyWithLogits\n",
    "from src.core.optim import AdamW\n",
    "from src.core.tensor import Tensor\n",
    "from src.utils.backend import xp\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "\n",
    "src = np.random.randint(low=0, high=16, size=(15, 15))\n",
    "x = src[:, :-1]\n",
    "y = src[:, 1:]\n",
    "\n",
    "x_mine = Tensor(x, requires_grad=False)\n",
    "y_mine = Tensor(y, requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49bfc84e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 6.875\n",
      "Epoch 5, Loss: 5.01953125\n",
      "Epoch 10, Loss: 4.08984375\n",
      "Epoch 15, Loss: 3.583984375\n",
      "Epoch 20, Loss: 3.2890625\n",
      "Epoch 25, Loss: 2.974609375\n",
      "Epoch 30, Loss: 2.73828125\n",
      "Epoch 35, Loss: 2.55859375\n",
      "Epoch 40, Loss: 2.419921875\n",
      "Epoch 45, Loss: 2.294921875\n",
      "Epoch 50, Loss: 2.18359375\n",
      "Epoch 55, Loss: 2.09375\n",
      "Epoch 60, Loss: 2.001953125\n",
      "Epoch 65, Loss: 1.92578125\n",
      "Epoch 70, Loss: 1.8486328125\n",
      "Epoch 75, Loss: 1.7744140625\n",
      "Epoch 80, Loss: 1.705078125\n",
      "Epoch 85, Loss: 1.6376953125\n",
      "Epoch 90, Loss: 1.572265625\n",
      "Epoch 95, Loss: 1.505859375\n",
      "Epoch 100, Loss: 1.44140625\n",
      "Epoch 105, Loss: 1.375\n",
      "Epoch 110, Loss: 1.3056640625\n",
      "Epoch 115, Loss: 1.2373046875\n",
      "Epoch 120, Loss: 1.1669921875\n",
      "Epoch 125, Loss: 1.095703125\n",
      "Epoch 130, Loss: 1.025390625\n",
      "Epoch 135, Loss: 0.95556640625\n",
      "Epoch 140, Loss: 0.88720703125\n",
      "Epoch 145, Loss: 0.82177734375\n",
      "Epoch 150, Loss: 0.7568359375\n",
      "Epoch 155, Loss: 0.6953125\n",
      "Epoch 160, Loss: 0.63671875\n",
      "Epoch 165, Loss: 0.58056640625\n",
      "Epoch 170, Loss: 0.5263671875\n",
      "Epoch 175, Loss: 0.476806640625\n",
      "Epoch 180, Loss: 0.42919921875\n",
      "Epoch 185, Loss: 0.38525390625\n",
      "Epoch 190, Loss: 0.3447265625\n",
      "Epoch 195, Loss: 0.30810546875\n",
      "Epoch 200, Loss: 0.274658203125\n",
      "Epoch 205, Loss: 0.245361328125\n",
      "Epoch 210, Loss: 0.2193603515625\n",
      "Epoch 215, Loss: 0.196533203125\n",
      "Epoch 220, Loss: 0.1763916015625\n",
      "Epoch 225, Loss: 0.15869140625\n",
      "Epoch 230, Loss: 0.14306640625\n",
      "Epoch 235, Loss: 0.1297607421875\n",
      "Epoch 240, Loss: 0.1182861328125\n",
      "Epoch 245, Loss: 0.10845947265625\n",
      "Epoch 250, Loss: 0.10003662109375\n",
      "Epoch 255, Loss: 0.0927734375\n",
      "Epoch 260, Loss: 0.08642578125\n",
      "Epoch 265, Loss: 0.08099365234375\n",
      "Epoch 270, Loss: 0.0762939453125\n",
      "Epoch 275, Loss: 0.07220458984375\n",
      "Epoch 280, Loss: 0.0684814453125\n",
      "Epoch 285, Loss: 0.0653076171875\n",
      "Epoch 290, Loss: 0.06256103515625\n",
      "Epoch 295, Loss: 0.05999755859375\n",
      "Epoch 300, Loss: 0.057769775390625\n",
      "Epoch 305, Loss: 0.05572509765625\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 55\u001b[39m\n\u001b[32m     51\u001b[39m model._build((\u001b[32m15\u001b[39m, \u001b[32m15\u001b[39m))\n\u001b[32m     52\u001b[39m optimizer = AdamW(model.parameters(), lr=\u001b[32m0.001\u001b[39m, precision=(xp.float32, xp.float32))\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_mine\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_mine\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 37\u001b[39m, in \u001b[36mNet.train\u001b[39m\u001b[34m(self, x, y, epochs, optimizer)\u001b[39m\n\u001b[32m     34\u001b[39m \u001b[38;5;66;03m# print(y_hat.shape, y.shape)\u001b[39;00m\n\u001b[32m     35\u001b[39m loss = CrossEntropyWithLogits(y_hat, y, axis=-\u001b[32m1\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     38\u001b[39m optimizer.step()\n\u001b[32m     39\u001b[39m optimizer.zero_grad()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ml-projects\\transformer\\src\\experiments\\notebooks\\../../..\\src\\core\\tensor.py:429\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, grad, _visited)\u001b[39m\n\u001b[32m    426\u001b[39m \u001b[38;5;28mself\u001b[39m.parents = ()\n\u001b[32m    428\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m parent, g \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(parents_local, grads):\n\u001b[32m--> \u001b[39m\u001b[32m429\u001b[39m     \u001b[43mparent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_visited\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ml-projects\\transformer\\src\\experiments\\notebooks\\../../..\\src\\core\\tensor.py:429\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, grad, _visited)\u001b[39m\n\u001b[32m    426\u001b[39m \u001b[38;5;28mself\u001b[39m.parents = ()\n\u001b[32m    428\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m parent, g \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(parents_local, grads):\n\u001b[32m--> \u001b[39m\u001b[32m429\u001b[39m     \u001b[43mparent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_visited\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[31m[... skipping similar frames: Tensor.backward at line 429 (18 times)]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ml-projects\\transformer\\src\\experiments\\notebooks\\../../..\\src\\core\\tensor.py:429\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, grad, _visited)\u001b[39m\n\u001b[32m    426\u001b[39m \u001b[38;5;28mself\u001b[39m.parents = ()\n\u001b[32m    428\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m parent, g \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(parents_local, grads):\n\u001b[32m--> \u001b[39m\u001b[32m429\u001b[39m     \u001b[43mparent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_visited\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ml-projects\\transformer\\src\\experiments\\notebooks\\../../..\\src\\core\\tensor.py:422\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, grad, _visited)\u001b[39m\n\u001b[32m    418\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.grad_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    419\u001b[39m     \u001b[38;5;66;03m# Save a local copy, then immediately sever the links so the\u001b[39;00m\n\u001b[32m    420\u001b[39m     \u001b[38;5;66;03m# graph from this Tensor backward is eligible for GC.\u001b[39;00m\n\u001b[32m    421\u001b[39m     parents_local = \u001b[38;5;28mself\u001b[39m.parents\n\u001b[32m--> \u001b[39m\u001b[32m422\u001b[39m     grads = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgrad_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    424\u001b[39m     \u001b[38;5;66;03m# *******  CRUCIAL: break reference cycles  *******\u001b[39;00m\n\u001b[32m    425\u001b[39m     \u001b[38;5;28mself\u001b[39m.grad_fn = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ml-projects\\transformer\\src\\experiments\\notebooks\\../../..\\src\\core\\tensor.py:182\u001b[39m, in \u001b[36mTensor.__matmul__.<locals>.grad_fn\u001b[39m\u001b[34m(grad)\u001b[39m\n\u001b[32m    177\u001b[39m \u001b[38;5;66;03m# For grad_other: self_T @ grad\u001b[39;00m\n\u001b[32m    178\u001b[39m \u001b[38;5;66;03m# Need to handle broadcasting by summing over batch dimensions\u001b[39;00m\n\u001b[32m    179\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.data.ndim > \u001b[32m2\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m other.data.ndim == \u001b[32m2\u001b[39m:\n\u001b[32m    180\u001b[39m     \u001b[38;5;66;03m# Case: (batch, ..., in) @ (in, out) -> (batch, ..., out)\u001b[39;00m\n\u001b[32m    181\u001b[39m     \u001b[38;5;66;03m# grad_other should be (in, out), so sum over batch dimensions\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m182\u001b[39m     grad_other = \u001b[43mxp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mself_T\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    183\u001b[39m     \u001b[38;5;66;03m# Sum over all dimensions except the last two\u001b[39;00m\n\u001b[32m    184\u001b[39m     sum_axes = \u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mrange\u001b[39m(grad_other.ndim - \u001b[32m2\u001b[39m))\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "class Net(Module):\n",
    "    def __init__(self, d_model, n_heads, vocab_size, max_seq_len, pad_idx=0):\n",
    "        super().__init__()\n",
    "\n",
    "        self.e = self.embedding(vocab_size, d_model, max_seq_len, pad_idx, name=\"Embedding\")\n",
    "\n",
    "        self.head1 = self.transformer(d_model=d_model, n_heads=n_heads)\n",
    "        self.head2 = self.transformer(d_model=d_model, n_heads=n_heads)\n",
    "        self.head3 = self.transformer(d_model=d_model, n_heads=n_heads)\n",
    "        self.head4 = self.transformer(d_model=d_model, n_heads=n_heads)\n",
    "        self.head5 = self.transformer(d_model=d_model, n_heads=n_heads)\n",
    "        self.head6 = self.transformer(d_model=d_model, n_heads=n_heads)\n",
    "        self.head7 = self.transformer(d_model=d_model, n_heads=n_heads)\n",
    "        self.head8 = self.transformer(d_model=d_model, n_heads=n_heads)\n",
    "        self.project = self.linear(d_model, vocab_size, name=\"project\")\n",
    "    \n",
    "    def forward(self, idx):\n",
    "        x, padding_mask = self.e.get_sentence_embedding(idx)\n",
    "        x = Tensor(x.data, requires_grad=False)\n",
    "        x = self.head1(x, padding_mask)\n",
    "        x = self.head2(x, padding_mask)\n",
    "        x = self.head3(x, padding_mask)\n",
    "        x = self.head4(x, padding_mask)\n",
    "        x = self.head5(x, padding_mask)\n",
    "        x = self.head6(x, padding_mask)\n",
    "        x = self.head7(x, padding_mask)\n",
    "        x = self.head8(x, padding_mask)\n",
    "        x = self.project(x)\n",
    "        return x\n",
    "\n",
    "    def train(self, x, y, epochs, optimizer):\n",
    "        for epoch in range(epochs):\n",
    "            y_hat = self.forward(x)\n",
    "            # print(y_hat.shape, y.shape)\n",
    "            loss = CrossEntropyWithLogits(y_hat, y, axis=-1)\n",
    "    \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            if epoch % 5 == 0:\n",
    "                print(f\"Epoch {epoch}, Loss: {loss.data}\")\n",
    "                \n",
    "if __name__ == \"__main__\":\n",
    "    D_MODEL = 16\n",
    "    VOCAB_SIZE = 20\n",
    "    N_HEADS = 2\n",
    "    MAX_SEQ_LEN = 16\n",
    "    PAD_IDX = 0\n",
    "\n",
    "    model = Net(d_model=D_MODEL, n_heads=N_HEADS, vocab_size=VOCAB_SIZE, max_seq_len=MAX_SEQ_LEN, pad_idx=PAD_IDX)\n",
    "    model._build((15, 15))\n",
    "    optimizer = AdamW(model.parameters(), lr=0.001, precision=(xp.float32, xp.float32))\n",
    "\n",
    "\n",
    "    model.train(x_mine, y_mine, epochs=1000, optimizer=optimizer)\n",
    "\n",
    "\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1945fa1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\luequ\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:382: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn was passed bias=False\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 3.2589\n",
      "Epoch 50, Loss: 2.3458\n",
      "Epoch 100, Loss: 1.6921\n",
      "Epoch 150, Loss: 1.1207\n",
      "Epoch 200, Loss: 0.6758\n",
      "Epoch 250, Loss: 0.3979\n",
      "Epoch 300, Loss: 0.3131\n",
      "Epoch 350, Loss: 0.1903\n",
      "Epoch 400, Loss: 0.1599\n",
      "Epoch 450, Loss: 0.1445\n",
      "Epoch 500, Loss: 0.0990\n",
      "Epoch 550, Loss: 0.0721\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 57\u001b[39m\n\u001b[32m     54\u001b[39m x_pt = torch.tensor(x).long()\n\u001b[32m     55\u001b[39m y_pt = torch.tensor(y).long()\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_pt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 37\u001b[39m, in \u001b[36mNet.train_model\u001b[39m\u001b[34m(self, x, y, epochs, optimizer, criterion)\u001b[39m\n\u001b[32m     35\u001b[39m loss = criterion(logits.view(-\u001b[32m1\u001b[39m, logits.size(-\u001b[32m1\u001b[39m)), y.view(-\u001b[32m1\u001b[39m))\n\u001b[32m     36\u001b[39m loss.backward()\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m \u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m epoch %  \u001b[32m50\u001b[39m== \u001b[32m0\u001b[39m:\n\u001b[32m     39\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss.item()\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\luequ\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\optim\\optimizer.py:485\u001b[39m, in \u001b[36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    480\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    481\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    482\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    483\u001b[39m             )\n\u001b[32m--> \u001b[39m\u001b[32m485\u001b[39m out = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    486\u001b[39m \u001b[38;5;28mself\u001b[39m._optimizer_step_code()\n\u001b[32m    488\u001b[39m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\luequ\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\optim\\optimizer.py:79\u001b[39m, in \u001b[36m_use_grad_for_differentiable.<locals>._use_grad\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     77\u001b[39m     torch.set_grad_enabled(\u001b[38;5;28mself\u001b[39m.defaults[\u001b[33m\"\u001b[39m\u001b[33mdifferentiable\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     78\u001b[39m     torch._dynamo.graph_break()\n\u001b[32m---> \u001b[39m\u001b[32m79\u001b[39m     ret = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     80\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     81\u001b[39m     torch._dynamo.graph_break()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\luequ\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\optim\\adam.py:246\u001b[39m, in \u001b[36mAdam.step\u001b[39m\u001b[34m(self, closure)\u001b[39m\n\u001b[32m    234\u001b[39m     beta1, beta2 = group[\u001b[33m\"\u001b[39m\u001b[33mbetas\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    236\u001b[39m     has_complex = \u001b[38;5;28mself\u001b[39m._init_group(\n\u001b[32m    237\u001b[39m         group,\n\u001b[32m    238\u001b[39m         params_with_grad,\n\u001b[32m   (...)\u001b[39m\u001b[32m    243\u001b[39m         state_steps,\n\u001b[32m    244\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m246\u001b[39m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    247\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    248\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    249\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    250\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mamsgrad\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    256\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    257\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    258\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweight_decay\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    259\u001b[39m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43meps\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    260\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmaximize\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    261\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mforeach\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    262\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcapturable\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    263\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdifferentiable\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    264\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfused\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    265\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgrad_scale\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    266\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfound_inf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    267\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdecoupled_weight_decay\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    268\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    270\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\luequ\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\optim\\optimizer.py:147\u001b[39m, in \u001b[36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    145\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(*args, **kwargs)\n\u001b[32m    146\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m147\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\luequ\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\optim\\adam.py:933\u001b[39m, in \u001b[36madam\u001b[39m\u001b[34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, decoupled_weight_decay, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[39m\n\u001b[32m    930\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    931\u001b[39m     func = _single_tensor_adam\n\u001b[32m--> \u001b[39m\u001b[32m933\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    934\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    935\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    936\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    937\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    938\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    939\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    940\u001b[39m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m=\u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    941\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    942\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[43m=\u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    947\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    948\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    949\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    950\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    951\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    952\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    953\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\luequ\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\optim\\adam.py:456\u001b[39m, in \u001b[36m_single_tensor_adam\u001b[39m\u001b[34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable, decoupled_weight_decay)\u001b[39m\n\u001b[32m    454\u001b[39m         exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=\u001b[32m1\u001b[39m - beta2)\n\u001b[32m    455\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m456\u001b[39m     \u001b[43mexp_avg_sq\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43maddcmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    458\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m capturable \u001b[38;5;129;01mor\u001b[39;00m differentiable:\n\u001b[32m    459\u001b[39m     step = step_t\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, vocab_size, max_seq_len, num_layers=1, pad_idx=0):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model, padding_idx=pad_idx)\n",
    "        self.pos_embed = nn.Embedding(max_seq_len, d_model)\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, \n",
    "            nhead=n_heads, \n",
    "            dim_feedforward=d_model * 4, \n",
    "            batch_first=True,\n",
    "            bias=False,\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.project = nn.Linear(d_model, vocab_size, bias=False)\n",
    "\n",
    "    def forward(self, idx):\n",
    "        batch_size, seq_len = idx.size()\n",
    "        pos = torch.arange(seq_len, device=idx.device).unsqueeze(0).expand(batch_size, seq_len)\n",
    "        \n",
    "        x = self.embedding(idx) + self.pos_embed(pos)\n",
    "        padding_mask = (idx == 0)\n",
    "        x = self.encoder(x, src_key_padding_mask=padding_mask)\n",
    "        logits = self.project(x)\n",
    "        return logits\n",
    "\n",
    "    def train_model(self, x, y, epochs, optimizer, criterion):\n",
    "        for epoch in range(epochs):\n",
    "            optimizer.zero_grad()\n",
    "            logits = self.forward(x)\n",
    "            loss = criterion(logits.view(-1, logits.size(-1)), y.view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if epoch %  50== 0:\n",
    "                print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "# --- Config ---\n",
    "D_MODEL = 16\n",
    "VOCAB_SIZE = 20\n",
    "N_HEADS = 2\n",
    "MAX_SEQ_LEN = 16\n",
    "PAD_IDX = 0\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "# --- Model Training ---\n",
    "model = Net(D_MODEL, N_HEADS, VOCAB_SIZE, MAX_SEQ_LEN, num_layers=8, pad_idx=PAD_IDX)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "\n",
    "x_pt = torch.tensor(x).long()\n",
    "y_pt = torch.tensor(y).long()\n",
    "\n",
    "model.train_model(x_pt, y_pt, epochs=1000, optimizer=optimizer, criterion=criterion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7b1b2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
