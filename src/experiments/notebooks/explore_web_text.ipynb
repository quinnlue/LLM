{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57fa0cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import lzma\n",
    "import tarfile\n",
    "import tqdm\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import shutil\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da67f608",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for path in os.listdir(\"openwebtext\"):\n",
    "    if not path.endswith(\".xz\"):\n",
    "        # delete the file or directory\n",
    "        full_path = os.path.join(\"openwebtext\", path)\n",
    "        if os.path.isdir(full_path):\n",
    "            shutil.rmtree(full_path)\n",
    "        else:\n",
    "            os.remove(full_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eee2a2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "xz archives: 100%|██████████| 1000/1000 [03:13<00:00,  5.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished. All texts are inside: openwebtext\\openwebtext.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "root = \"openwebtext\"\n",
    "xz_files = [f for f in os.listdir(root) if f.endswith(\".xz\")]\n",
    "\n",
    "# Parquet writer ----------------------------------------------------------------\n",
    "parquet_path = os.path.join(root, \"openwebtext.parquet\")\n",
    "schema = pa.schema([\n",
    "    (\"source\", pa.string()),   # filename inside archive or any ID you like\n",
    "    (\"text\",   pa.string())    # the document itself\n",
    "])\n",
    "writer = pq.ParquetWriter(parquet_path, schema, compression=\"zstd\")\n",
    "\n",
    "# Helper for batching -----------------------------------------------------------\n",
    "BATCH_SIZE = 1_000\n",
    "buffer = {\"source\": [], \"text\": []}\n",
    "\n",
    "def flush():\n",
    "    \"\"\"Write the current buffer to parquet and clear it.\"\"\"\n",
    "    if buffer[\"text\"]:                      # non-empty\n",
    "        table = pa.Table.from_pydict(buffer, schema=schema)\n",
    "        writer.write_table(table)\n",
    "        buffer[\"source\"].clear()\n",
    "        buffer[\"text\"].clear()\n",
    "\n",
    "# Main extraction loop ----------------------------------------------------------\n",
    "for xz_file in tqdm.tqdm(xz_files, desc=\"xz archives\"):\n",
    "    xz_path = os.path.join(root, xz_file)\n",
    "\n",
    "    # 1. stream-decompress the .xz\n",
    "    with lzma.open(xz_path) as lzma_file:\n",
    "        # 2. open the tar stream\n",
    "        with tarfile.open(fileobj=lzma_file) as tar:\n",
    "            for member in tar.getmembers():\n",
    "                if member.isfile() and member.name.endswith(\".txt\"):\n",
    "                    member_file = tar.extractfile(member)\n",
    "                    if member_file is None:\n",
    "                        continue\n",
    "\n",
    "                    # 3. read bytes -> decode -> append to buffer\n",
    "                    txt = member_file.read().decode(\"utf-8\", errors=\"ignore\")\n",
    "                    buffer[\"source\"].append(\n",
    "                        f\"{xz_file[:-3]}_{os.path.basename(member.name)}\"\n",
    "                    )\n",
    "                    buffer[\"text\"].append(txt)\n",
    "\n",
    "                    # 4. flush every BATCH_SIZE rows\n",
    "                    if len(buffer[\"text\"]) >= BATCH_SIZE:\n",
    "                        flush()\n",
    "\n",
    "# final flush, close parquet ----------------------------------------------------\n",
    "flush()\n",
    "writer.close()\n",
    "\n",
    "print(f\"Finished. All texts are inside: {parquet_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49aa8b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b53368b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded size: 3400.46 MB\n",
      "Saved openwebtext/splits/openwebtext_part_00.parquet with 32352 rows\n",
      "Saved openwebtext/splits/openwebtext_part_01.parquet with 32352 rows\n",
      "Saved openwebtext/splits/openwebtext_part_02.parquet with 32352 rows\n",
      "Saved openwebtext/splits/openwebtext_part_03.parquet with 32352 rows\n",
      "Saved openwebtext/splits/openwebtext_part_04.parquet with 32352 rows\n",
      "Saved openwebtext/splits/openwebtext_part_05.parquet with 32352 rows\n",
      "Saved openwebtext/splits/openwebtext_part_06.parquet with 32352 rows\n",
      "Saved openwebtext/splits/openwebtext_part_07.parquet with 32352 rows\n",
      "Saved openwebtext/splits/openwebtext_part_08.parquet with 32352 rows\n",
      "Saved openwebtext/splits/openwebtext_part_09.parquet with 32352 rows\n",
      "Saved openwebtext/splits/openwebtext_part_10.parquet with 32352 rows\n",
      "Saved openwebtext/splits/openwebtext_part_11.parquet with 32350 rows\n"
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "# import os\n",
    "# import math\n",
    "# import psutil\n",
    "\n",
    "# # Load your full dataset\n",
    "# df = pd.read_parquet(\"openwebtext/openwebtext.parquet\")\n",
    "\n",
    "# # Estimate size in memory\n",
    "# mem_bytes = df.memory_usage(deep=True).sum()\n",
    "# mem_mb = mem_bytes / 1024**2\n",
    "# print(f\"Loaded size: {mem_mb:.2f} MB\")\n",
    "\n",
    "# # Define target size per split (adjust to taste, baddie)\n",
    "# target_mb = 300\n",
    "# n_splits = math.ceil(mem_mb / target_mb)\n",
    "\n",
    "# # Split and save\n",
    "# os.makedirs(\"openwebtext/splits\", exist_ok=True)\n",
    "\n",
    "# chunk_size = math.ceil(len(df) / n_splits)\n",
    "\n",
    "# for i in range(n_splits):\n",
    "#     start = i * chunk_size\n",
    "#     end = min((i + 1) * chunk_size, len(df))\n",
    "#     chunk = df.iloc[start:end]\n",
    "#     out_path = f\"openwebtext/splits/openwebtext_part_{i:02d}.parquet\"\n",
    "#     chunk.to_parquet(out_path, index=False)\n",
    "#     print(f\"Saved {out_path} with {len(chunk)} rows\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "195f912f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(r\"../../data/tokenized/owt_part_00.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1982c3ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(38133042)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'].apply(len).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4b6614bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for path in os.listdir(\"owt\"):\n",
    "    if path.endswith(\".parquet\"):\n",
    "        df = pd.read_parquet(f\"owt/{path}\")\n",
    "        df['text'] = df['text'].apply(clean_text)\n",
    "        df.to_parquet(f\"owt/{path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be213d7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(1178.6919510385758)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'].apply(len).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ccccbe64",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = df[df['text'].apply(len) <= 2096]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "58cbc5ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(786.1747071463228)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df['text'].apply(len).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "805b914a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(22684285)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df['text'].apply(len).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c7e665da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12163521"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "12_163_521"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a57e620a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bin(text):\n",
    "    return len(text)//16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4de5fb4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\luequ\\AppData\\Local\\Temp\\ipykernel_19212\\1360437756.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_df['bin'] = new_df['text'].apply(lambda x: len(x)//16)\n"
     ]
    }
   ],
   "source": [
    "new_df['bin'] = new_df['text'].apply(lambda x: len(x)//16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0ed464d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "557\n",
      "530\n",
      "530\n",
      "529\n",
      "527\n",
      "525\n",
      "516\n",
      "516\n",
      "514\n",
      "512\n",
      "509\n",
      "507\n",
      "502\n",
      "499\n",
      "496\n",
      "491\n",
      "473\n",
      "460\n",
      "451\n",
      "451\n",
      "440\n",
      "432\n",
      "418\n",
      "407\n",
      "405\n",
      "385\n",
      "382\n",
      "382\n",
      "369\n",
      "360\n",
      "351\n",
      "349\n",
      "347\n",
      "337\n",
      "335\n",
      "328\n",
      "328\n",
      "317\n",
      "304\n",
      "296\n",
      "294\n",
      "290\n",
      "290\n",
      "278\n",
      "269\n",
      "269\n",
      "259\n",
      "258\n",
      "258\n",
      "248\n",
      "241\n",
      "226\n",
      "221\n",
      "217\n",
      "116\n",
      "100\n",
      "22\n",
      "2\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "for b in new_df['bin'].value_counts():\n",
    "    print(b)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc63687",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\luequ\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\luequ\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\luequ\\.cache\\huggingface\\hub\\datasets--Skylion007--openwebtext. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Dataset scripts are no longer supported, but found openwebtext.py",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[37]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m dataset = \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mSkylion007/openwebtext\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstreaming\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\luequ\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\datasets\\load.py:1392\u001b[39m, in \u001b[36mload_dataset\u001b[39m\u001b[34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, keep_in_memory, save_infos, revision, token, streaming, num_proc, storage_options, **config_kwargs)\u001b[39m\n\u001b[32m   1387\u001b[39m verification_mode = VerificationMode(\n\u001b[32m   1388\u001b[39m     (verification_mode \u001b[38;5;129;01mor\u001b[39;00m VerificationMode.BASIC_CHECKS) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m save_infos \u001b[38;5;28;01melse\u001b[39;00m VerificationMode.ALL_CHECKS\n\u001b[32m   1389\u001b[39m )\n\u001b[32m   1391\u001b[39m \u001b[38;5;66;03m# Create a dataset builder\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1392\u001b[39m builder_instance = \u001b[43mload_dataset_builder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1393\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1394\u001b[39m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1395\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1396\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1397\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1398\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1399\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1400\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1401\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1402\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1403\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1404\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1405\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1407\u001b[39m \u001b[38;5;66;03m# Return iterable dataset in case of streaming\u001b[39;00m\n\u001b[32m   1408\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m streaming:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\luequ\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\datasets\\load.py:1132\u001b[39m, in \u001b[36mload_dataset_builder\u001b[39m\u001b[34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, token, storage_options, **config_kwargs)\u001b[39m\n\u001b[32m   1130\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m features \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1131\u001b[39m     features = _fix_for_backward_compatible_features(features)\n\u001b[32m-> \u001b[39m\u001b[32m1132\u001b[39m dataset_module = \u001b[43mdataset_module_factory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1133\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1134\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1135\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1136\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1137\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1138\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1139\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1140\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1141\u001b[39m \u001b[38;5;66;03m# Get dataset builder class\u001b[39;00m\n\u001b[32m   1142\u001b[39m builder_kwargs = dataset_module.builder_kwargs\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\luequ\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\datasets\\load.py:1031\u001b[39m, in \u001b[36mdataset_module_factory\u001b[39m\u001b[34m(path, revision, download_config, download_mode, data_dir, data_files, cache_dir, **download_kwargs)\u001b[39m\n\u001b[32m   1026\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e1, \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m):\n\u001b[32m   1027\u001b[39m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\n\u001b[32m   1028\u001b[39m                     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCouldn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt find any data file at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrelative_to_absolute_path(path)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1029\u001b[39m                     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCouldn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt find \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m on the Hugging Face Hub either: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(e1).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me1\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1030\u001b[39m                 ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1031\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m e1 \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1032\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1033\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCouldn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt find any data file at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrelative_to_absolute_path(path)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\luequ\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\datasets\\load.py:989\u001b[39m, in \u001b[36mdataset_module_factory\u001b[39m\u001b[34m(path, revision, download_config, download_mode, data_dir, data_files, cache_dir, **download_kwargs)\u001b[39m\n\u001b[32m    981\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    982\u001b[39m     api.hf_hub_download(\n\u001b[32m    983\u001b[39m         repo_id=path,\n\u001b[32m    984\u001b[39m         filename=filename,\n\u001b[32m   (...)\u001b[39m\u001b[32m    987\u001b[39m         proxies=download_config.proxies,\n\u001b[32m    988\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m989\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDataset scripts are no longer supported, but found \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilename\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    990\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError:\n\u001b[32m    991\u001b[39m     \u001b[38;5;66;03m# Use the infos from the parquet export except in some cases:\u001b[39;00m\n\u001b[32m    992\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m data_dir \u001b[38;5;129;01mor\u001b[39;00m data_files \u001b[38;5;129;01mor\u001b[39;00m (revision \u001b[38;5;129;01mand\u001b[39;00m revision != \u001b[33m\"\u001b[39m\u001b[33mmain\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[31mRuntimeError\u001b[39m: Dataset scripts are no longer supported, but found openwebtext.py"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"Skylion007/openwebtext\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0209f3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\luequ\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:980: UserWarning: `local_dir_use_symlinks` parameter is deprecated and will be ignored. The process to download files to a local folder has been updated and do not rely on symlinks anymore. You only need to pass a destination folder as`local_dir`.\n",
      "For more details, check out https://huggingface.co/docs/huggingface_hub/main/en/guides/download#download-files-to-local-folder.\n",
      "  warnings.warn(\n",
      "Fetching 24 files:   4%|▍         | 1/24 [00:00<00:15,  1.53it/s]"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "snapshot_download(\n",
    "    repo_id=\"Skylion007/openwebtext\",\n",
    "    repo_type=\"dataset\",\n",
    "    local_dir=\"openwebtext_dataset\",  # or wherever you want it saved\n",
    "    local_dir_use_symlinks=False  # make it an actual copy\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3bd7ee6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
