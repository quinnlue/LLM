{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57fa0cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import lzma\n",
    "import tarfile\n",
    "import tqdm\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import shutil\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da67f608",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for path in os.listdir(\"openwebtext\"):\n",
    "    if not path.endswith(\".xz\"):\n",
    "        # delete the file or directory\n",
    "        full_path = os.path.join(\"openwebtext\", path)\n",
    "        if os.path.isdir(full_path):\n",
    "            shutil.rmtree(full_path)\n",
    "        else:\n",
    "            os.remove(full_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eee2a2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "xz archives: 100%|██████████| 1000/1000 [03:13<00:00,  5.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished. All texts are inside: openwebtext\\openwebtext.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "root = \"openwebtext\"\n",
    "xz_files = [f for f in os.listdir(root) if f.endswith(\".xz\")]\n",
    "\n",
    "# Parquet writer ----------------------------------------------------------------\n",
    "parquet_path = os.path.join(root, \"openwebtext.parquet\")\n",
    "schema = pa.schema([\n",
    "    (\"source\", pa.string()),   # filename inside archive or any ID you like\n",
    "    (\"text\",   pa.string())    # the document itself\n",
    "])\n",
    "writer = pq.ParquetWriter(parquet_path, schema, compression=\"zstd\")\n",
    "\n",
    "# Helper for batching -----------------------------------------------------------\n",
    "BATCH_SIZE = 1_000\n",
    "buffer = {\"source\": [], \"text\": []}\n",
    "\n",
    "def flush():\n",
    "    \"\"\"Write the current buffer to parquet and clear it.\"\"\"\n",
    "    if buffer[\"text\"]:                      # non-empty\n",
    "        table = pa.Table.from_pydict(buffer, schema=schema)\n",
    "        writer.write_table(table)\n",
    "        buffer[\"source\"].clear()\n",
    "        buffer[\"text\"].clear()\n",
    "\n",
    "# Main extraction loop ----------------------------------------------------------\n",
    "for xz_file in tqdm.tqdm(xz_files, desc=\"xz archives\"):\n",
    "    xz_path = os.path.join(root, xz_file)\n",
    "\n",
    "    # 1. stream-decompress the .xz\n",
    "    with lzma.open(xz_path) as lzma_file:\n",
    "        # 2. open the tar stream\n",
    "        with tarfile.open(fileobj=lzma_file) as tar:\n",
    "            for member in tar.getmembers():\n",
    "                if member.isfile() and member.name.endswith(\".txt\"):\n",
    "                    member_file = tar.extractfile(member)\n",
    "                    if member_file is None:\n",
    "                        continue\n",
    "\n",
    "                    # 3. read bytes -> decode -> append to buffer\n",
    "                    txt = member_file.read().decode(\"utf-8\", errors=\"ignore\")\n",
    "                    buffer[\"source\"].append(\n",
    "                        f\"{xz_file[:-3]}_{os.path.basename(member.name)}\"\n",
    "                    )\n",
    "                    buffer[\"text\"].append(txt)\n",
    "\n",
    "                    # 4. flush every BATCH_SIZE rows\n",
    "                    if len(buffer[\"text\"]) >= BATCH_SIZE:\n",
    "                        flush()\n",
    "\n",
    "# final flush, close parquet ----------------------------------------------------\n",
    "flush()\n",
    "writer.close()\n",
    "\n",
    "print(f\"Finished. All texts are inside: {parquet_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49aa8b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b53368b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded size: 3400.46 MB\n",
      "Saved openwebtext/splits/openwebtext_part_00.parquet with 32352 rows\n",
      "Saved openwebtext/splits/openwebtext_part_01.parquet with 32352 rows\n",
      "Saved openwebtext/splits/openwebtext_part_02.parquet with 32352 rows\n",
      "Saved openwebtext/splits/openwebtext_part_03.parquet with 32352 rows\n",
      "Saved openwebtext/splits/openwebtext_part_04.parquet with 32352 rows\n",
      "Saved openwebtext/splits/openwebtext_part_05.parquet with 32352 rows\n",
      "Saved openwebtext/splits/openwebtext_part_06.parquet with 32352 rows\n",
      "Saved openwebtext/splits/openwebtext_part_07.parquet with 32352 rows\n",
      "Saved openwebtext/splits/openwebtext_part_08.parquet with 32352 rows\n",
      "Saved openwebtext/splits/openwebtext_part_09.parquet with 32352 rows\n",
      "Saved openwebtext/splits/openwebtext_part_10.parquet with 32352 rows\n",
      "Saved openwebtext/splits/openwebtext_part_11.parquet with 32350 rows\n"
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "# import os\n",
    "# import math\n",
    "# import psutil\n",
    "\n",
    "# # Load your full dataset\n",
    "# df = pd.read_parquet(\"openwebtext/openwebtext.parquet\")\n",
    "\n",
    "# # Estimate size in memory\n",
    "# mem_bytes = df.memory_usage(deep=True).sum()\n",
    "# mem_mb = mem_bytes / 1024**2\n",
    "# print(f\"Loaded size: {mem_mb:.2f} MB\")\n",
    "\n",
    "# # Define target size per split (adjust to taste, baddie)\n",
    "# target_mb = 300\n",
    "# n_splits = math.ceil(mem_mb / target_mb)\n",
    "\n",
    "# # Split and save\n",
    "# os.makedirs(\"openwebtext/splits\", exist_ok=True)\n",
    "\n",
    "# chunk_size = math.ceil(len(df) / n_splits)\n",
    "\n",
    "# for i in range(n_splits):\n",
    "#     start = i * chunk_size\n",
    "#     end = min((i + 1) * chunk_size, len(df))\n",
    "#     chunk = df.iloc[start:end]\n",
    "#     out_path = f\"openwebtext/splits/openwebtext_part_{i:02d}.parquet\"\n",
    "#     chunk.to_parquet(out_path, index=False)\n",
    "#     print(f\"Saved {out_path} with {len(chunk)} rows\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "195f912f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"owt/owt_part_00.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "782ddf7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "allowed_chars = set(string.ascii_letters + string.digits + string.punctuation + \" \\n\\t\")\n",
    "\n",
    "def clean_text(text):\n",
    "    text = \"\".join(c for c in text if c in allowed_chars)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1982c3ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(159341890)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'].apply(len).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4b6614bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for path in os.listdir(\"owt\"):\n",
    "    if path.endswith(\".parquet\"):\n",
    "        df = pd.read_parquet(f\"owt/{path}\")\n",
    "        df['text'] = df['text'].apply(clean_text)\n",
    "        df.to_parquet(f\"owt/{path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be213d7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
