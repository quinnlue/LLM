{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a45ac30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['model', 'optimizer', 'scheduler', 'scaler'])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Load the checkpoint\n",
    "checkpoint = torch.load(\"checkpoints/model.pt\", map_location=\"cpu\")  # map_location ensures no GPU needed\n",
    "\n",
    "# Inspect keys\n",
    "print(checkpoint.keys())  # usually you'll see 'model_state_dict' or similar\n",
    "\n",
    "# Example: get all parameters as NumPy arrays\n",
    "import numpy as np\n",
    "\n",
    "params_np = {}\n",
    "for k, v in checkpoint['model'].items():\n",
    "    params_np[k] = v.cpu().numpy()  # convert tensor to numpy\n",
    "\n",
    "# now params_np is a dict of numpy arrays\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1aac6da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3705ff58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['pos_emb', 'token_emb.weight', 'blocks.0.ln1.weight', 'blocks.0.ln1.bias', 'blocks.0.attn.q_lora_A.weight', 'blocks.0.attn.k_lora_A.weight', 'blocks.0.attn.v_lora_A.weight', 'blocks.0.attn.q_lora_B.weight', 'blocks.0.attn.k_lora_B.weight', 'blocks.0.attn.v_lora_B.weight', 'blocks.0.attn.o_lora_A.weight', 'blocks.0.attn.o_lora_B.weight', 'blocks.0.attn.qkv.weight', 'blocks.0.attn.qkv.bias', 'blocks.0.attn.out_proj.weight', 'blocks.0.attn.out_proj.bias', 'blocks.0.ln2.weight', 'blocks.0.ln2.bias', 'blocks.0.proj_up_lora_A.weight', 'blocks.0.proj_up_lora_B.weight', 'blocks.0.proj_down_lora_A.weight', 'blocks.0.proj_down_lora_B.weight', 'blocks.0.mlp.0.weight', 'blocks.0.mlp.0.bias', 'blocks.0.mlp.2.weight', 'blocks.0.mlp.2.bias', 'blocks.1.ln1.weight', 'blocks.1.ln1.bias', 'blocks.1.attn.q_lora_A.weight', 'blocks.1.attn.k_lora_A.weight', 'blocks.1.attn.v_lora_A.weight', 'blocks.1.attn.q_lora_B.weight', 'blocks.1.attn.k_lora_B.weight', 'blocks.1.attn.v_lora_B.weight', 'blocks.1.attn.o_lora_A.weight', 'blocks.1.attn.o_lora_B.weight', 'blocks.1.attn.qkv.weight', 'blocks.1.attn.qkv.bias', 'blocks.1.attn.out_proj.weight', 'blocks.1.attn.out_proj.bias', 'blocks.1.ln2.weight', 'blocks.1.ln2.bias', 'blocks.1.proj_up_lora_A.weight', 'blocks.1.proj_up_lora_B.weight', 'blocks.1.proj_down_lora_A.weight', 'blocks.1.proj_down_lora_B.weight', 'blocks.1.mlp.0.weight', 'blocks.1.mlp.0.bias', 'blocks.1.mlp.2.weight', 'blocks.1.mlp.2.bias', 'blocks.2.ln1.weight', 'blocks.2.ln1.bias', 'blocks.2.attn.q_lora_A.weight', 'blocks.2.attn.k_lora_A.weight', 'blocks.2.attn.v_lora_A.weight', 'blocks.2.attn.q_lora_B.weight', 'blocks.2.attn.k_lora_B.weight', 'blocks.2.attn.v_lora_B.weight', 'blocks.2.attn.o_lora_A.weight', 'blocks.2.attn.o_lora_B.weight', 'blocks.2.attn.qkv.weight', 'blocks.2.attn.qkv.bias', 'blocks.2.attn.out_proj.weight', 'blocks.2.attn.out_proj.bias', 'blocks.2.ln2.weight', 'blocks.2.ln2.bias', 'blocks.2.proj_up_lora_A.weight', 'blocks.2.proj_up_lora_B.weight', 'blocks.2.proj_down_lora_A.weight', 'blocks.2.proj_down_lora_B.weight', 'blocks.2.mlp.0.weight', 'blocks.2.mlp.0.bias', 'blocks.2.mlp.2.weight', 'blocks.2.mlp.2.bias', 'blocks.3.ln1.weight', 'blocks.3.ln1.bias', 'blocks.3.attn.q_lora_A.weight', 'blocks.3.attn.k_lora_A.weight', 'blocks.3.attn.v_lora_A.weight', 'blocks.3.attn.q_lora_B.weight', 'blocks.3.attn.k_lora_B.weight', 'blocks.3.attn.v_lora_B.weight', 'blocks.3.attn.o_lora_A.weight', 'blocks.3.attn.o_lora_B.weight', 'blocks.3.attn.qkv.weight', 'blocks.3.attn.qkv.bias', 'blocks.3.attn.out_proj.weight', 'blocks.3.attn.out_proj.bias', 'blocks.3.ln2.weight', 'blocks.3.ln2.bias', 'blocks.3.proj_up_lora_A.weight', 'blocks.3.proj_up_lora_B.weight', 'blocks.3.proj_down_lora_A.weight', 'blocks.3.proj_down_lora_B.weight', 'blocks.3.mlp.0.weight', 'blocks.3.mlp.0.bias', 'blocks.3.mlp.2.weight', 'blocks.3.mlp.2.bias', 'blocks.4.ln1.weight', 'blocks.4.ln1.bias', 'blocks.4.attn.q_lora_A.weight', 'blocks.4.attn.k_lora_A.weight', 'blocks.4.attn.v_lora_A.weight', 'blocks.4.attn.q_lora_B.weight', 'blocks.4.attn.k_lora_B.weight', 'blocks.4.attn.v_lora_B.weight', 'blocks.4.attn.o_lora_A.weight', 'blocks.4.attn.o_lora_B.weight', 'blocks.4.attn.qkv.weight', 'blocks.4.attn.qkv.bias', 'blocks.4.attn.out_proj.weight', 'blocks.4.attn.out_proj.bias', 'blocks.4.ln2.weight', 'blocks.4.ln2.bias', 'blocks.4.proj_up_lora_A.weight', 'blocks.4.proj_up_lora_B.weight', 'blocks.4.proj_down_lora_A.weight', 'blocks.4.proj_down_lora_B.weight', 'blocks.4.mlp.0.weight', 'blocks.4.mlp.0.bias', 'blocks.4.mlp.2.weight', 'blocks.4.mlp.2.bias', 'blocks.5.ln1.weight', 'blocks.5.ln1.bias', 'blocks.5.attn.q_lora_A.weight', 'blocks.5.attn.k_lora_A.weight', 'blocks.5.attn.v_lora_A.weight', 'blocks.5.attn.q_lora_B.weight', 'blocks.5.attn.k_lora_B.weight', 'blocks.5.attn.v_lora_B.weight', 'blocks.5.attn.o_lora_A.weight', 'blocks.5.attn.o_lora_B.weight', 'blocks.5.attn.qkv.weight', 'blocks.5.attn.qkv.bias', 'blocks.5.attn.out_proj.weight', 'blocks.5.attn.out_proj.bias', 'blocks.5.ln2.weight', 'blocks.5.ln2.bias', 'blocks.5.proj_up_lora_A.weight', 'blocks.5.proj_up_lora_B.weight', 'blocks.5.proj_down_lora_A.weight', 'blocks.5.proj_down_lora_B.weight', 'blocks.5.mlp.0.weight', 'blocks.5.mlp.0.bias', 'blocks.5.mlp.2.weight', 'blocks.5.mlp.2.bias', 'blocks.6.ln1.weight', 'blocks.6.ln1.bias', 'blocks.6.attn.q_lora_A.weight', 'blocks.6.attn.k_lora_A.weight', 'blocks.6.attn.v_lora_A.weight', 'blocks.6.attn.q_lora_B.weight', 'blocks.6.attn.k_lora_B.weight', 'blocks.6.attn.v_lora_B.weight', 'blocks.6.attn.o_lora_A.weight', 'blocks.6.attn.o_lora_B.weight', 'blocks.6.attn.qkv.weight', 'blocks.6.attn.qkv.bias', 'blocks.6.attn.out_proj.weight', 'blocks.6.attn.out_proj.bias', 'blocks.6.ln2.weight', 'blocks.6.ln2.bias', 'blocks.6.proj_up_lora_A.weight', 'blocks.6.proj_up_lora_B.weight', 'blocks.6.proj_down_lora_A.weight', 'blocks.6.proj_down_lora_B.weight', 'blocks.6.mlp.0.weight', 'blocks.6.mlp.0.bias', 'blocks.6.mlp.2.weight', 'blocks.6.mlp.2.bias', 'blocks.7.ln1.weight', 'blocks.7.ln1.bias', 'blocks.7.attn.q_lora_A.weight', 'blocks.7.attn.k_lora_A.weight', 'blocks.7.attn.v_lora_A.weight', 'blocks.7.attn.q_lora_B.weight', 'blocks.7.attn.k_lora_B.weight', 'blocks.7.attn.v_lora_B.weight', 'blocks.7.attn.o_lora_A.weight', 'blocks.7.attn.o_lora_B.weight', 'blocks.7.attn.qkv.weight', 'blocks.7.attn.qkv.bias', 'blocks.7.attn.out_proj.weight', 'blocks.7.attn.out_proj.bias', 'blocks.7.ln2.weight', 'blocks.7.ln2.bias', 'blocks.7.proj_up_lora_A.weight', 'blocks.7.proj_up_lora_B.weight', 'blocks.7.proj_down_lora_A.weight', 'blocks.7.proj_down_lora_B.weight', 'blocks.7.mlp.0.weight', 'blocks.7.mlp.0.bias', 'blocks.7.mlp.2.weight', 'blocks.7.mlp.2.bias', 'blocks.8.ln1.weight', 'blocks.8.ln1.bias', 'blocks.8.attn.q_lora_A.weight', 'blocks.8.attn.k_lora_A.weight', 'blocks.8.attn.v_lora_A.weight', 'blocks.8.attn.q_lora_B.weight', 'blocks.8.attn.k_lora_B.weight', 'blocks.8.attn.v_lora_B.weight', 'blocks.8.attn.o_lora_A.weight', 'blocks.8.attn.o_lora_B.weight', 'blocks.8.attn.qkv.weight', 'blocks.8.attn.qkv.bias', 'blocks.8.attn.out_proj.weight', 'blocks.8.attn.out_proj.bias', 'blocks.8.ln2.weight', 'blocks.8.ln2.bias', 'blocks.8.proj_up_lora_A.weight', 'blocks.8.proj_up_lora_B.weight', 'blocks.8.proj_down_lora_A.weight', 'blocks.8.proj_down_lora_B.weight', 'blocks.8.mlp.0.weight', 'blocks.8.mlp.0.bias', 'blocks.8.mlp.2.weight', 'blocks.8.mlp.2.bias', 'blocks.9.ln1.weight', 'blocks.9.ln1.bias', 'blocks.9.attn.q_lora_A.weight', 'blocks.9.attn.k_lora_A.weight', 'blocks.9.attn.v_lora_A.weight', 'blocks.9.attn.q_lora_B.weight', 'blocks.9.attn.k_lora_B.weight', 'blocks.9.attn.v_lora_B.weight', 'blocks.9.attn.o_lora_A.weight', 'blocks.9.attn.o_lora_B.weight', 'blocks.9.attn.qkv.weight', 'blocks.9.attn.qkv.bias', 'blocks.9.attn.out_proj.weight', 'blocks.9.attn.out_proj.bias', 'blocks.9.ln2.weight', 'blocks.9.ln2.bias', 'blocks.9.proj_up_lora_A.weight', 'blocks.9.proj_up_lora_B.weight', 'blocks.9.proj_down_lora_A.weight', 'blocks.9.proj_down_lora_B.weight', 'blocks.9.mlp.0.weight', 'blocks.9.mlp.0.bias', 'blocks.9.mlp.2.weight', 'blocks.9.mlp.2.bias', 'blocks.10.ln1.weight', 'blocks.10.ln1.bias', 'blocks.10.attn.q_lora_A.weight', 'blocks.10.attn.k_lora_A.weight', 'blocks.10.attn.v_lora_A.weight', 'blocks.10.attn.q_lora_B.weight', 'blocks.10.attn.k_lora_B.weight', 'blocks.10.attn.v_lora_B.weight', 'blocks.10.attn.o_lora_A.weight', 'blocks.10.attn.o_lora_B.weight', 'blocks.10.attn.qkv.weight', 'blocks.10.attn.qkv.bias', 'blocks.10.attn.out_proj.weight', 'blocks.10.attn.out_proj.bias', 'blocks.10.ln2.weight', 'blocks.10.ln2.bias', 'blocks.10.proj_up_lora_A.weight', 'blocks.10.proj_up_lora_B.weight', 'blocks.10.proj_down_lora_A.weight', 'blocks.10.proj_down_lora_B.weight', 'blocks.10.mlp.0.weight', 'blocks.10.mlp.0.bias', 'blocks.10.mlp.2.weight', 'blocks.10.mlp.2.bias', 'blocks.11.ln1.weight', 'blocks.11.ln1.bias', 'blocks.11.attn.q_lora_A.weight', 'blocks.11.attn.k_lora_A.weight', 'blocks.11.attn.v_lora_A.weight', 'blocks.11.attn.q_lora_B.weight', 'blocks.11.attn.k_lora_B.weight', 'blocks.11.attn.v_lora_B.weight', 'blocks.11.attn.o_lora_A.weight', 'blocks.11.attn.o_lora_B.weight', 'blocks.11.attn.qkv.weight', 'blocks.11.attn.qkv.bias', 'blocks.11.attn.out_proj.weight', 'blocks.11.attn.out_proj.bias', 'blocks.11.ln2.weight', 'blocks.11.ln2.bias', 'blocks.11.proj_up_lora_A.weight', 'blocks.11.proj_up_lora_B.weight', 'blocks.11.proj_down_lora_A.weight', 'blocks.11.proj_down_lora_B.weight', 'blocks.11.mlp.0.weight', 'blocks.11.mlp.0.bias', 'blocks.11.mlp.2.weight', 'blocks.11.mlp.2.bias', 'lm_head.weight', 'lm_head.bias'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params_np.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38c9d384",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "259344866"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([v.size for v in params_np.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f31796a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, pathlib\n",
    "sys.path.append(str(pathlib.Path(r\"c:\\ml-projects\").resolve()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c25736d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA not available\n",
      "CUDA not available\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add dlx to path\n",
    "sys.path.insert(0, r'c:\\ml-projects\\dlx')\n",
    "\n",
    "# Force NumPy backend\n",
    "import dlx.utils.backend as backend_module\n",
    "import numpy as np\n",
    "from dlx.nn.optim import AdamW\n",
    "from dlx.nn.tensor import Tensor\n",
    "backend_module.xp = np\n",
    "\n",
    "# Now import\n",
    "from training.model import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a3e7d5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpt1.torch_train import model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b0a70e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 51682\n",
    "D_MODEL = 1024\n",
    "N_HEADS = 16\n",
    "MAX_SEQ_LEN = 512\n",
    "PAD_IDX = 0\n",
    "DEPTH = 12\n",
    "\n",
    "dlx_model = Model(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    d_model=D_MODEL,\n",
    "    max_seq_len=MAX_SEQ_LEN,\n",
    "    pad_idx=PAD_IDX,\n",
    "    n_heads=N_HEADS,\n",
    "    transformer_depth=DEPTH,\n",
    "    checkpoint_interval_seconds=3600,\n",
    "    train_dir=\"data/train\",\n",
    "    validation_dir=\"data/validation\",\n",
    "    checkpoint_dir=\"checkpoints\",\n",
    "    epochs=1,\n",
    "    mini_batch_per_step=8,\n",
    "    lora=True,\n",
    "    lora_r=8,\n",
    "    lora_alpha=8,\n",
    "    mlp_ratio=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d7aca9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "259344866"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dlx_model.num_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d02868c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_model = model.Model(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    d_model=D_MODEL,\n",
    "    max_seq_len=MAX_SEQ_LEN,\n",
    "    pad_idx=PAD_IDX,\n",
    "    n_heads=N_HEADS,\n",
    "    transformer_depth=DEPTH,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "42d756b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=[], unexpected_keys=['blocks.0.proj_up_lora_A.weight', 'blocks.0.proj_up_lora_B.weight', 'blocks.0.proj_down_lora_A.weight', 'blocks.0.proj_down_lora_B.weight', 'blocks.0.attn.q_lora_A.weight', 'blocks.0.attn.k_lora_A.weight', 'blocks.0.attn.v_lora_A.weight', 'blocks.0.attn.q_lora_B.weight', 'blocks.0.attn.k_lora_B.weight', 'blocks.0.attn.v_lora_B.weight', 'blocks.0.attn.o_lora_A.weight', 'blocks.0.attn.o_lora_B.weight', 'blocks.1.proj_up_lora_A.weight', 'blocks.1.proj_up_lora_B.weight', 'blocks.1.proj_down_lora_A.weight', 'blocks.1.proj_down_lora_B.weight', 'blocks.1.attn.q_lora_A.weight', 'blocks.1.attn.k_lora_A.weight', 'blocks.1.attn.v_lora_A.weight', 'blocks.1.attn.q_lora_B.weight', 'blocks.1.attn.k_lora_B.weight', 'blocks.1.attn.v_lora_B.weight', 'blocks.1.attn.o_lora_A.weight', 'blocks.1.attn.o_lora_B.weight', 'blocks.2.proj_up_lora_A.weight', 'blocks.2.proj_up_lora_B.weight', 'blocks.2.proj_down_lora_A.weight', 'blocks.2.proj_down_lora_B.weight', 'blocks.2.attn.q_lora_A.weight', 'blocks.2.attn.k_lora_A.weight', 'blocks.2.attn.v_lora_A.weight', 'blocks.2.attn.q_lora_B.weight', 'blocks.2.attn.k_lora_B.weight', 'blocks.2.attn.v_lora_B.weight', 'blocks.2.attn.o_lora_A.weight', 'blocks.2.attn.o_lora_B.weight', 'blocks.3.proj_up_lora_A.weight', 'blocks.3.proj_up_lora_B.weight', 'blocks.3.proj_down_lora_A.weight', 'blocks.3.proj_down_lora_B.weight', 'blocks.3.attn.q_lora_A.weight', 'blocks.3.attn.k_lora_A.weight', 'blocks.3.attn.v_lora_A.weight', 'blocks.3.attn.q_lora_B.weight', 'blocks.3.attn.k_lora_B.weight', 'blocks.3.attn.v_lora_B.weight', 'blocks.3.attn.o_lora_A.weight', 'blocks.3.attn.o_lora_B.weight', 'blocks.4.proj_up_lora_A.weight', 'blocks.4.proj_up_lora_B.weight', 'blocks.4.proj_down_lora_A.weight', 'blocks.4.proj_down_lora_B.weight', 'blocks.4.attn.q_lora_A.weight', 'blocks.4.attn.k_lora_A.weight', 'blocks.4.attn.v_lora_A.weight', 'blocks.4.attn.q_lora_B.weight', 'blocks.4.attn.k_lora_B.weight', 'blocks.4.attn.v_lora_B.weight', 'blocks.4.attn.o_lora_A.weight', 'blocks.4.attn.o_lora_B.weight', 'blocks.5.proj_up_lora_A.weight', 'blocks.5.proj_up_lora_B.weight', 'blocks.5.proj_down_lora_A.weight', 'blocks.5.proj_down_lora_B.weight', 'blocks.5.attn.q_lora_A.weight', 'blocks.5.attn.k_lora_A.weight', 'blocks.5.attn.v_lora_A.weight', 'blocks.5.attn.q_lora_B.weight', 'blocks.5.attn.k_lora_B.weight', 'blocks.5.attn.v_lora_B.weight', 'blocks.5.attn.o_lora_A.weight', 'blocks.5.attn.o_lora_B.weight', 'blocks.6.proj_up_lora_A.weight', 'blocks.6.proj_up_lora_B.weight', 'blocks.6.proj_down_lora_A.weight', 'blocks.6.proj_down_lora_B.weight', 'blocks.6.attn.q_lora_A.weight', 'blocks.6.attn.k_lora_A.weight', 'blocks.6.attn.v_lora_A.weight', 'blocks.6.attn.q_lora_B.weight', 'blocks.6.attn.k_lora_B.weight', 'blocks.6.attn.v_lora_B.weight', 'blocks.6.attn.o_lora_A.weight', 'blocks.6.attn.o_lora_B.weight', 'blocks.7.proj_up_lora_A.weight', 'blocks.7.proj_up_lora_B.weight', 'blocks.7.proj_down_lora_A.weight', 'blocks.7.proj_down_lora_B.weight', 'blocks.7.attn.q_lora_A.weight', 'blocks.7.attn.k_lora_A.weight', 'blocks.7.attn.v_lora_A.weight', 'blocks.7.attn.q_lora_B.weight', 'blocks.7.attn.k_lora_B.weight', 'blocks.7.attn.v_lora_B.weight', 'blocks.7.attn.o_lora_A.weight', 'blocks.7.attn.o_lora_B.weight', 'blocks.8.proj_up_lora_A.weight', 'blocks.8.proj_up_lora_B.weight', 'blocks.8.proj_down_lora_A.weight', 'blocks.8.proj_down_lora_B.weight', 'blocks.8.attn.q_lora_A.weight', 'blocks.8.attn.k_lora_A.weight', 'blocks.8.attn.v_lora_A.weight', 'blocks.8.attn.q_lora_B.weight', 'blocks.8.attn.k_lora_B.weight', 'blocks.8.attn.v_lora_B.weight', 'blocks.8.attn.o_lora_A.weight', 'blocks.8.attn.o_lora_B.weight', 'blocks.9.proj_up_lora_A.weight', 'blocks.9.proj_up_lora_B.weight', 'blocks.9.proj_down_lora_A.weight', 'blocks.9.proj_down_lora_B.weight', 'blocks.9.attn.q_lora_A.weight', 'blocks.9.attn.k_lora_A.weight', 'blocks.9.attn.v_lora_A.weight', 'blocks.9.attn.q_lora_B.weight', 'blocks.9.attn.k_lora_B.weight', 'blocks.9.attn.v_lora_B.weight', 'blocks.9.attn.o_lora_A.weight', 'blocks.9.attn.o_lora_B.weight', 'blocks.10.proj_up_lora_A.weight', 'blocks.10.proj_up_lora_B.weight', 'blocks.10.proj_down_lora_A.weight', 'blocks.10.proj_down_lora_B.weight', 'blocks.10.attn.q_lora_A.weight', 'blocks.10.attn.k_lora_A.weight', 'blocks.10.attn.v_lora_A.weight', 'blocks.10.attn.q_lora_B.weight', 'blocks.10.attn.k_lora_B.weight', 'blocks.10.attn.v_lora_B.weight', 'blocks.10.attn.o_lora_A.weight', 'blocks.10.attn.o_lora_B.weight', 'blocks.11.proj_up_lora_A.weight', 'blocks.11.proj_up_lora_B.weight', 'blocks.11.proj_down_lora_A.weight', 'blocks.11.proj_down_lora_B.weight', 'blocks.11.attn.q_lora_A.weight', 'blocks.11.attn.k_lora_A.weight', 'blocks.11.attn.v_lora_A.weight', 'blocks.11.attn.q_lora_B.weight', 'blocks.11.attn.k_lora_B.weight', 'blocks.11.attn.v_lora_B.weight', 'blocks.11.attn.o_lora_A.weight', 'blocks.11.attn.o_lora_B.weight'])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_model.load_state_dict(checkpoint['model'], strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5f4923a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "292"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(params_np.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2e154c6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['pos_emb', 'token_emb.weight', 'blocks.0.ln1.weight', 'blocks.0.ln1.bias', 'blocks.0.attn.q_lora_A.weight', 'blocks.0.attn.k_lora_A.weight', 'blocks.0.attn.v_lora_A.weight', 'blocks.0.attn.q_lora_B.weight', 'blocks.0.attn.k_lora_B.weight', 'blocks.0.attn.v_lora_B.weight', 'blocks.0.attn.o_lora_A.weight', 'blocks.0.attn.o_lora_B.weight', 'blocks.0.attn.qkv.weight', 'blocks.0.attn.qkv.bias', 'blocks.0.attn.out_proj.weight', 'blocks.0.attn.out_proj.bias', 'blocks.0.ln2.weight', 'blocks.0.ln2.bias', 'blocks.0.proj_up_lora_A.weight', 'blocks.0.proj_up_lora_B.weight', 'blocks.0.proj_down_lora_A.weight', 'blocks.0.proj_down_lora_B.weight', 'blocks.0.mlp.0.weight', 'blocks.0.mlp.0.bias', 'blocks.0.mlp.2.weight', 'blocks.0.mlp.2.bias', 'blocks.1.ln1.weight', 'blocks.1.ln1.bias', 'blocks.1.attn.q_lora_A.weight', 'blocks.1.attn.k_lora_A.weight', 'blocks.1.attn.v_lora_A.weight', 'blocks.1.attn.q_lora_B.weight', 'blocks.1.attn.k_lora_B.weight', 'blocks.1.attn.v_lora_B.weight', 'blocks.1.attn.o_lora_A.weight', 'blocks.1.attn.o_lora_B.weight', 'blocks.1.attn.qkv.weight', 'blocks.1.attn.qkv.bias', 'blocks.1.attn.out_proj.weight', 'blocks.1.attn.out_proj.bias', 'blocks.1.ln2.weight', 'blocks.1.ln2.bias', 'blocks.1.proj_up_lora_A.weight', 'blocks.1.proj_up_lora_B.weight', 'blocks.1.proj_down_lora_A.weight', 'blocks.1.proj_down_lora_B.weight', 'blocks.1.mlp.0.weight', 'blocks.1.mlp.0.bias', 'blocks.1.mlp.2.weight', 'blocks.1.mlp.2.bias', 'blocks.2.ln1.weight', 'blocks.2.ln1.bias', 'blocks.2.attn.q_lora_A.weight', 'blocks.2.attn.k_lora_A.weight', 'blocks.2.attn.v_lora_A.weight', 'blocks.2.attn.q_lora_B.weight', 'blocks.2.attn.k_lora_B.weight', 'blocks.2.attn.v_lora_B.weight', 'blocks.2.attn.o_lora_A.weight', 'blocks.2.attn.o_lora_B.weight', 'blocks.2.attn.qkv.weight', 'blocks.2.attn.qkv.bias', 'blocks.2.attn.out_proj.weight', 'blocks.2.attn.out_proj.bias', 'blocks.2.ln2.weight', 'blocks.2.ln2.bias', 'blocks.2.proj_up_lora_A.weight', 'blocks.2.proj_up_lora_B.weight', 'blocks.2.proj_down_lora_A.weight', 'blocks.2.proj_down_lora_B.weight', 'blocks.2.mlp.0.weight', 'blocks.2.mlp.0.bias', 'blocks.2.mlp.2.weight', 'blocks.2.mlp.2.bias', 'blocks.3.ln1.weight', 'blocks.3.ln1.bias', 'blocks.3.attn.q_lora_A.weight', 'blocks.3.attn.k_lora_A.weight', 'blocks.3.attn.v_lora_A.weight', 'blocks.3.attn.q_lora_B.weight', 'blocks.3.attn.k_lora_B.weight', 'blocks.3.attn.v_lora_B.weight', 'blocks.3.attn.o_lora_A.weight', 'blocks.3.attn.o_lora_B.weight', 'blocks.3.attn.qkv.weight', 'blocks.3.attn.qkv.bias', 'blocks.3.attn.out_proj.weight', 'blocks.3.attn.out_proj.bias', 'blocks.3.ln2.weight', 'blocks.3.ln2.bias', 'blocks.3.proj_up_lora_A.weight', 'blocks.3.proj_up_lora_B.weight', 'blocks.3.proj_down_lora_A.weight', 'blocks.3.proj_down_lora_B.weight', 'blocks.3.mlp.0.weight', 'blocks.3.mlp.0.bias', 'blocks.3.mlp.2.weight', 'blocks.3.mlp.2.bias', 'blocks.4.ln1.weight', 'blocks.4.ln1.bias', 'blocks.4.attn.q_lora_A.weight', 'blocks.4.attn.k_lora_A.weight', 'blocks.4.attn.v_lora_A.weight', 'blocks.4.attn.q_lora_B.weight', 'blocks.4.attn.k_lora_B.weight', 'blocks.4.attn.v_lora_B.weight', 'blocks.4.attn.o_lora_A.weight', 'blocks.4.attn.o_lora_B.weight', 'blocks.4.attn.qkv.weight', 'blocks.4.attn.qkv.bias', 'blocks.4.attn.out_proj.weight', 'blocks.4.attn.out_proj.bias', 'blocks.4.ln2.weight', 'blocks.4.ln2.bias', 'blocks.4.proj_up_lora_A.weight', 'blocks.4.proj_up_lora_B.weight', 'blocks.4.proj_down_lora_A.weight', 'blocks.4.proj_down_lora_B.weight', 'blocks.4.mlp.0.weight', 'blocks.4.mlp.0.bias', 'blocks.4.mlp.2.weight', 'blocks.4.mlp.2.bias', 'blocks.5.ln1.weight', 'blocks.5.ln1.bias', 'blocks.5.attn.q_lora_A.weight', 'blocks.5.attn.k_lora_A.weight', 'blocks.5.attn.v_lora_A.weight', 'blocks.5.attn.q_lora_B.weight', 'blocks.5.attn.k_lora_B.weight', 'blocks.5.attn.v_lora_B.weight', 'blocks.5.attn.o_lora_A.weight', 'blocks.5.attn.o_lora_B.weight', 'blocks.5.attn.qkv.weight', 'blocks.5.attn.qkv.bias', 'blocks.5.attn.out_proj.weight', 'blocks.5.attn.out_proj.bias', 'blocks.5.ln2.weight', 'blocks.5.ln2.bias', 'blocks.5.proj_up_lora_A.weight', 'blocks.5.proj_up_lora_B.weight', 'blocks.5.proj_down_lora_A.weight', 'blocks.5.proj_down_lora_B.weight', 'blocks.5.mlp.0.weight', 'blocks.5.mlp.0.bias', 'blocks.5.mlp.2.weight', 'blocks.5.mlp.2.bias', 'blocks.6.ln1.weight', 'blocks.6.ln1.bias', 'blocks.6.attn.q_lora_A.weight', 'blocks.6.attn.k_lora_A.weight', 'blocks.6.attn.v_lora_A.weight', 'blocks.6.attn.q_lora_B.weight', 'blocks.6.attn.k_lora_B.weight', 'blocks.6.attn.v_lora_B.weight', 'blocks.6.attn.o_lora_A.weight', 'blocks.6.attn.o_lora_B.weight', 'blocks.6.attn.qkv.weight', 'blocks.6.attn.qkv.bias', 'blocks.6.attn.out_proj.weight', 'blocks.6.attn.out_proj.bias', 'blocks.6.ln2.weight', 'blocks.6.ln2.bias', 'blocks.6.proj_up_lora_A.weight', 'blocks.6.proj_up_lora_B.weight', 'blocks.6.proj_down_lora_A.weight', 'blocks.6.proj_down_lora_B.weight', 'blocks.6.mlp.0.weight', 'blocks.6.mlp.0.bias', 'blocks.6.mlp.2.weight', 'blocks.6.mlp.2.bias', 'blocks.7.ln1.weight', 'blocks.7.ln1.bias', 'blocks.7.attn.q_lora_A.weight', 'blocks.7.attn.k_lora_A.weight', 'blocks.7.attn.v_lora_A.weight', 'blocks.7.attn.q_lora_B.weight', 'blocks.7.attn.k_lora_B.weight', 'blocks.7.attn.v_lora_B.weight', 'blocks.7.attn.o_lora_A.weight', 'blocks.7.attn.o_lora_B.weight', 'blocks.7.attn.qkv.weight', 'blocks.7.attn.qkv.bias', 'blocks.7.attn.out_proj.weight', 'blocks.7.attn.out_proj.bias', 'blocks.7.ln2.weight', 'blocks.7.ln2.bias', 'blocks.7.proj_up_lora_A.weight', 'blocks.7.proj_up_lora_B.weight', 'blocks.7.proj_down_lora_A.weight', 'blocks.7.proj_down_lora_B.weight', 'blocks.7.mlp.0.weight', 'blocks.7.mlp.0.bias', 'blocks.7.mlp.2.weight', 'blocks.7.mlp.2.bias', 'blocks.8.ln1.weight', 'blocks.8.ln1.bias', 'blocks.8.attn.q_lora_A.weight', 'blocks.8.attn.k_lora_A.weight', 'blocks.8.attn.v_lora_A.weight', 'blocks.8.attn.q_lora_B.weight', 'blocks.8.attn.k_lora_B.weight', 'blocks.8.attn.v_lora_B.weight', 'blocks.8.attn.o_lora_A.weight', 'blocks.8.attn.o_lora_B.weight', 'blocks.8.attn.qkv.weight', 'blocks.8.attn.qkv.bias', 'blocks.8.attn.out_proj.weight', 'blocks.8.attn.out_proj.bias', 'blocks.8.ln2.weight', 'blocks.8.ln2.bias', 'blocks.8.proj_up_lora_A.weight', 'blocks.8.proj_up_lora_B.weight', 'blocks.8.proj_down_lora_A.weight', 'blocks.8.proj_down_lora_B.weight', 'blocks.8.mlp.0.weight', 'blocks.8.mlp.0.bias', 'blocks.8.mlp.2.weight', 'blocks.8.mlp.2.bias', 'blocks.9.ln1.weight', 'blocks.9.ln1.bias', 'blocks.9.attn.q_lora_A.weight', 'blocks.9.attn.k_lora_A.weight', 'blocks.9.attn.v_lora_A.weight', 'blocks.9.attn.q_lora_B.weight', 'blocks.9.attn.k_lora_B.weight', 'blocks.9.attn.v_lora_B.weight', 'blocks.9.attn.o_lora_A.weight', 'blocks.9.attn.o_lora_B.weight', 'blocks.9.attn.qkv.weight', 'blocks.9.attn.qkv.bias', 'blocks.9.attn.out_proj.weight', 'blocks.9.attn.out_proj.bias', 'blocks.9.ln2.weight', 'blocks.9.ln2.bias', 'blocks.9.proj_up_lora_A.weight', 'blocks.9.proj_up_lora_B.weight', 'blocks.9.proj_down_lora_A.weight', 'blocks.9.proj_down_lora_B.weight', 'blocks.9.mlp.0.weight', 'blocks.9.mlp.0.bias', 'blocks.9.mlp.2.weight', 'blocks.9.mlp.2.bias', 'blocks.10.ln1.weight', 'blocks.10.ln1.bias', 'blocks.10.attn.q_lora_A.weight', 'blocks.10.attn.k_lora_A.weight', 'blocks.10.attn.v_lora_A.weight', 'blocks.10.attn.q_lora_B.weight', 'blocks.10.attn.k_lora_B.weight', 'blocks.10.attn.v_lora_B.weight', 'blocks.10.attn.o_lora_A.weight', 'blocks.10.attn.o_lora_B.weight', 'blocks.10.attn.qkv.weight', 'blocks.10.attn.qkv.bias', 'blocks.10.attn.out_proj.weight', 'blocks.10.attn.out_proj.bias', 'blocks.10.ln2.weight', 'blocks.10.ln2.bias', 'blocks.10.proj_up_lora_A.weight', 'blocks.10.proj_up_lora_B.weight', 'blocks.10.proj_down_lora_A.weight', 'blocks.10.proj_down_lora_B.weight', 'blocks.10.mlp.0.weight', 'blocks.10.mlp.0.bias', 'blocks.10.mlp.2.weight', 'blocks.10.mlp.2.bias', 'blocks.11.ln1.weight', 'blocks.11.ln1.bias', 'blocks.11.attn.q_lora_A.weight', 'blocks.11.attn.k_lora_A.weight', 'blocks.11.attn.v_lora_A.weight', 'blocks.11.attn.q_lora_B.weight', 'blocks.11.attn.k_lora_B.weight', 'blocks.11.attn.v_lora_B.weight', 'blocks.11.attn.o_lora_A.weight', 'blocks.11.attn.o_lora_B.weight', 'blocks.11.attn.qkv.weight', 'blocks.11.attn.qkv.bias', 'blocks.11.attn.out_proj.weight', 'blocks.11.attn.out_proj.bias', 'blocks.11.ln2.weight', 'blocks.11.ln2.bias', 'blocks.11.proj_up_lora_A.weight', 'blocks.11.proj_up_lora_B.weight', 'blocks.11.proj_down_lora_A.weight', 'blocks.11.proj_down_lora_B.weight', 'blocks.11.mlp.0.weight', 'blocks.11.mlp.0.bias', 'blocks.11.mlp.2.weight', 'blocks.11.mlp.2.bias', 'lm_head.weight', 'lm_head.bias'])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params_np.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['0_1_embedding_1_embed', '0_1_embedding_1_pe', 'transformer_1_linear_1_qkv_weight', 'transformer_1_linear_1_qkv_bias', 'transformer_1_linear_2_o_weight', 'transformer_1_linear_2_o_bias', 'transformer_1_linear_3_q_lora_A_weight', 'transformer_1_linear_4_k_lora_A_weight', 'transformer_1_linear_5_v_lora_A_weight', 'transformer_1_linear_6_q_lora_B_weight', 'transformer_1_linear_7_k_lora_B_weight', 'transformer_1_linear_8_v_lora_B_weight', 'transformer_1_linear_9_o_lora_A_weight', 'transformer_1_linear_10_o_lora_B_weight', 'transformer_1_linear_11_proj_up_lora_A_weight', 'transformer_1_linear_12_proj_up_lora_B_weight', 'transformer_1_linear_13_proj_down_lora_A_weight', 'transformer_1_linear_14_proj_down_lora_B_weight', 'transformer_1_linear_15_proj_up_weight', 'transformer_1_linear_15_proj_up_bias', 'transformer_1_linear_16_proj_down_weight', 'transformer_1_linear_16_proj_down_bias', 'transformer_1_layernorm_1_gamma', 'transformer_1_layernorm_1_beta', 'transformer_1_layernorm_2_gamma', 'transformer_1_layernorm_2_beta', 'transformer_2_linear_1_qkv_weight', 'transformer_2_linear_1_qkv_bias', 'transformer_2_linear_2_o_weight', 'transformer_2_linear_2_o_bias', 'transformer_2_linear_3_q_lora_A_weight', 'transformer_2_linear_4_k_lora_A_weight', 'transformer_2_linear_5_v_lora_A_weight', 'transformer_2_linear_6_q_lora_B_weight', 'transformer_2_linear_7_k_lora_B_weight', 'transformer_2_linear_8_v_lora_B_weight', 'transformer_2_linear_9_o_lora_A_weight', 'transformer_2_linear_10_o_lora_B_weight', 'transformer_2_linear_11_proj_up_lora_A_weight', 'transformer_2_linear_12_proj_up_lora_B_weight', 'transformer_2_linear_13_proj_down_lora_A_weight', 'transformer_2_linear_14_proj_down_lora_B_weight', 'transformer_2_linear_15_proj_up_weight', 'transformer_2_linear_15_proj_up_bias', 'transformer_2_linear_16_proj_down_weight', 'transformer_2_linear_16_proj_down_bias', 'transformer_2_layernorm_1_gamma', 'transformer_2_layernorm_1_beta', 'transformer_2_layernorm_2_gamma', 'transformer_2_layernorm_2_beta', 'transformer_3_linear_1_qkv_weight', 'transformer_3_linear_1_qkv_bias', 'transformer_3_linear_2_o_weight', 'transformer_3_linear_2_o_bias', 'transformer_3_linear_3_q_lora_A_weight', 'transformer_3_linear_4_k_lora_A_weight', 'transformer_3_linear_5_v_lora_A_weight', 'transformer_3_linear_6_q_lora_B_weight', 'transformer_3_linear_7_k_lora_B_weight', 'transformer_3_linear_8_v_lora_B_weight', 'transformer_3_linear_9_o_lora_A_weight', 'transformer_3_linear_10_o_lora_B_weight', 'transformer_3_linear_11_proj_up_lora_A_weight', 'transformer_3_linear_12_proj_up_lora_B_weight', 'transformer_3_linear_13_proj_down_lora_A_weight', 'transformer_3_linear_14_proj_down_lora_B_weight', 'transformer_3_linear_15_proj_up_weight', 'transformer_3_linear_15_proj_up_bias', 'transformer_3_linear_16_proj_down_weight', 'transformer_3_linear_16_proj_down_bias', 'transformer_3_layernorm_1_gamma', 'transformer_3_layernorm_1_beta', 'transformer_3_layernorm_2_gamma', 'transformer_3_layernorm_2_beta', 'transformer_4_linear_1_qkv_weight', 'transformer_4_linear_1_qkv_bias', 'transformer_4_linear_2_o_weight', 'transformer_4_linear_2_o_bias', 'transformer_4_linear_3_q_lora_A_weight', 'transformer_4_linear_4_k_lora_A_weight', 'transformer_4_linear_5_v_lora_A_weight', 'transformer_4_linear_6_q_lora_B_weight', 'transformer_4_linear_7_k_lora_B_weight', 'transformer_4_linear_8_v_lora_B_weight', 'transformer_4_linear_9_o_lora_A_weight', 'transformer_4_linear_10_o_lora_B_weight', 'transformer_4_linear_11_proj_up_lora_A_weight', 'transformer_4_linear_12_proj_up_lora_B_weight', 'transformer_4_linear_13_proj_down_lora_A_weight', 'transformer_4_linear_14_proj_down_lora_B_weight', 'transformer_4_linear_15_proj_up_weight', 'transformer_4_linear_15_proj_up_bias', 'transformer_4_linear_16_proj_down_weight', 'transformer_4_linear_16_proj_down_bias', 'transformer_4_layernorm_1_gamma', 'transformer_4_layernorm_1_beta', 'transformer_4_layernorm_2_gamma', 'transformer_4_layernorm_2_beta', 'transformer_5_linear_1_qkv_weight', 'transformer_5_linear_1_qkv_bias', 'transformer_5_linear_2_o_weight', 'transformer_5_linear_2_o_bias', 'transformer_5_linear_3_q_lora_A_weight', 'transformer_5_linear_4_k_lora_A_weight', 'transformer_5_linear_5_v_lora_A_weight', 'transformer_5_linear_6_q_lora_B_weight', 'transformer_5_linear_7_k_lora_B_weight', 'transformer_5_linear_8_v_lora_B_weight', 'transformer_5_linear_9_o_lora_A_weight', 'transformer_5_linear_10_o_lora_B_weight', 'transformer_5_linear_11_proj_up_lora_A_weight', 'transformer_5_linear_12_proj_up_lora_B_weight', 'transformer_5_linear_13_proj_down_lora_A_weight', 'transformer_5_linear_14_proj_down_lora_B_weight', 'transformer_5_linear_15_proj_up_weight', 'transformer_5_linear_15_proj_up_bias', 'transformer_5_linear_16_proj_down_weight', 'transformer_5_linear_16_proj_down_bias', 'transformer_5_layernorm_1_gamma', 'transformer_5_layernorm_1_beta', 'transformer_5_layernorm_2_gamma', 'transformer_5_layernorm_2_beta', 'transformer_6_linear_1_qkv_weight', 'transformer_6_linear_1_qkv_bias', 'transformer_6_linear_2_o_weight', 'transformer_6_linear_2_o_bias', 'transformer_6_linear_3_q_lora_A_weight', 'transformer_6_linear_4_k_lora_A_weight', 'transformer_6_linear_5_v_lora_A_weight', 'transformer_6_linear_6_q_lora_B_weight', 'transformer_6_linear_7_k_lora_B_weight', 'transformer_6_linear_8_v_lora_B_weight', 'transformer_6_linear_9_o_lora_A_weight', 'transformer_6_linear_10_o_lora_B_weight', 'transformer_6_linear_11_proj_up_lora_A_weight', 'transformer_6_linear_12_proj_up_lora_B_weight', 'transformer_6_linear_13_proj_down_lora_A_weight', 'transformer_6_linear_14_proj_down_lora_B_weight', 'transformer_6_linear_15_proj_up_weight', 'transformer_6_linear_15_proj_up_bias', 'transformer_6_linear_16_proj_down_weight', 'transformer_6_linear_16_proj_down_bias', 'transformer_6_layernorm_1_gamma', 'transformer_6_layernorm_1_beta', 'transformer_6_layernorm_2_gamma', 'transformer_6_layernorm_2_beta', 'transformer_7_linear_1_qkv_weight', 'transformer_7_linear_1_qkv_bias', 'transformer_7_linear_2_o_weight', 'transformer_7_linear_2_o_bias', 'transformer_7_linear_3_q_lora_A_weight', 'transformer_7_linear_4_k_lora_A_weight', 'transformer_7_linear_5_v_lora_A_weight', 'transformer_7_linear_6_q_lora_B_weight', 'transformer_7_linear_7_k_lora_B_weight', 'transformer_7_linear_8_v_lora_B_weight', 'transformer_7_linear_9_o_lora_A_weight', 'transformer_7_linear_10_o_lora_B_weight', 'transformer_7_linear_11_proj_up_lora_A_weight', 'transformer_7_linear_12_proj_up_lora_B_weight', 'transformer_7_linear_13_proj_down_lora_A_weight', 'transformer_7_linear_14_proj_down_lora_B_weight', 'transformer_7_linear_15_proj_up_weight', 'transformer_7_linear_15_proj_up_bias', 'transformer_7_linear_16_proj_down_weight', 'transformer_7_linear_16_proj_down_bias', 'transformer_7_layernorm_1_gamma', 'transformer_7_layernorm_1_beta', 'transformer_7_layernorm_2_gamma', 'transformer_7_layernorm_2_beta', 'transformer_8_linear_1_qkv_weight', 'transformer_8_linear_1_qkv_bias', 'transformer_8_linear_2_o_weight', 'transformer_8_linear_2_o_bias', 'transformer_8_linear_3_q_lora_A_weight', 'transformer_8_linear_4_k_lora_A_weight', 'transformer_8_linear_5_v_lora_A_weight', 'transformer_8_linear_6_q_lora_B_weight', 'transformer_8_linear_7_k_lora_B_weight', 'transformer_8_linear_8_v_lora_B_weight', 'transformer_8_linear_9_o_lora_A_weight', 'transformer_8_linear_10_o_lora_B_weight', 'transformer_8_linear_11_proj_up_lora_A_weight', 'transformer_8_linear_12_proj_up_lora_B_weight', 'transformer_8_linear_13_proj_down_lora_A_weight', 'transformer_8_linear_14_proj_down_lora_B_weight', 'transformer_8_linear_15_proj_up_weight', 'transformer_8_linear_15_proj_up_bias', 'transformer_8_linear_16_proj_down_weight', 'transformer_8_linear_16_proj_down_bias', 'transformer_8_layernorm_1_gamma', 'transformer_8_layernorm_1_beta', 'transformer_8_layernorm_2_gamma', 'transformer_8_layernorm_2_beta', 'transformer_9_linear_1_qkv_weight', 'transformer_9_linear_1_qkv_bias', 'transformer_9_linear_2_o_weight', 'transformer_9_linear_2_o_bias', 'transformer_9_linear_3_q_lora_A_weight', 'transformer_9_linear_4_k_lora_A_weight', 'transformer_9_linear_5_v_lora_A_weight', 'transformer_9_linear_6_q_lora_B_weight', 'transformer_9_linear_7_k_lora_B_weight', 'transformer_9_linear_8_v_lora_B_weight', 'transformer_9_linear_9_o_lora_A_weight', 'transformer_9_linear_10_o_lora_B_weight', 'transformer_9_linear_11_proj_up_lora_A_weight', 'transformer_9_linear_12_proj_up_lora_B_weight', 'transformer_9_linear_13_proj_down_lora_A_weight', 'transformer_9_linear_14_proj_down_lora_B_weight', 'transformer_9_linear_15_proj_up_weight', 'transformer_9_linear_15_proj_up_bias', 'transformer_9_linear_16_proj_down_weight', 'transformer_9_linear_16_proj_down_bias', 'transformer_9_layernorm_1_gamma', 'transformer_9_layernorm_1_beta', 'transformer_9_layernorm_2_gamma', 'transformer_9_layernorm_2_beta', 'transformer_10_linear_1_qkv_weight', 'transformer_10_linear_1_qkv_bias', 'transformer_10_linear_2_o_weight', 'transformer_10_linear_2_o_bias', 'transformer_10_linear_3_q_lora_A_weight', 'transformer_10_linear_4_k_lora_A_weight', 'transformer_10_linear_5_v_lora_A_weight', 'transformer_10_linear_6_q_lora_B_weight', 'transformer_10_linear_7_k_lora_B_weight', 'transformer_10_linear_8_v_lora_B_weight', 'transformer_10_linear_9_o_lora_A_weight', 'transformer_10_linear_10_o_lora_B_weight', 'transformer_10_linear_11_proj_up_lora_A_weight', 'transformer_10_linear_12_proj_up_lora_B_weight', 'transformer_10_linear_13_proj_down_lora_A_weight', 'transformer_10_linear_14_proj_down_lora_B_weight', 'transformer_10_linear_15_proj_up_weight', 'transformer_10_linear_15_proj_up_bias', 'transformer_10_linear_16_proj_down_weight', 'transformer_10_linear_16_proj_down_bias', 'transformer_10_layernorm_1_gamma', 'transformer_10_layernorm_1_beta', 'transformer_10_layernorm_2_gamma', 'transformer_10_layernorm_2_beta', 'transformer_11_linear_1_qkv_weight', 'transformer_11_linear_1_qkv_bias', 'transformer_11_linear_2_o_weight', 'transformer_11_linear_2_o_bias', 'transformer_11_linear_3_q_lora_A_weight', 'transformer_11_linear_4_k_lora_A_weight', 'transformer_11_linear_5_v_lora_A_weight', 'transformer_11_linear_6_q_lora_B_weight', 'transformer_11_linear_7_k_lora_B_weight', 'transformer_11_linear_8_v_lora_B_weight', 'transformer_11_linear_9_o_lora_A_weight', 'transformer_11_linear_10_o_lora_B_weight', 'transformer_11_linear_11_proj_up_lora_A_weight', 'transformer_11_linear_12_proj_up_lora_B_weight', 'transformer_11_linear_13_proj_down_lora_A_weight', 'transformer_11_linear_14_proj_down_lora_B_weight', 'transformer_11_linear_15_proj_up_weight', 'transformer_11_linear_15_proj_up_bias', 'transformer_11_linear_16_proj_down_weight', 'transformer_11_linear_16_proj_down_bias', 'transformer_11_layernorm_1_gamma', 'transformer_11_layernorm_1_beta', 'transformer_11_layernorm_2_gamma', 'transformer_11_layernorm_2_beta', 'transformer_12_linear_1_qkv_weight', 'transformer_12_linear_1_qkv_bias', 'transformer_12_linear_2_o_weight', 'transformer_12_linear_2_o_bias', 'transformer_12_linear_3_q_lora_A_weight', 'transformer_12_linear_4_k_lora_A_weight', 'transformer_12_linear_5_v_lora_A_weight', 'transformer_12_linear_6_q_lora_B_weight', 'transformer_12_linear_7_k_lora_B_weight', 'transformer_12_linear_8_v_lora_B_weight', 'transformer_12_linear_9_o_lora_A_weight', 'transformer_12_linear_10_o_lora_B_weight', 'transformer_12_linear_11_proj_up_lora_A_weight', 'transformer_12_linear_12_proj_up_lora_B_weight', 'transformer_12_linear_13_proj_down_lora_A_weight', 'transformer_12_linear_14_proj_down_lora_B_weight', 'transformer_12_linear_15_proj_up_weight', 'transformer_12_linear_15_proj_up_bias', 'transformer_12_linear_16_proj_down_weight', 'transformer_12_linear_16_proj_down_bias', 'transformer_12_layernorm_1_gamma', 'transformer_12_layernorm_1_beta', 'transformer_12_layernorm_2_gamma', 'transformer_12_layernorm_2_beta', 'linear_1_linear_1_project_weight', 'linear_1_linear_1_project_bias'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dlx_model.parameters().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5438633",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133df7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in torch_model.named_parameters():\n",
    "    print(k, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "32465fed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "257575394"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dlx_model.num_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bf6485e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = AdamW(dlx_model.parameters(), precision=(np.float32, np.float32))\n",
    "optim.load_state(\"checkpoints/pretraining\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd846644",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpt1.torch_train import model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c2562305",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['pos_emb', 'token_emb.weight', 'blocks.0.ln1.weight', 'blocks.0.ln1.bias', 'blocks.0.attn.q_lora_A.weight', 'blocks.0.attn.k_lora_A.weight', 'blocks.0.attn.v_lora_A.weight', 'blocks.0.attn.q_lora_B.weight', 'blocks.0.attn.k_lora_B.weight', 'blocks.0.attn.v_lora_B.weight', 'blocks.0.attn.o_lora_A.weight', 'blocks.0.attn.o_lora_B.weight', 'blocks.0.attn.qkv.weight', 'blocks.0.attn.qkv.bias', 'blocks.0.attn.out_proj.weight', 'blocks.0.attn.out_proj.bias', 'blocks.0.ln2.weight', 'blocks.0.ln2.bias', 'blocks.0.proj_up_lora_A.weight', 'blocks.0.proj_up_lora_B.weight', 'blocks.0.proj_down_lora_A.weight', 'blocks.0.proj_down_lora_B.weight', 'blocks.0.mlp.0.weight', 'blocks.0.mlp.0.bias', 'blocks.0.mlp.2.weight', 'blocks.0.mlp.2.bias', 'blocks.1.ln1.weight', 'blocks.1.ln1.bias', 'blocks.1.attn.q_lora_A.weight', 'blocks.1.attn.k_lora_A.weight', 'blocks.1.attn.v_lora_A.weight', 'blocks.1.attn.q_lora_B.weight', 'blocks.1.attn.k_lora_B.weight', 'blocks.1.attn.v_lora_B.weight', 'blocks.1.attn.o_lora_A.weight', 'blocks.1.attn.o_lora_B.weight', 'blocks.1.attn.qkv.weight', 'blocks.1.attn.qkv.bias', 'blocks.1.attn.out_proj.weight', 'blocks.1.attn.out_proj.bias', 'blocks.1.ln2.weight', 'blocks.1.ln2.bias', 'blocks.1.proj_up_lora_A.weight', 'blocks.1.proj_up_lora_B.weight', 'blocks.1.proj_down_lora_A.weight', 'blocks.1.proj_down_lora_B.weight', 'blocks.1.mlp.0.weight', 'blocks.1.mlp.0.bias', 'blocks.1.mlp.2.weight', 'blocks.1.mlp.2.bias', 'blocks.2.ln1.weight', 'blocks.2.ln1.bias', 'blocks.2.attn.q_lora_A.weight', 'blocks.2.attn.k_lora_A.weight', 'blocks.2.attn.v_lora_A.weight', 'blocks.2.attn.q_lora_B.weight', 'blocks.2.attn.k_lora_B.weight', 'blocks.2.attn.v_lora_B.weight', 'blocks.2.attn.o_lora_A.weight', 'blocks.2.attn.o_lora_B.weight', 'blocks.2.attn.qkv.weight', 'blocks.2.attn.qkv.bias', 'blocks.2.attn.out_proj.weight', 'blocks.2.attn.out_proj.bias', 'blocks.2.ln2.weight', 'blocks.2.ln2.bias', 'blocks.2.proj_up_lora_A.weight', 'blocks.2.proj_up_lora_B.weight', 'blocks.2.proj_down_lora_A.weight', 'blocks.2.proj_down_lora_B.weight', 'blocks.2.mlp.0.weight', 'blocks.2.mlp.0.bias', 'blocks.2.mlp.2.weight', 'blocks.2.mlp.2.bias', 'blocks.3.ln1.weight', 'blocks.3.ln1.bias', 'blocks.3.attn.q_lora_A.weight', 'blocks.3.attn.k_lora_A.weight', 'blocks.3.attn.v_lora_A.weight', 'blocks.3.attn.q_lora_B.weight', 'blocks.3.attn.k_lora_B.weight', 'blocks.3.attn.v_lora_B.weight', 'blocks.3.attn.o_lora_A.weight', 'blocks.3.attn.o_lora_B.weight', 'blocks.3.attn.qkv.weight', 'blocks.3.attn.qkv.bias', 'blocks.3.attn.out_proj.weight', 'blocks.3.attn.out_proj.bias', 'blocks.3.ln2.weight', 'blocks.3.ln2.bias', 'blocks.3.proj_up_lora_A.weight', 'blocks.3.proj_up_lora_B.weight', 'blocks.3.proj_down_lora_A.weight', 'blocks.3.proj_down_lora_B.weight', 'blocks.3.mlp.0.weight', 'blocks.3.mlp.0.bias', 'blocks.3.mlp.2.weight', 'blocks.3.mlp.2.bias', 'blocks.4.ln1.weight', 'blocks.4.ln1.bias', 'blocks.4.attn.q_lora_A.weight', 'blocks.4.attn.k_lora_A.weight', 'blocks.4.attn.v_lora_A.weight', 'blocks.4.attn.q_lora_B.weight', 'blocks.4.attn.k_lora_B.weight', 'blocks.4.attn.v_lora_B.weight', 'blocks.4.attn.o_lora_A.weight', 'blocks.4.attn.o_lora_B.weight', 'blocks.4.attn.qkv.weight', 'blocks.4.attn.qkv.bias', 'blocks.4.attn.out_proj.weight', 'blocks.4.attn.out_proj.bias', 'blocks.4.ln2.weight', 'blocks.4.ln2.bias', 'blocks.4.proj_up_lora_A.weight', 'blocks.4.proj_up_lora_B.weight', 'blocks.4.proj_down_lora_A.weight', 'blocks.4.proj_down_lora_B.weight', 'blocks.4.mlp.0.weight', 'blocks.4.mlp.0.bias', 'blocks.4.mlp.2.weight', 'blocks.4.mlp.2.bias', 'blocks.5.ln1.weight', 'blocks.5.ln1.bias', 'blocks.5.attn.q_lora_A.weight', 'blocks.5.attn.k_lora_A.weight', 'blocks.5.attn.v_lora_A.weight', 'blocks.5.attn.q_lora_B.weight', 'blocks.5.attn.k_lora_B.weight', 'blocks.5.attn.v_lora_B.weight', 'blocks.5.attn.o_lora_A.weight', 'blocks.5.attn.o_lora_B.weight', 'blocks.5.attn.qkv.weight', 'blocks.5.attn.qkv.bias', 'blocks.5.attn.out_proj.weight', 'blocks.5.attn.out_proj.bias', 'blocks.5.ln2.weight', 'blocks.5.ln2.bias', 'blocks.5.proj_up_lora_A.weight', 'blocks.5.proj_up_lora_B.weight', 'blocks.5.proj_down_lora_A.weight', 'blocks.5.proj_down_lora_B.weight', 'blocks.5.mlp.0.weight', 'blocks.5.mlp.0.bias', 'blocks.5.mlp.2.weight', 'blocks.5.mlp.2.bias', 'blocks.6.ln1.weight', 'blocks.6.ln1.bias', 'blocks.6.attn.q_lora_A.weight', 'blocks.6.attn.k_lora_A.weight', 'blocks.6.attn.v_lora_A.weight', 'blocks.6.attn.q_lora_B.weight', 'blocks.6.attn.k_lora_B.weight', 'blocks.6.attn.v_lora_B.weight', 'blocks.6.attn.o_lora_A.weight', 'blocks.6.attn.o_lora_B.weight', 'blocks.6.attn.qkv.weight', 'blocks.6.attn.qkv.bias', 'blocks.6.attn.out_proj.weight', 'blocks.6.attn.out_proj.bias', 'blocks.6.ln2.weight', 'blocks.6.ln2.bias', 'blocks.6.proj_up_lora_A.weight', 'blocks.6.proj_up_lora_B.weight', 'blocks.6.proj_down_lora_A.weight', 'blocks.6.proj_down_lora_B.weight', 'blocks.6.mlp.0.weight', 'blocks.6.mlp.0.bias', 'blocks.6.mlp.2.weight', 'blocks.6.mlp.2.bias', 'blocks.7.ln1.weight', 'blocks.7.ln1.bias', 'blocks.7.attn.q_lora_A.weight', 'blocks.7.attn.k_lora_A.weight', 'blocks.7.attn.v_lora_A.weight', 'blocks.7.attn.q_lora_B.weight', 'blocks.7.attn.k_lora_B.weight', 'blocks.7.attn.v_lora_B.weight', 'blocks.7.attn.o_lora_A.weight', 'blocks.7.attn.o_lora_B.weight', 'blocks.7.attn.qkv.weight', 'blocks.7.attn.qkv.bias', 'blocks.7.attn.out_proj.weight', 'blocks.7.attn.out_proj.bias', 'blocks.7.ln2.weight', 'blocks.7.ln2.bias', 'blocks.7.proj_up_lora_A.weight', 'blocks.7.proj_up_lora_B.weight', 'blocks.7.proj_down_lora_A.weight', 'blocks.7.proj_down_lora_B.weight', 'blocks.7.mlp.0.weight', 'blocks.7.mlp.0.bias', 'blocks.7.mlp.2.weight', 'blocks.7.mlp.2.bias', 'blocks.8.ln1.weight', 'blocks.8.ln1.bias', 'blocks.8.attn.q_lora_A.weight', 'blocks.8.attn.k_lora_A.weight', 'blocks.8.attn.v_lora_A.weight', 'blocks.8.attn.q_lora_B.weight', 'blocks.8.attn.k_lora_B.weight', 'blocks.8.attn.v_lora_B.weight', 'blocks.8.attn.o_lora_A.weight', 'blocks.8.attn.o_lora_B.weight', 'blocks.8.attn.qkv.weight', 'blocks.8.attn.qkv.bias', 'blocks.8.attn.out_proj.weight', 'blocks.8.attn.out_proj.bias', 'blocks.8.ln2.weight', 'blocks.8.ln2.bias', 'blocks.8.proj_up_lora_A.weight', 'blocks.8.proj_up_lora_B.weight', 'blocks.8.proj_down_lora_A.weight', 'blocks.8.proj_down_lora_B.weight', 'blocks.8.mlp.0.weight', 'blocks.8.mlp.0.bias', 'blocks.8.mlp.2.weight', 'blocks.8.mlp.2.bias', 'blocks.9.ln1.weight', 'blocks.9.ln1.bias', 'blocks.9.attn.q_lora_A.weight', 'blocks.9.attn.k_lora_A.weight', 'blocks.9.attn.v_lora_A.weight', 'blocks.9.attn.q_lora_B.weight', 'blocks.9.attn.k_lora_B.weight', 'blocks.9.attn.v_lora_B.weight', 'blocks.9.attn.o_lora_A.weight', 'blocks.9.attn.o_lora_B.weight', 'blocks.9.attn.qkv.weight', 'blocks.9.attn.qkv.bias', 'blocks.9.attn.out_proj.weight', 'blocks.9.attn.out_proj.bias', 'blocks.9.ln2.weight', 'blocks.9.ln2.bias', 'blocks.9.proj_up_lora_A.weight', 'blocks.9.proj_up_lora_B.weight', 'blocks.9.proj_down_lora_A.weight', 'blocks.9.proj_down_lora_B.weight', 'blocks.9.mlp.0.weight', 'blocks.9.mlp.0.bias', 'blocks.9.mlp.2.weight', 'blocks.9.mlp.2.bias', 'blocks.10.ln1.weight', 'blocks.10.ln1.bias', 'blocks.10.attn.q_lora_A.weight', 'blocks.10.attn.k_lora_A.weight', 'blocks.10.attn.v_lora_A.weight', 'blocks.10.attn.q_lora_B.weight', 'blocks.10.attn.k_lora_B.weight', 'blocks.10.attn.v_lora_B.weight', 'blocks.10.attn.o_lora_A.weight', 'blocks.10.attn.o_lora_B.weight', 'blocks.10.attn.qkv.weight', 'blocks.10.attn.qkv.bias', 'blocks.10.attn.out_proj.weight', 'blocks.10.attn.out_proj.bias', 'blocks.10.ln2.weight', 'blocks.10.ln2.bias', 'blocks.10.proj_up_lora_A.weight', 'blocks.10.proj_up_lora_B.weight', 'blocks.10.proj_down_lora_A.weight', 'blocks.10.proj_down_lora_B.weight', 'blocks.10.mlp.0.weight', 'blocks.10.mlp.0.bias', 'blocks.10.mlp.2.weight', 'blocks.10.mlp.2.bias', 'blocks.11.ln1.weight', 'blocks.11.ln1.bias', 'blocks.11.attn.q_lora_A.weight', 'blocks.11.attn.k_lora_A.weight', 'blocks.11.attn.v_lora_A.weight', 'blocks.11.attn.q_lora_B.weight', 'blocks.11.attn.k_lora_B.weight', 'blocks.11.attn.v_lora_B.weight', 'blocks.11.attn.o_lora_A.weight', 'blocks.11.attn.o_lora_B.weight', 'blocks.11.attn.qkv.weight', 'blocks.11.attn.qkv.bias', 'blocks.11.attn.out_proj.weight', 'blocks.11.attn.out_proj.bias', 'blocks.11.ln2.weight', 'blocks.11.ln2.bias', 'blocks.11.proj_up_lora_A.weight', 'blocks.11.proj_up_lora_B.weight', 'blocks.11.proj_down_lora_A.weight', 'blocks.11.proj_down_lora_B.weight', 'blocks.11.mlp.0.weight', 'blocks.11.mlp.0.bias', 'blocks.11.mlp.2.weight', 'blocks.11.mlp.2.bias', 'lm_head.weight', 'lm_head.bias'])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params_np.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8d41df94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['0_1_embedding_1_embed', '0_1_embedding_1_pe', 'transformer_1_linear_1_qkv_weight', 'transformer_1_linear_1_qkv_bias', 'transformer_1_linear_2_o_weight', 'transformer_1_linear_2_o_bias', 'transformer_1_linear_3_q_lora_A_weight', 'transformer_1_linear_4_k_lora_A_weight', 'transformer_1_linear_5_v_lora_A_weight', 'transformer_1_linear_6_q_lora_B_weight', 'transformer_1_linear_7_k_lora_B_weight', 'transformer_1_linear_8_v_lora_B_weight', 'transformer_1_linear_9_o_lora_A_weight', 'transformer_1_linear_10_o_lora_B_weight', 'transformer_1_linear_11_proj_up_lora_A_weight', 'transformer_1_linear_12_proj_up_lora_B_weight', 'transformer_1_linear_13_proj_down_lora_A_weight', 'transformer_1_linear_14_proj_down_lora_B_weight', 'transformer_1_linear_15_proj_up_weight', 'transformer_1_linear_15_proj_up_bias', 'transformer_1_linear_16_proj_down_weight', 'transformer_1_linear_16_proj_down_bias', 'transformer_1_layernorm_1_gamma', 'transformer_1_layernorm_1_beta', 'transformer_1_layernorm_2_gamma', 'transformer_1_layernorm_2_beta', 'transformer_2_linear_1_qkv_weight', 'transformer_2_linear_1_qkv_bias', 'transformer_2_linear_2_o_weight', 'transformer_2_linear_2_o_bias', 'transformer_2_linear_3_q_lora_A_weight', 'transformer_2_linear_4_k_lora_A_weight', 'transformer_2_linear_5_v_lora_A_weight', 'transformer_2_linear_6_q_lora_B_weight', 'transformer_2_linear_7_k_lora_B_weight', 'transformer_2_linear_8_v_lora_B_weight', 'transformer_2_linear_9_o_lora_A_weight', 'transformer_2_linear_10_o_lora_B_weight', 'transformer_2_linear_11_proj_up_lora_A_weight', 'transformer_2_linear_12_proj_up_lora_B_weight', 'transformer_2_linear_13_proj_down_lora_A_weight', 'transformer_2_linear_14_proj_down_lora_B_weight', 'transformer_2_linear_15_proj_up_weight', 'transformer_2_linear_15_proj_up_bias', 'transformer_2_linear_16_proj_down_weight', 'transformer_2_linear_16_proj_down_bias', 'transformer_2_layernorm_1_gamma', 'transformer_2_layernorm_1_beta', 'transformer_2_layernorm_2_gamma', 'transformer_2_layernorm_2_beta', 'transformer_3_linear_1_qkv_weight', 'transformer_3_linear_1_qkv_bias', 'transformer_3_linear_2_o_weight', 'transformer_3_linear_2_o_bias', 'transformer_3_linear_3_q_lora_A_weight', 'transformer_3_linear_4_k_lora_A_weight', 'transformer_3_linear_5_v_lora_A_weight', 'transformer_3_linear_6_q_lora_B_weight', 'transformer_3_linear_7_k_lora_B_weight', 'transformer_3_linear_8_v_lora_B_weight', 'transformer_3_linear_9_o_lora_A_weight', 'transformer_3_linear_10_o_lora_B_weight', 'transformer_3_linear_11_proj_up_lora_A_weight', 'transformer_3_linear_12_proj_up_lora_B_weight', 'transformer_3_linear_13_proj_down_lora_A_weight', 'transformer_3_linear_14_proj_down_lora_B_weight', 'transformer_3_linear_15_proj_up_weight', 'transformer_3_linear_15_proj_up_bias', 'transformer_3_linear_16_proj_down_weight', 'transformer_3_linear_16_proj_down_bias', 'transformer_3_layernorm_1_gamma', 'transformer_3_layernorm_1_beta', 'transformer_3_layernorm_2_gamma', 'transformer_3_layernorm_2_beta', 'transformer_4_linear_1_qkv_weight', 'transformer_4_linear_1_qkv_bias', 'transformer_4_linear_2_o_weight', 'transformer_4_linear_2_o_bias', 'transformer_4_linear_3_q_lora_A_weight', 'transformer_4_linear_4_k_lora_A_weight', 'transformer_4_linear_5_v_lora_A_weight', 'transformer_4_linear_6_q_lora_B_weight', 'transformer_4_linear_7_k_lora_B_weight', 'transformer_4_linear_8_v_lora_B_weight', 'transformer_4_linear_9_o_lora_A_weight', 'transformer_4_linear_10_o_lora_B_weight', 'transformer_4_linear_11_proj_up_lora_A_weight', 'transformer_4_linear_12_proj_up_lora_B_weight', 'transformer_4_linear_13_proj_down_lora_A_weight', 'transformer_4_linear_14_proj_down_lora_B_weight', 'transformer_4_linear_15_proj_up_weight', 'transformer_4_linear_15_proj_up_bias', 'transformer_4_linear_16_proj_down_weight', 'transformer_4_linear_16_proj_down_bias', 'transformer_4_layernorm_1_gamma', 'transformer_4_layernorm_1_beta', 'transformer_4_layernorm_2_gamma', 'transformer_4_layernorm_2_beta', 'transformer_5_linear_1_qkv_weight', 'transformer_5_linear_1_qkv_bias', 'transformer_5_linear_2_o_weight', 'transformer_5_linear_2_o_bias', 'transformer_5_linear_3_q_lora_A_weight', 'transformer_5_linear_4_k_lora_A_weight', 'transformer_5_linear_5_v_lora_A_weight', 'transformer_5_linear_6_q_lora_B_weight', 'transformer_5_linear_7_k_lora_B_weight', 'transformer_5_linear_8_v_lora_B_weight', 'transformer_5_linear_9_o_lora_A_weight', 'transformer_5_linear_10_o_lora_B_weight', 'transformer_5_linear_11_proj_up_lora_A_weight', 'transformer_5_linear_12_proj_up_lora_B_weight', 'transformer_5_linear_13_proj_down_lora_A_weight', 'transformer_5_linear_14_proj_down_lora_B_weight', 'transformer_5_linear_15_proj_up_weight', 'transformer_5_linear_15_proj_up_bias', 'transformer_5_linear_16_proj_down_weight', 'transformer_5_linear_16_proj_down_bias', 'transformer_5_layernorm_1_gamma', 'transformer_5_layernorm_1_beta', 'transformer_5_layernorm_2_gamma', 'transformer_5_layernorm_2_beta', 'transformer_6_linear_1_qkv_weight', 'transformer_6_linear_1_qkv_bias', 'transformer_6_linear_2_o_weight', 'transformer_6_linear_2_o_bias', 'transformer_6_linear_3_q_lora_A_weight', 'transformer_6_linear_4_k_lora_A_weight', 'transformer_6_linear_5_v_lora_A_weight', 'transformer_6_linear_6_q_lora_B_weight', 'transformer_6_linear_7_k_lora_B_weight', 'transformer_6_linear_8_v_lora_B_weight', 'transformer_6_linear_9_o_lora_A_weight', 'transformer_6_linear_10_o_lora_B_weight', 'transformer_6_linear_11_proj_up_lora_A_weight', 'transformer_6_linear_12_proj_up_lora_B_weight', 'transformer_6_linear_13_proj_down_lora_A_weight', 'transformer_6_linear_14_proj_down_lora_B_weight', 'transformer_6_linear_15_proj_up_weight', 'transformer_6_linear_15_proj_up_bias', 'transformer_6_linear_16_proj_down_weight', 'transformer_6_linear_16_proj_down_bias', 'transformer_6_layernorm_1_gamma', 'transformer_6_layernorm_1_beta', 'transformer_6_layernorm_2_gamma', 'transformer_6_layernorm_2_beta', 'transformer_7_linear_1_qkv_weight', 'transformer_7_linear_1_qkv_bias', 'transformer_7_linear_2_o_weight', 'transformer_7_linear_2_o_bias', 'transformer_7_linear_3_q_lora_A_weight', 'transformer_7_linear_4_k_lora_A_weight', 'transformer_7_linear_5_v_lora_A_weight', 'transformer_7_linear_6_q_lora_B_weight', 'transformer_7_linear_7_k_lora_B_weight', 'transformer_7_linear_8_v_lora_B_weight', 'transformer_7_linear_9_o_lora_A_weight', 'transformer_7_linear_10_o_lora_B_weight', 'transformer_7_linear_11_proj_up_lora_A_weight', 'transformer_7_linear_12_proj_up_lora_B_weight', 'transformer_7_linear_13_proj_down_lora_A_weight', 'transformer_7_linear_14_proj_down_lora_B_weight', 'transformer_7_linear_15_proj_up_weight', 'transformer_7_linear_15_proj_up_bias', 'transformer_7_linear_16_proj_down_weight', 'transformer_7_linear_16_proj_down_bias', 'transformer_7_layernorm_1_gamma', 'transformer_7_layernorm_1_beta', 'transformer_7_layernorm_2_gamma', 'transformer_7_layernorm_2_beta', 'transformer_8_linear_1_qkv_weight', 'transformer_8_linear_1_qkv_bias', 'transformer_8_linear_2_o_weight', 'transformer_8_linear_2_o_bias', 'transformer_8_linear_3_q_lora_A_weight', 'transformer_8_linear_4_k_lora_A_weight', 'transformer_8_linear_5_v_lora_A_weight', 'transformer_8_linear_6_q_lora_B_weight', 'transformer_8_linear_7_k_lora_B_weight', 'transformer_8_linear_8_v_lora_B_weight', 'transformer_8_linear_9_o_lora_A_weight', 'transformer_8_linear_10_o_lora_B_weight', 'transformer_8_linear_11_proj_up_lora_A_weight', 'transformer_8_linear_12_proj_up_lora_B_weight', 'transformer_8_linear_13_proj_down_lora_A_weight', 'transformer_8_linear_14_proj_down_lora_B_weight', 'transformer_8_linear_15_proj_up_weight', 'transformer_8_linear_15_proj_up_bias', 'transformer_8_linear_16_proj_down_weight', 'transformer_8_linear_16_proj_down_bias', 'transformer_8_layernorm_1_gamma', 'transformer_8_layernorm_1_beta', 'transformer_8_layernorm_2_gamma', 'transformer_8_layernorm_2_beta', 'transformer_9_linear_1_qkv_weight', 'transformer_9_linear_1_qkv_bias', 'transformer_9_linear_2_o_weight', 'transformer_9_linear_2_o_bias', 'transformer_9_linear_3_q_lora_A_weight', 'transformer_9_linear_4_k_lora_A_weight', 'transformer_9_linear_5_v_lora_A_weight', 'transformer_9_linear_6_q_lora_B_weight', 'transformer_9_linear_7_k_lora_B_weight', 'transformer_9_linear_8_v_lora_B_weight', 'transformer_9_linear_9_o_lora_A_weight', 'transformer_9_linear_10_o_lora_B_weight', 'transformer_9_linear_11_proj_up_lora_A_weight', 'transformer_9_linear_12_proj_up_lora_B_weight', 'transformer_9_linear_13_proj_down_lora_A_weight', 'transformer_9_linear_14_proj_down_lora_B_weight', 'transformer_9_linear_15_proj_up_weight', 'transformer_9_linear_15_proj_up_bias', 'transformer_9_linear_16_proj_down_weight', 'transformer_9_linear_16_proj_down_bias', 'transformer_9_layernorm_1_gamma', 'transformer_9_layernorm_1_beta', 'transformer_9_layernorm_2_gamma', 'transformer_9_layernorm_2_beta', 'transformer_10_linear_1_qkv_weight', 'transformer_10_linear_1_qkv_bias', 'transformer_10_linear_2_o_weight', 'transformer_10_linear_2_o_bias', 'transformer_10_linear_3_q_lora_A_weight', 'transformer_10_linear_4_k_lora_A_weight', 'transformer_10_linear_5_v_lora_A_weight', 'transformer_10_linear_6_q_lora_B_weight', 'transformer_10_linear_7_k_lora_B_weight', 'transformer_10_linear_8_v_lora_B_weight', 'transformer_10_linear_9_o_lora_A_weight', 'transformer_10_linear_10_o_lora_B_weight', 'transformer_10_linear_11_proj_up_lora_A_weight', 'transformer_10_linear_12_proj_up_lora_B_weight', 'transformer_10_linear_13_proj_down_lora_A_weight', 'transformer_10_linear_14_proj_down_lora_B_weight', 'transformer_10_linear_15_proj_up_weight', 'transformer_10_linear_15_proj_up_bias', 'transformer_10_linear_16_proj_down_weight', 'transformer_10_linear_16_proj_down_bias', 'transformer_10_layernorm_1_gamma', 'transformer_10_layernorm_1_beta', 'transformer_10_layernorm_2_gamma', 'transformer_10_layernorm_2_beta', 'transformer_11_linear_1_qkv_weight', 'transformer_11_linear_1_qkv_bias', 'transformer_11_linear_2_o_weight', 'transformer_11_linear_2_o_bias', 'transformer_11_linear_3_q_lora_A_weight', 'transformer_11_linear_4_k_lora_A_weight', 'transformer_11_linear_5_v_lora_A_weight', 'transformer_11_linear_6_q_lora_B_weight', 'transformer_11_linear_7_k_lora_B_weight', 'transformer_11_linear_8_v_lora_B_weight', 'transformer_11_linear_9_o_lora_A_weight', 'transformer_11_linear_10_o_lora_B_weight', 'transformer_11_linear_11_proj_up_lora_A_weight', 'transformer_11_linear_12_proj_up_lora_B_weight', 'transformer_11_linear_13_proj_down_lora_A_weight', 'transformer_11_linear_14_proj_down_lora_B_weight', 'transformer_11_linear_15_proj_up_weight', 'transformer_11_linear_15_proj_up_bias', 'transformer_11_linear_16_proj_down_weight', 'transformer_11_linear_16_proj_down_bias', 'transformer_11_layernorm_1_gamma', 'transformer_11_layernorm_1_beta', 'transformer_11_layernorm_2_gamma', 'transformer_11_layernorm_2_beta', 'transformer_12_linear_1_qkv_weight', 'transformer_12_linear_1_qkv_bias', 'transformer_12_linear_2_o_weight', 'transformer_12_linear_2_o_bias', 'transformer_12_linear_3_q_lora_A_weight', 'transformer_12_linear_4_k_lora_A_weight', 'transformer_12_linear_5_v_lora_A_weight', 'transformer_12_linear_6_q_lora_B_weight', 'transformer_12_linear_7_k_lora_B_weight', 'transformer_12_linear_8_v_lora_B_weight', 'transformer_12_linear_9_o_lora_A_weight', 'transformer_12_linear_10_o_lora_B_weight', 'transformer_12_linear_11_proj_up_lora_A_weight', 'transformer_12_linear_12_proj_up_lora_B_weight', 'transformer_12_linear_13_proj_down_lora_A_weight', 'transformer_12_linear_14_proj_down_lora_B_weight', 'transformer_12_linear_15_proj_up_weight', 'transformer_12_linear_15_proj_up_bias', 'transformer_12_linear_16_proj_down_weight', 'transformer_12_linear_16_proj_down_bias', 'transformer_12_layernorm_1_gamma', 'transformer_12_layernorm_1_beta', 'transformer_12_layernorm_2_gamma', 'transformer_12_layernorm_2_beta', 'linear_1_linear_1_project_weight', 'linear_1_linear_1_project_bias'])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dlx_model.parameters().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "af65c1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_blocks = 12\n",
    "pt_to_my_lib = {\n",
    "    # embeddings\n",
    "    \"token_emb.weight\": \"0_1_embedding_1_embed\",\n",
    "    \"pos_emb\": \"0_1_embedding_1_pe\",\n",
    "    # lm head\n",
    "    \"lm_head.weight\": \"linear_1_linear_1_project_weight\",\n",
    "    \"lm_head.bias\": \"linear_1_linear_1_project_bias\",\n",
    "}\n",
    "\n",
    "for i in range(num_blocks):\n",
    "    lib_block_idx = i + 1  # your library blocks start at 1\n",
    "    pt_prefix = f\"blocks.{i}\"\n",
    "    lib_prefix = f\"transformer_{lib_block_idx}\"\n",
    "\n",
    "    # LayerNorm\n",
    "    pt_to_my_lib[f\"{pt_prefix}.ln1.weight\"] = f\"{lib_prefix}_layernorm_1_gamma\"\n",
    "    pt_to_my_lib[f\"{pt_prefix}.ln1.bias\"]   = f\"{lib_prefix}_layernorm_1_beta\"\n",
    "    pt_to_my_lib[f\"{pt_prefix}.ln2.weight\"] = f\"{lib_prefix}_layernorm_2_gamma\"\n",
    "    pt_to_my_lib[f\"{pt_prefix}.ln2.bias\"]   = f\"{lib_prefix}_layernorm_2_beta\"\n",
    "\n",
    "    # Attention - Base projections\n",
    "    pt_to_my_lib[f\"{pt_prefix}.attn.qkv.weight\"]     = f\"{lib_prefix}_linear_1_qkv_weight\"\n",
    "    pt_to_my_lib[f\"{pt_prefix}.attn.qkv.bias\"]       = f\"{lib_prefix}_linear_1_qkv_bias\"\n",
    "    pt_to_my_lib[f\"{pt_prefix}.attn.out_proj.weight\"]= f\"{lib_prefix}_linear_2_o_weight\"\n",
    "    pt_to_my_lib[f\"{pt_prefix}.attn.out_proj.bias\"]  = f\"{lib_prefix}_linear_2_o_bias\"\n",
    "\n",
    "    # Attention - LoRA adapters\n",
    "    pt_to_my_lib[f\"{pt_prefix}.attn.q_lora_A.weight\"] = f\"{lib_prefix}_linear_3_q_lora_A_weight\"\n",
    "    pt_to_my_lib[f\"{pt_prefix}.attn.k_lora_A.weight\"] = f\"{lib_prefix}_linear_4_k_lora_A_weight\"\n",
    "    pt_to_my_lib[f\"{pt_prefix}.attn.v_lora_A.weight\"] = f\"{lib_prefix}_linear_5_v_lora_A_weight\"\n",
    "    pt_to_my_lib[f\"{pt_prefix}.attn.q_lora_B.weight\"] = f\"{lib_prefix}_linear_6_q_lora_B_weight\"\n",
    "    pt_to_my_lib[f\"{pt_prefix}.attn.k_lora_B.weight\"] = f\"{lib_prefix}_linear_7_k_lora_B_weight\"\n",
    "    pt_to_my_lib[f\"{pt_prefix}.attn.v_lora_B.weight\"] = f\"{lib_prefix}_linear_8_v_lora_B_weight\"\n",
    "    pt_to_my_lib[f\"{pt_prefix}.attn.o_lora_A.weight\"] = f\"{lib_prefix}_linear_9_o_lora_A_weight\"\n",
    "    pt_to_my_lib[f\"{pt_prefix}.attn.o_lora_B.weight\"] = f\"{lib_prefix}_linear_10_o_lora_B_weight\"\n",
    "\n",
    "    # MLP - LoRA adapters\n",
    "    pt_to_my_lib[f\"{pt_prefix}.proj_up_lora_A.weight\"]   = f\"{lib_prefix}_linear_11_proj_up_lora_A_weight\"\n",
    "    pt_to_my_lib[f\"{pt_prefix}.proj_up_lora_B.weight\"]   = f\"{lib_prefix}_linear_12_proj_up_lora_B_weight\"\n",
    "    pt_to_my_lib[f\"{pt_prefix}.proj_down_lora_A.weight\"] = f\"{lib_prefix}_linear_13_proj_down_lora_A_weight\"\n",
    "    pt_to_my_lib[f\"{pt_prefix}.proj_down_lora_B.weight\"] = f\"{lib_prefix}_linear_14_proj_down_lora_B_weight\"\n",
    "\n",
    "    # MLP - Base projections (CORRECTED: should be linear_15 and linear_16)\n",
    "    pt_to_my_lib[f\"{pt_prefix}.mlp.0.weight\"] = f\"{lib_prefix}_linear_15_proj_up_weight\"\n",
    "    pt_to_my_lib[f\"{pt_prefix}.mlp.0.bias\"]   = f\"{lib_prefix}_linear_15_proj_up_bias\"\n",
    "    pt_to_my_lib[f\"{pt_prefix}.mlp.2.weight\"] = f\"{lib_prefix}_linear_16_proj_down_weight\"\n",
    "    pt_to_my_lib[f\"{pt_prefix}.mlp.2.bias\"]   = f\"{lib_prefix}_linear_16_proj_down_bias\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b09ade3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token_emb.weight': '0_1_embedding_1_embed',\n",
       " 'pos_emb': '0_1_embedding_1_pe',\n",
       " 'lm_head.weight': 'linear_1_linear_1_project_weight',\n",
       " 'lm_head.bias': 'linear_1_linear_1_project_bias',\n",
       " 'blocks.0.ln1.weight': 'transformer_1_layernorm_1_gamma',\n",
       " 'blocks.0.ln1.bias': 'transformer_1_layernorm_1_beta',\n",
       " 'blocks.0.ln2.weight': 'transformer_1_layernorm_2_gamma',\n",
       " 'blocks.0.ln2.bias': 'transformer_1_layernorm_2_beta',\n",
       " 'blocks.0.attn.qkv.weight': 'transformer_1_linear_1_qkv_weight',\n",
       " 'blocks.0.attn.qkv.bias': 'transformer_1_linear_1_qkv_bias',\n",
       " 'blocks.0.attn.out_proj.weight': 'transformer_1_linear_2_o_weight',\n",
       " 'blocks.0.attn.out_proj.bias': 'transformer_1_linear_2_o_bias',\n",
       " 'blocks.0.attn.q_lora_A.weight': 'transformer_1_linear_3_q_lora_A_weight',\n",
       " 'blocks.0.attn.k_lora_A.weight': 'transformer_1_linear_4_k_lora_A_weight',\n",
       " 'blocks.0.attn.v_lora_A.weight': 'transformer_1_linear_5_v_lora_A_weight',\n",
       " 'blocks.0.attn.q_lora_B.weight': 'transformer_1_linear_6_q_lora_B_weight',\n",
       " 'blocks.0.attn.k_lora_B.weight': 'transformer_1_linear_7_k_lora_B_weight',\n",
       " 'blocks.0.attn.v_lora_B.weight': 'transformer_1_linear_8_v_lora_B_weight',\n",
       " 'blocks.0.attn.o_lora_A.weight': 'transformer_1_linear_9_o_lora_A_weight',\n",
       " 'blocks.0.attn.o_lora_B.weight': 'transformer_1_linear_10_o_lora_B_weight',\n",
       " 'blocks.0.proj_up_lora_A.weight': 'transformer_1_linear_11_proj_up_lora_A_weight',\n",
       " 'blocks.0.proj_up_lora_B.weight': 'transformer_1_linear_12_proj_up_lora_B_weight',\n",
       " 'blocks.0.proj_down_lora_A.weight': 'transformer_1_linear_13_proj_down_lora_A_weight',\n",
       " 'blocks.0.proj_down_lora_B.weight': 'transformer_1_linear_14_proj_down_lora_B_weight',\n",
       " 'blocks.0.mlp.0.weight': 'transformer_1_linear_15_proj_up_weight',\n",
       " 'blocks.0.mlp.0.bias': 'transformer_1_linear_15_proj_up_bias',\n",
       " 'blocks.0.mlp.2.weight': 'transformer_1_linear_16_proj_down_weight',\n",
       " 'blocks.0.mlp.2.bias': 'transformer_1_linear_16_proj_down_bias',\n",
       " 'blocks.1.ln1.weight': 'transformer_2_layernorm_1_gamma',\n",
       " 'blocks.1.ln1.bias': 'transformer_2_layernorm_1_beta',\n",
       " 'blocks.1.ln2.weight': 'transformer_2_layernorm_2_gamma',\n",
       " 'blocks.1.ln2.bias': 'transformer_2_layernorm_2_beta',\n",
       " 'blocks.1.attn.qkv.weight': 'transformer_2_linear_1_qkv_weight',\n",
       " 'blocks.1.attn.qkv.bias': 'transformer_2_linear_1_qkv_bias',\n",
       " 'blocks.1.attn.out_proj.weight': 'transformer_2_linear_2_o_weight',\n",
       " 'blocks.1.attn.out_proj.bias': 'transformer_2_linear_2_o_bias',\n",
       " 'blocks.1.attn.q_lora_A.weight': 'transformer_2_linear_3_q_lora_A_weight',\n",
       " 'blocks.1.attn.k_lora_A.weight': 'transformer_2_linear_4_k_lora_A_weight',\n",
       " 'blocks.1.attn.v_lora_A.weight': 'transformer_2_linear_5_v_lora_A_weight',\n",
       " 'blocks.1.attn.q_lora_B.weight': 'transformer_2_linear_6_q_lora_B_weight',\n",
       " 'blocks.1.attn.k_lora_B.weight': 'transformer_2_linear_7_k_lora_B_weight',\n",
       " 'blocks.1.attn.v_lora_B.weight': 'transformer_2_linear_8_v_lora_B_weight',\n",
       " 'blocks.1.attn.o_lora_A.weight': 'transformer_2_linear_9_o_lora_A_weight',\n",
       " 'blocks.1.attn.o_lora_B.weight': 'transformer_2_linear_10_o_lora_B_weight',\n",
       " 'blocks.1.proj_up_lora_A.weight': 'transformer_2_linear_11_proj_up_lora_A_weight',\n",
       " 'blocks.1.proj_up_lora_B.weight': 'transformer_2_linear_12_proj_up_lora_B_weight',\n",
       " 'blocks.1.proj_down_lora_A.weight': 'transformer_2_linear_13_proj_down_lora_A_weight',\n",
       " 'blocks.1.proj_down_lora_B.weight': 'transformer_2_linear_14_proj_down_lora_B_weight',\n",
       " 'blocks.1.mlp.0.weight': 'transformer_2_linear_15_proj_up_weight',\n",
       " 'blocks.1.mlp.0.bias': 'transformer_2_linear_15_proj_up_bias',\n",
       " 'blocks.1.mlp.2.weight': 'transformer_2_linear_16_proj_down_weight',\n",
       " 'blocks.1.mlp.2.bias': 'transformer_2_linear_16_proj_down_bias',\n",
       " 'blocks.2.ln1.weight': 'transformer_3_layernorm_1_gamma',\n",
       " 'blocks.2.ln1.bias': 'transformer_3_layernorm_1_beta',\n",
       " 'blocks.2.ln2.weight': 'transformer_3_layernorm_2_gamma',\n",
       " 'blocks.2.ln2.bias': 'transformer_3_layernorm_2_beta',\n",
       " 'blocks.2.attn.qkv.weight': 'transformer_3_linear_1_qkv_weight',\n",
       " 'blocks.2.attn.qkv.bias': 'transformer_3_linear_1_qkv_bias',\n",
       " 'blocks.2.attn.out_proj.weight': 'transformer_3_linear_2_o_weight',\n",
       " 'blocks.2.attn.out_proj.bias': 'transformer_3_linear_2_o_bias',\n",
       " 'blocks.2.attn.q_lora_A.weight': 'transformer_3_linear_3_q_lora_A_weight',\n",
       " 'blocks.2.attn.k_lora_A.weight': 'transformer_3_linear_4_k_lora_A_weight',\n",
       " 'blocks.2.attn.v_lora_A.weight': 'transformer_3_linear_5_v_lora_A_weight',\n",
       " 'blocks.2.attn.q_lora_B.weight': 'transformer_3_linear_6_q_lora_B_weight',\n",
       " 'blocks.2.attn.k_lora_B.weight': 'transformer_3_linear_7_k_lora_B_weight',\n",
       " 'blocks.2.attn.v_lora_B.weight': 'transformer_3_linear_8_v_lora_B_weight',\n",
       " 'blocks.2.attn.o_lora_A.weight': 'transformer_3_linear_9_o_lora_A_weight',\n",
       " 'blocks.2.attn.o_lora_B.weight': 'transformer_3_linear_10_o_lora_B_weight',\n",
       " 'blocks.2.proj_up_lora_A.weight': 'transformer_3_linear_11_proj_up_lora_A_weight',\n",
       " 'blocks.2.proj_up_lora_B.weight': 'transformer_3_linear_12_proj_up_lora_B_weight',\n",
       " 'blocks.2.proj_down_lora_A.weight': 'transformer_3_linear_13_proj_down_lora_A_weight',\n",
       " 'blocks.2.proj_down_lora_B.weight': 'transformer_3_linear_14_proj_down_lora_B_weight',\n",
       " 'blocks.2.mlp.0.weight': 'transformer_3_linear_15_proj_up_weight',\n",
       " 'blocks.2.mlp.0.bias': 'transformer_3_linear_15_proj_up_bias',\n",
       " 'blocks.2.mlp.2.weight': 'transformer_3_linear_16_proj_down_weight',\n",
       " 'blocks.2.mlp.2.bias': 'transformer_3_linear_16_proj_down_bias',\n",
       " 'blocks.3.ln1.weight': 'transformer_4_layernorm_1_gamma',\n",
       " 'blocks.3.ln1.bias': 'transformer_4_layernorm_1_beta',\n",
       " 'blocks.3.ln2.weight': 'transformer_4_layernorm_2_gamma',\n",
       " 'blocks.3.ln2.bias': 'transformer_4_layernorm_2_beta',\n",
       " 'blocks.3.attn.qkv.weight': 'transformer_4_linear_1_qkv_weight',\n",
       " 'blocks.3.attn.qkv.bias': 'transformer_4_linear_1_qkv_bias',\n",
       " 'blocks.3.attn.out_proj.weight': 'transformer_4_linear_2_o_weight',\n",
       " 'blocks.3.attn.out_proj.bias': 'transformer_4_linear_2_o_bias',\n",
       " 'blocks.3.attn.q_lora_A.weight': 'transformer_4_linear_3_q_lora_A_weight',\n",
       " 'blocks.3.attn.k_lora_A.weight': 'transformer_4_linear_4_k_lora_A_weight',\n",
       " 'blocks.3.attn.v_lora_A.weight': 'transformer_4_linear_5_v_lora_A_weight',\n",
       " 'blocks.3.attn.q_lora_B.weight': 'transformer_4_linear_6_q_lora_B_weight',\n",
       " 'blocks.3.attn.k_lora_B.weight': 'transformer_4_linear_7_k_lora_B_weight',\n",
       " 'blocks.3.attn.v_lora_B.weight': 'transformer_4_linear_8_v_lora_B_weight',\n",
       " 'blocks.3.attn.o_lora_A.weight': 'transformer_4_linear_9_o_lora_A_weight',\n",
       " 'blocks.3.attn.o_lora_B.weight': 'transformer_4_linear_10_o_lora_B_weight',\n",
       " 'blocks.3.proj_up_lora_A.weight': 'transformer_4_linear_11_proj_up_lora_A_weight',\n",
       " 'blocks.3.proj_up_lora_B.weight': 'transformer_4_linear_12_proj_up_lora_B_weight',\n",
       " 'blocks.3.proj_down_lora_A.weight': 'transformer_4_linear_13_proj_down_lora_A_weight',\n",
       " 'blocks.3.proj_down_lora_B.weight': 'transformer_4_linear_14_proj_down_lora_B_weight',\n",
       " 'blocks.3.mlp.0.weight': 'transformer_4_linear_15_proj_up_weight',\n",
       " 'blocks.3.mlp.0.bias': 'transformer_4_linear_15_proj_up_bias',\n",
       " 'blocks.3.mlp.2.weight': 'transformer_4_linear_16_proj_down_weight',\n",
       " 'blocks.3.mlp.2.bias': 'transformer_4_linear_16_proj_down_bias',\n",
       " 'blocks.4.ln1.weight': 'transformer_5_layernorm_1_gamma',\n",
       " 'blocks.4.ln1.bias': 'transformer_5_layernorm_1_beta',\n",
       " 'blocks.4.ln2.weight': 'transformer_5_layernorm_2_gamma',\n",
       " 'blocks.4.ln2.bias': 'transformer_5_layernorm_2_beta',\n",
       " 'blocks.4.attn.qkv.weight': 'transformer_5_linear_1_qkv_weight',\n",
       " 'blocks.4.attn.qkv.bias': 'transformer_5_linear_1_qkv_bias',\n",
       " 'blocks.4.attn.out_proj.weight': 'transformer_5_linear_2_o_weight',\n",
       " 'blocks.4.attn.out_proj.bias': 'transformer_5_linear_2_o_bias',\n",
       " 'blocks.4.attn.q_lora_A.weight': 'transformer_5_linear_3_q_lora_A_weight',\n",
       " 'blocks.4.attn.k_lora_A.weight': 'transformer_5_linear_4_k_lora_A_weight',\n",
       " 'blocks.4.attn.v_lora_A.weight': 'transformer_5_linear_5_v_lora_A_weight',\n",
       " 'blocks.4.attn.q_lora_B.weight': 'transformer_5_linear_6_q_lora_B_weight',\n",
       " 'blocks.4.attn.k_lora_B.weight': 'transformer_5_linear_7_k_lora_B_weight',\n",
       " 'blocks.4.attn.v_lora_B.weight': 'transformer_5_linear_8_v_lora_B_weight',\n",
       " 'blocks.4.attn.o_lora_A.weight': 'transformer_5_linear_9_o_lora_A_weight',\n",
       " 'blocks.4.attn.o_lora_B.weight': 'transformer_5_linear_10_o_lora_B_weight',\n",
       " 'blocks.4.proj_up_lora_A.weight': 'transformer_5_linear_11_proj_up_lora_A_weight',\n",
       " 'blocks.4.proj_up_lora_B.weight': 'transformer_5_linear_12_proj_up_lora_B_weight',\n",
       " 'blocks.4.proj_down_lora_A.weight': 'transformer_5_linear_13_proj_down_lora_A_weight',\n",
       " 'blocks.4.proj_down_lora_B.weight': 'transformer_5_linear_14_proj_down_lora_B_weight',\n",
       " 'blocks.4.mlp.0.weight': 'transformer_5_linear_15_proj_up_weight',\n",
       " 'blocks.4.mlp.0.bias': 'transformer_5_linear_15_proj_up_bias',\n",
       " 'blocks.4.mlp.2.weight': 'transformer_5_linear_16_proj_down_weight',\n",
       " 'blocks.4.mlp.2.bias': 'transformer_5_linear_16_proj_down_bias',\n",
       " 'blocks.5.ln1.weight': 'transformer_6_layernorm_1_gamma',\n",
       " 'blocks.5.ln1.bias': 'transformer_6_layernorm_1_beta',\n",
       " 'blocks.5.ln2.weight': 'transformer_6_layernorm_2_gamma',\n",
       " 'blocks.5.ln2.bias': 'transformer_6_layernorm_2_beta',\n",
       " 'blocks.5.attn.qkv.weight': 'transformer_6_linear_1_qkv_weight',\n",
       " 'blocks.5.attn.qkv.bias': 'transformer_6_linear_1_qkv_bias',\n",
       " 'blocks.5.attn.out_proj.weight': 'transformer_6_linear_2_o_weight',\n",
       " 'blocks.5.attn.out_proj.bias': 'transformer_6_linear_2_o_bias',\n",
       " 'blocks.5.attn.q_lora_A.weight': 'transformer_6_linear_3_q_lora_A_weight',\n",
       " 'blocks.5.attn.k_lora_A.weight': 'transformer_6_linear_4_k_lora_A_weight',\n",
       " 'blocks.5.attn.v_lora_A.weight': 'transformer_6_linear_5_v_lora_A_weight',\n",
       " 'blocks.5.attn.q_lora_B.weight': 'transformer_6_linear_6_q_lora_B_weight',\n",
       " 'blocks.5.attn.k_lora_B.weight': 'transformer_6_linear_7_k_lora_B_weight',\n",
       " 'blocks.5.attn.v_lora_B.weight': 'transformer_6_linear_8_v_lora_B_weight',\n",
       " 'blocks.5.attn.o_lora_A.weight': 'transformer_6_linear_9_o_lora_A_weight',\n",
       " 'blocks.5.attn.o_lora_B.weight': 'transformer_6_linear_10_o_lora_B_weight',\n",
       " 'blocks.5.proj_up_lora_A.weight': 'transformer_6_linear_11_proj_up_lora_A_weight',\n",
       " 'blocks.5.proj_up_lora_B.weight': 'transformer_6_linear_12_proj_up_lora_B_weight',\n",
       " 'blocks.5.proj_down_lora_A.weight': 'transformer_6_linear_13_proj_down_lora_A_weight',\n",
       " 'blocks.5.proj_down_lora_B.weight': 'transformer_6_linear_14_proj_down_lora_B_weight',\n",
       " 'blocks.5.mlp.0.weight': 'transformer_6_linear_15_proj_up_weight',\n",
       " 'blocks.5.mlp.0.bias': 'transformer_6_linear_15_proj_up_bias',\n",
       " 'blocks.5.mlp.2.weight': 'transformer_6_linear_16_proj_down_weight',\n",
       " 'blocks.5.mlp.2.bias': 'transformer_6_linear_16_proj_down_bias',\n",
       " 'blocks.6.ln1.weight': 'transformer_7_layernorm_1_gamma',\n",
       " 'blocks.6.ln1.bias': 'transformer_7_layernorm_1_beta',\n",
       " 'blocks.6.ln2.weight': 'transformer_7_layernorm_2_gamma',\n",
       " 'blocks.6.ln2.bias': 'transformer_7_layernorm_2_beta',\n",
       " 'blocks.6.attn.qkv.weight': 'transformer_7_linear_1_qkv_weight',\n",
       " 'blocks.6.attn.qkv.bias': 'transformer_7_linear_1_qkv_bias',\n",
       " 'blocks.6.attn.out_proj.weight': 'transformer_7_linear_2_o_weight',\n",
       " 'blocks.6.attn.out_proj.bias': 'transformer_7_linear_2_o_bias',\n",
       " 'blocks.6.attn.q_lora_A.weight': 'transformer_7_linear_3_q_lora_A_weight',\n",
       " 'blocks.6.attn.k_lora_A.weight': 'transformer_7_linear_4_k_lora_A_weight',\n",
       " 'blocks.6.attn.v_lora_A.weight': 'transformer_7_linear_5_v_lora_A_weight',\n",
       " 'blocks.6.attn.q_lora_B.weight': 'transformer_7_linear_6_q_lora_B_weight',\n",
       " 'blocks.6.attn.k_lora_B.weight': 'transformer_7_linear_7_k_lora_B_weight',\n",
       " 'blocks.6.attn.v_lora_B.weight': 'transformer_7_linear_8_v_lora_B_weight',\n",
       " 'blocks.6.attn.o_lora_A.weight': 'transformer_7_linear_9_o_lora_A_weight',\n",
       " 'blocks.6.attn.o_lora_B.weight': 'transformer_7_linear_10_o_lora_B_weight',\n",
       " 'blocks.6.proj_up_lora_A.weight': 'transformer_7_linear_11_proj_up_lora_A_weight',\n",
       " 'blocks.6.proj_up_lora_B.weight': 'transformer_7_linear_12_proj_up_lora_B_weight',\n",
       " 'blocks.6.proj_down_lora_A.weight': 'transformer_7_linear_13_proj_down_lora_A_weight',\n",
       " 'blocks.6.proj_down_lora_B.weight': 'transformer_7_linear_14_proj_down_lora_B_weight',\n",
       " 'blocks.6.mlp.0.weight': 'transformer_7_linear_15_proj_up_weight',\n",
       " 'blocks.6.mlp.0.bias': 'transformer_7_linear_15_proj_up_bias',\n",
       " 'blocks.6.mlp.2.weight': 'transformer_7_linear_16_proj_down_weight',\n",
       " 'blocks.6.mlp.2.bias': 'transformer_7_linear_16_proj_down_bias',\n",
       " 'blocks.7.ln1.weight': 'transformer_8_layernorm_1_gamma',\n",
       " 'blocks.7.ln1.bias': 'transformer_8_layernorm_1_beta',\n",
       " 'blocks.7.ln2.weight': 'transformer_8_layernorm_2_gamma',\n",
       " 'blocks.7.ln2.bias': 'transformer_8_layernorm_2_beta',\n",
       " 'blocks.7.attn.qkv.weight': 'transformer_8_linear_1_qkv_weight',\n",
       " 'blocks.7.attn.qkv.bias': 'transformer_8_linear_1_qkv_bias',\n",
       " 'blocks.7.attn.out_proj.weight': 'transformer_8_linear_2_o_weight',\n",
       " 'blocks.7.attn.out_proj.bias': 'transformer_8_linear_2_o_bias',\n",
       " 'blocks.7.attn.q_lora_A.weight': 'transformer_8_linear_3_q_lora_A_weight',\n",
       " 'blocks.7.attn.k_lora_A.weight': 'transformer_8_linear_4_k_lora_A_weight',\n",
       " 'blocks.7.attn.v_lora_A.weight': 'transformer_8_linear_5_v_lora_A_weight',\n",
       " 'blocks.7.attn.q_lora_B.weight': 'transformer_8_linear_6_q_lora_B_weight',\n",
       " 'blocks.7.attn.k_lora_B.weight': 'transformer_8_linear_7_k_lora_B_weight',\n",
       " 'blocks.7.attn.v_lora_B.weight': 'transformer_8_linear_8_v_lora_B_weight',\n",
       " 'blocks.7.attn.o_lora_A.weight': 'transformer_8_linear_9_o_lora_A_weight',\n",
       " 'blocks.7.attn.o_lora_B.weight': 'transformer_8_linear_10_o_lora_B_weight',\n",
       " 'blocks.7.proj_up_lora_A.weight': 'transformer_8_linear_11_proj_up_lora_A_weight',\n",
       " 'blocks.7.proj_up_lora_B.weight': 'transformer_8_linear_12_proj_up_lora_B_weight',\n",
       " 'blocks.7.proj_down_lora_A.weight': 'transformer_8_linear_13_proj_down_lora_A_weight',\n",
       " 'blocks.7.proj_down_lora_B.weight': 'transformer_8_linear_14_proj_down_lora_B_weight',\n",
       " 'blocks.7.mlp.0.weight': 'transformer_8_linear_15_proj_up_weight',\n",
       " 'blocks.7.mlp.0.bias': 'transformer_8_linear_15_proj_up_bias',\n",
       " 'blocks.7.mlp.2.weight': 'transformer_8_linear_16_proj_down_weight',\n",
       " 'blocks.7.mlp.2.bias': 'transformer_8_linear_16_proj_down_bias',\n",
       " 'blocks.8.ln1.weight': 'transformer_9_layernorm_1_gamma',\n",
       " 'blocks.8.ln1.bias': 'transformer_9_layernorm_1_beta',\n",
       " 'blocks.8.ln2.weight': 'transformer_9_layernorm_2_gamma',\n",
       " 'blocks.8.ln2.bias': 'transformer_9_layernorm_2_beta',\n",
       " 'blocks.8.attn.qkv.weight': 'transformer_9_linear_1_qkv_weight',\n",
       " 'blocks.8.attn.qkv.bias': 'transformer_9_linear_1_qkv_bias',\n",
       " 'blocks.8.attn.out_proj.weight': 'transformer_9_linear_2_o_weight',\n",
       " 'blocks.8.attn.out_proj.bias': 'transformer_9_linear_2_o_bias',\n",
       " 'blocks.8.attn.q_lora_A.weight': 'transformer_9_linear_3_q_lora_A_weight',\n",
       " 'blocks.8.attn.k_lora_A.weight': 'transformer_9_linear_4_k_lora_A_weight',\n",
       " 'blocks.8.attn.v_lora_A.weight': 'transformer_9_linear_5_v_lora_A_weight',\n",
       " 'blocks.8.attn.q_lora_B.weight': 'transformer_9_linear_6_q_lora_B_weight',\n",
       " 'blocks.8.attn.k_lora_B.weight': 'transformer_9_linear_7_k_lora_B_weight',\n",
       " 'blocks.8.attn.v_lora_B.weight': 'transformer_9_linear_8_v_lora_B_weight',\n",
       " 'blocks.8.attn.o_lora_A.weight': 'transformer_9_linear_9_o_lora_A_weight',\n",
       " 'blocks.8.attn.o_lora_B.weight': 'transformer_9_linear_10_o_lora_B_weight',\n",
       " 'blocks.8.proj_up_lora_A.weight': 'transformer_9_linear_11_proj_up_lora_A_weight',\n",
       " 'blocks.8.proj_up_lora_B.weight': 'transformer_9_linear_12_proj_up_lora_B_weight',\n",
       " 'blocks.8.proj_down_lora_A.weight': 'transformer_9_linear_13_proj_down_lora_A_weight',\n",
       " 'blocks.8.proj_down_lora_B.weight': 'transformer_9_linear_14_proj_down_lora_B_weight',\n",
       " 'blocks.8.mlp.0.weight': 'transformer_9_linear_15_proj_up_weight',\n",
       " 'blocks.8.mlp.0.bias': 'transformer_9_linear_15_proj_up_bias',\n",
       " 'blocks.8.mlp.2.weight': 'transformer_9_linear_16_proj_down_weight',\n",
       " 'blocks.8.mlp.2.bias': 'transformer_9_linear_16_proj_down_bias',\n",
       " 'blocks.9.ln1.weight': 'transformer_10_layernorm_1_gamma',\n",
       " 'blocks.9.ln1.bias': 'transformer_10_layernorm_1_beta',\n",
       " 'blocks.9.ln2.weight': 'transformer_10_layernorm_2_gamma',\n",
       " 'blocks.9.ln2.bias': 'transformer_10_layernorm_2_beta',\n",
       " 'blocks.9.attn.qkv.weight': 'transformer_10_linear_1_qkv_weight',\n",
       " 'blocks.9.attn.qkv.bias': 'transformer_10_linear_1_qkv_bias',\n",
       " 'blocks.9.attn.out_proj.weight': 'transformer_10_linear_2_o_weight',\n",
       " 'blocks.9.attn.out_proj.bias': 'transformer_10_linear_2_o_bias',\n",
       " 'blocks.9.attn.q_lora_A.weight': 'transformer_10_linear_3_q_lora_A_weight',\n",
       " 'blocks.9.attn.k_lora_A.weight': 'transformer_10_linear_4_k_lora_A_weight',\n",
       " 'blocks.9.attn.v_lora_A.weight': 'transformer_10_linear_5_v_lora_A_weight',\n",
       " 'blocks.9.attn.q_lora_B.weight': 'transformer_10_linear_6_q_lora_B_weight',\n",
       " 'blocks.9.attn.k_lora_B.weight': 'transformer_10_linear_7_k_lora_B_weight',\n",
       " 'blocks.9.attn.v_lora_B.weight': 'transformer_10_linear_8_v_lora_B_weight',\n",
       " 'blocks.9.attn.o_lora_A.weight': 'transformer_10_linear_9_o_lora_A_weight',\n",
       " 'blocks.9.attn.o_lora_B.weight': 'transformer_10_linear_10_o_lora_B_weight',\n",
       " 'blocks.9.proj_up_lora_A.weight': 'transformer_10_linear_11_proj_up_lora_A_weight',\n",
       " 'blocks.9.proj_up_lora_B.weight': 'transformer_10_linear_12_proj_up_lora_B_weight',\n",
       " 'blocks.9.proj_down_lora_A.weight': 'transformer_10_linear_13_proj_down_lora_A_weight',\n",
       " 'blocks.9.proj_down_lora_B.weight': 'transformer_10_linear_14_proj_down_lora_B_weight',\n",
       " 'blocks.9.mlp.0.weight': 'transformer_10_linear_15_proj_up_weight',\n",
       " 'blocks.9.mlp.0.bias': 'transformer_10_linear_15_proj_up_bias',\n",
       " 'blocks.9.mlp.2.weight': 'transformer_10_linear_16_proj_down_weight',\n",
       " 'blocks.9.mlp.2.bias': 'transformer_10_linear_16_proj_down_bias',\n",
       " 'blocks.10.ln1.weight': 'transformer_11_layernorm_1_gamma',\n",
       " 'blocks.10.ln1.bias': 'transformer_11_layernorm_1_beta',\n",
       " 'blocks.10.ln2.weight': 'transformer_11_layernorm_2_gamma',\n",
       " 'blocks.10.ln2.bias': 'transformer_11_layernorm_2_beta',\n",
       " 'blocks.10.attn.qkv.weight': 'transformer_11_linear_1_qkv_weight',\n",
       " 'blocks.10.attn.qkv.bias': 'transformer_11_linear_1_qkv_bias',\n",
       " 'blocks.10.attn.out_proj.weight': 'transformer_11_linear_2_o_weight',\n",
       " 'blocks.10.attn.out_proj.bias': 'transformer_11_linear_2_o_bias',\n",
       " 'blocks.10.attn.q_lora_A.weight': 'transformer_11_linear_3_q_lora_A_weight',\n",
       " 'blocks.10.attn.k_lora_A.weight': 'transformer_11_linear_4_k_lora_A_weight',\n",
       " 'blocks.10.attn.v_lora_A.weight': 'transformer_11_linear_5_v_lora_A_weight',\n",
       " 'blocks.10.attn.q_lora_B.weight': 'transformer_11_linear_6_q_lora_B_weight',\n",
       " 'blocks.10.attn.k_lora_B.weight': 'transformer_11_linear_7_k_lora_B_weight',\n",
       " 'blocks.10.attn.v_lora_B.weight': 'transformer_11_linear_8_v_lora_B_weight',\n",
       " 'blocks.10.attn.o_lora_A.weight': 'transformer_11_linear_9_o_lora_A_weight',\n",
       " 'blocks.10.attn.o_lora_B.weight': 'transformer_11_linear_10_o_lora_B_weight',\n",
       " 'blocks.10.proj_up_lora_A.weight': 'transformer_11_linear_11_proj_up_lora_A_weight',\n",
       " 'blocks.10.proj_up_lora_B.weight': 'transformer_11_linear_12_proj_up_lora_B_weight',\n",
       " 'blocks.10.proj_down_lora_A.weight': 'transformer_11_linear_13_proj_down_lora_A_weight',\n",
       " 'blocks.10.proj_down_lora_B.weight': 'transformer_11_linear_14_proj_down_lora_B_weight',\n",
       " 'blocks.10.mlp.0.weight': 'transformer_11_linear_15_proj_up_weight',\n",
       " 'blocks.10.mlp.0.bias': 'transformer_11_linear_15_proj_up_bias',\n",
       " 'blocks.10.mlp.2.weight': 'transformer_11_linear_16_proj_down_weight',\n",
       " 'blocks.10.mlp.2.bias': 'transformer_11_linear_16_proj_down_bias',\n",
       " 'blocks.11.ln1.weight': 'transformer_12_layernorm_1_gamma',\n",
       " 'blocks.11.ln1.bias': 'transformer_12_layernorm_1_beta',\n",
       " 'blocks.11.ln2.weight': 'transformer_12_layernorm_2_gamma',\n",
       " 'blocks.11.ln2.bias': 'transformer_12_layernorm_2_beta',\n",
       " 'blocks.11.attn.qkv.weight': 'transformer_12_linear_1_qkv_weight',\n",
       " 'blocks.11.attn.qkv.bias': 'transformer_12_linear_1_qkv_bias',\n",
       " 'blocks.11.attn.out_proj.weight': 'transformer_12_linear_2_o_weight',\n",
       " 'blocks.11.attn.out_proj.bias': 'transformer_12_linear_2_o_bias',\n",
       " 'blocks.11.attn.q_lora_A.weight': 'transformer_12_linear_3_q_lora_A_weight',\n",
       " 'blocks.11.attn.k_lora_A.weight': 'transformer_12_linear_4_k_lora_A_weight',\n",
       " 'blocks.11.attn.v_lora_A.weight': 'transformer_12_linear_5_v_lora_A_weight',\n",
       " 'blocks.11.attn.q_lora_B.weight': 'transformer_12_linear_6_q_lora_B_weight',\n",
       " 'blocks.11.attn.k_lora_B.weight': 'transformer_12_linear_7_k_lora_B_weight',\n",
       " 'blocks.11.attn.v_lora_B.weight': 'transformer_12_linear_8_v_lora_B_weight',\n",
       " 'blocks.11.attn.o_lora_A.weight': 'transformer_12_linear_9_o_lora_A_weight',\n",
       " 'blocks.11.attn.o_lora_B.weight': 'transformer_12_linear_10_o_lora_B_weight',\n",
       " 'blocks.11.proj_up_lora_A.weight': 'transformer_12_linear_11_proj_up_lora_A_weight',\n",
       " 'blocks.11.proj_up_lora_B.weight': 'transformer_12_linear_12_proj_up_lora_B_weight',\n",
       " 'blocks.11.proj_down_lora_A.weight': 'transformer_12_linear_13_proj_down_lora_A_weight',\n",
       " 'blocks.11.proj_down_lora_B.weight': 'transformer_12_linear_14_proj_down_lora_B_weight',\n",
       " 'blocks.11.mlp.0.weight': 'transformer_12_linear_15_proj_up_weight',\n",
       " 'blocks.11.mlp.0.bias': 'transformer_12_linear_15_proj_up_bias',\n",
       " 'blocks.11.mlp.2.weight': 'transformer_12_linear_16_proj_down_weight',\n",
       " 'blocks.11.mlp.2.bias': 'transformer_12_linear_16_proj_down_bias'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pt_to_my_lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528900e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f887051c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m t_params = \u001b[43mtorch_model\u001b[49m.state_dict()\n",
      "\u001b[31mNameError\u001b[39m: name 'torch_model' is not defined"
     ]
    }
   ],
   "source": [
    "t_params = torch_model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fb2a4dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "dlx_params = dlx_model.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5070ed41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0_1_embedding_1_embed': Tensor(data=[[ 0.01789019  0.01250056  0.02061453 ... -0.02228284  0.01846989\n",
       "   -0.0053018 ]\n",
       "  [-0.01292583 -0.07405201 -0.06376181 ...  0.00762339  0.02328694\n",
       "   -0.01242082]\n",
       "  [ 0.01697432 -0.00109404  0.00110288 ... -0.01558423 -0.00830132\n",
       "    0.01966907]\n",
       "  ...\n",
       "  [ 0.03131422 -0.01982697 -0.07430051 ... -0.01443854 -0.01563413\n",
       "    0.01851392]\n",
       "  [-0.03506761  0.01713121 -0.00233865 ... -0.02049896 -0.01373542\n",
       "   -0.0165358 ]\n",
       "  [-0.03744936  0.01774619  0.06139588 ...  0.00537806 -0.00201732\n",
       "    0.00901594]], shape=(51682, 1024), dtype=float32),\n",
       " '0_1_embedding_1_pe': Tensor(data=[[ 0.01765298  0.1509603  -0.02300312 ... -0.02270602  0.02222843\n",
       "    0.01217857]\n",
       "  [ 0.05000131  0.03053647 -0.03375666 ... -0.03249162  0.0179548\n",
       "    0.00756455]\n",
       "  [ 0.05904716 -0.0301312  -0.03745729 ... -0.01414995  0.01011304\n",
       "   -0.04104885]\n",
       "  ...\n",
       "  [-0.02065871 -0.07007099  0.0541271  ...  0.02158256 -0.01228334\n",
       "    0.01542529]\n",
       "  [ 0.00090509 -0.04632602  0.04650491 ...  0.02214322 -0.02901969\n",
       "   -0.0014961 ]\n",
       "  [-0.01636651 -0.01464839  0.03936961 ... -0.01202546 -0.00017766\n",
       "    0.040844  ]], shape=(512, 1024), dtype=float32),\n",
       " 'transformer_1_linear_1_qkv_weight': Tensor(data=[[ 0.08080873 -0.00072871 -0.00669279 ... -0.02632872  0.07526547\n",
       "    0.02211172]\n",
       "  [-0.03750416  0.00247538 -0.00832325 ...  0.01986558 -0.00361058\n",
       "    0.01005378]\n",
       "  [-0.02502557 -0.04170775 -0.01532007 ...  0.01546556 -0.04641759\n",
       "   -0.06068198]\n",
       "  ...\n",
       "  [-0.02130116  0.02649267  0.04139977 ... -0.07124548 -0.10196821\n",
       "    0.01899082]\n",
       "  [ 0.00488295 -0.01395783  0.0152039  ... -0.00843465 -0.08136126\n",
       "    0.01877243]\n",
       "  [-0.00406006  0.0048068  -0.0336455  ...  0.04810989  0.03996198\n",
       "    0.00094859]], shape=(1024, 3072), dtype=float32),\n",
       " 'transformer_1_linear_1_qkv_bias': Tensor(data=[-0.0051051   0.00224676  0.01275476 ... -0.03639524 -0.00470018\n",
       "  -0.11364553], shape=(3072,), dtype=float32),\n",
       " 'transformer_1_linear_2_o_weight': Tensor(data=[[ 0.04067531  0.0076703   0.0167912  ...  0.06223715 -0.02727475\n",
       "   -0.00674467]\n",
       "  [-0.13652316  0.00336078  0.0156313  ...  0.02997684 -0.00191657\n",
       "    0.04540452]\n",
       "  [-0.02226829  0.02151927  0.01273231 ...  0.05801637 -0.03436866\n",
       "    0.06193439]\n",
       "  ...\n",
       "  [ 0.02125752  0.05078047 -0.0131924  ... -0.05562861  0.01529784\n",
       "    0.00486406]\n",
       "  [ 0.07058229  0.04663316 -0.0110021  ...  0.01521587 -0.03108057\n",
       "    0.05834106]\n",
       "  [ 0.0335981   0.02324358  0.04042039 ...  0.06864328  0.01399211\n",
       "    0.05435937]], shape=(1024, 1024), dtype=float32),\n",
       " 'transformer_1_linear_2_o_bias': Tensor(data=[-0.023203    0.03431173  0.13939783 ...  0.00411586 -0.00828341\n",
       "   0.13819773], shape=(1024,), dtype=float32),\n",
       " 'transformer_1_linear_3_proj_up_weight': Tensor(data=[[-0.11977176  0.0387928  -0.0581344  ... -0.08674138 -0.04808436\n",
       "    0.05296699]\n",
       "  [ 0.02316524 -0.09515692 -0.0430326  ...  0.00932549 -0.0335918\n",
       "   -0.01418061]\n",
       "  [ 0.02582037  0.02126375 -0.0315362  ... -0.11987654 -0.02031377\n",
       "   -0.04976846]\n",
       "  ...\n",
       "  [-0.01107124  0.03243599 -0.00606091 ...  0.02677008 -0.0089361\n",
       "    0.01987105]\n",
       "  [-0.04246588  0.05874783  0.0027905  ...  0.02209079 -0.00933637\n",
       "    0.00961117]\n",
       "  [ 0.01558012 -0.03042872  0.01853997 ... -0.02256186  0.0541038\n",
       "   -0.0938717 ]], shape=(1024, 4096), dtype=float32),\n",
       " 'transformer_1_linear_3_proj_up_bias': Tensor(data=[-0.06602088 -0.06538022 -0.06306728 ... -0.04786595 -0.00863727\n",
       "  -0.0672894 ], shape=(4096,), dtype=float32),\n",
       " 'transformer_1_linear_4_proj_down_weight': Tensor(data=[[ 0.05122843 -0.01400381  0.01550582 ... -0.05832512  0.0917965\n",
       "   -0.01312335]\n",
       "  [-0.03216363 -0.08231869 -0.02034813 ... -0.05775009 -0.04693266\n",
       "   -0.01724197]\n",
       "  [-0.00270003 -0.01874072  0.00169076 ...  0.03223089  0.00324242\n",
       "    0.00186275]\n",
       "  ...\n",
       "  [ 0.03217909  0.06725369 -0.0483216  ... -0.02748546 -0.09194272\n",
       "    0.00746533]\n",
       "  [-0.01478459 -0.0179476  -0.0554219  ...  0.00239713  0.03265478\n",
       "    0.01847817]\n",
       "  [ 0.02051356 -0.02115568  0.0754417  ...  0.00989274  0.01962872\n",
       "    0.03568461]], shape=(4096, 1024), dtype=float32),\n",
       " 'transformer_1_linear_4_proj_down_bias': Tensor(data=[-5.48212665e-05  2.77116289e-03  1.03829615e-02 ...  8.18580110e-03\n",
       "  -4.28667408e-04 -8.46972049e-04], shape=(1024,), dtype=float32),\n",
       " 'transformer_1_layernorm_1_gamma': Tensor(data=[0.46643955 0.34634942 0.5085433  ... 0.53313285 0.5480227  0.566662  ], shape=(1024,), dtype=float32),\n",
       " 'transformer_1_layernorm_1_beta': Tensor(data=[-0.00075018 -0.00016589  0.06328239 ... -0.03617238  0.0537997\n",
       "  -0.01098302], shape=(1024,), dtype=float32),\n",
       " 'transformer_1_layernorm_2_gamma': Tensor(data=[0.91889316 0.8047638  1.0429381  ... 0.8937101  0.8945863  1.0118186 ], shape=(1024,), dtype=float32),\n",
       " 'transformer_1_layernorm_2_beta': Tensor(data=[-0.02372772  0.0053164   0.16820873 ...  0.05191046 -0.01618209\n",
       "   0.14723088], shape=(1024,), dtype=float32),\n",
       " 'transformer_2_linear_1_qkv_weight': Tensor(data=[[ 0.03568123  0.01642654  0.00125804 ... -0.08036052 -0.02507285\n",
       "   -0.00416691]\n",
       "  [ 0.01248429  0.04801658  0.03042922 ...  0.00684901  0.02927737\n",
       "    0.02015148]\n",
       "  [ 0.01382029 -0.05443954 -0.01944327 ... -0.01142064 -0.04107675\n",
       "   -0.02886235]\n",
       "  ...\n",
       "  [-0.00171498  0.03524138 -0.02897283 ... -0.05885615  0.00054274\n",
       "    0.00621324]\n",
       "  [-0.02647119 -0.00882811 -0.02393839 ...  0.02077367  0.08698675\n",
       "    0.06141461]\n",
       "  [ 0.00032909  0.00178104  0.00688497 ... -0.01282296  0.01284566\n",
       "   -0.04099721]], shape=(1024, 3072), dtype=float32),\n",
       " 'transformer_2_linear_1_qkv_bias': Tensor(data=[-0.01392747 -0.02392065  0.02708887 ... -0.02787595 -0.0277429\n",
       "   0.00278293], shape=(3072,), dtype=float32),\n",
       " 'transformer_2_linear_2_o_weight': Tensor(data=[[-0.03037019  0.04320236 -0.02866899 ... -0.02037778 -0.0043332\n",
       "   -0.02089023]\n",
       "  [ 0.03352254  0.0201306   0.02614534 ... -0.02181697  0.02706525\n",
       "   -0.01675528]\n",
       "  [ 0.02188621  0.00595802  0.01745381 ... -0.01244575 -0.0276948\n",
       "   -0.05948017]\n",
       "  ...\n",
       "  [ 0.01509169 -0.00412952  0.04559226 ...  0.03737497 -0.03222251\n",
       "    0.02870679]\n",
       "  [-0.04618365 -0.03832112  0.00499006 ...  0.00613632 -0.02715245\n",
       "    0.0458648 ]\n",
       "  [ 0.04935471  0.01922871  0.01028869 ...  0.04968962 -0.02489179\n",
       "    0.048774  ]], shape=(1024, 1024), dtype=float32),\n",
       " 'transformer_2_linear_2_o_bias': Tensor(data=[-0.02604024 -0.02144104 -0.0016672  ... -0.00686249  0.00883578\n",
       "   0.01932043], shape=(1024,), dtype=float32),\n",
       " 'transformer_2_linear_3_proj_up_weight': Tensor(data=[[ 0.07472596 -0.02978798  0.0271643  ... -0.13249335  0.02722071\n",
       "   -0.07249792]\n",
       "  [ 0.00827404  0.06237078 -0.05133452 ... -0.04088858  0.05116425\n",
       "   -0.00213398]\n",
       "  [ 0.08325605 -0.02430722 -0.10255469 ...  0.01484204  0.02727358\n",
       "   -0.05426082]\n",
       "  ...\n",
       "  [-0.01728299 -0.00702348 -0.05564281 ...  0.05822176 -0.0270465\n",
       "    0.02158697]\n",
       "  [-0.07014988 -0.01755767 -0.0649846  ...  0.01733884  0.04285537\n",
       "   -0.04437889]\n",
       "  [-0.17012468 -0.05128974 -0.00533657 ...  0.02692854 -0.00409461\n",
       "    0.02168441]], shape=(1024, 4096), dtype=float32),\n",
       " 'transformer_2_linear_3_proj_up_bias': Tensor(data=[-0.03245128 -0.04109535 -0.02621646 ... -0.02440364 -0.01948303\n",
       "   0.01197941], shape=(4096,), dtype=float32),\n",
       " 'transformer_2_linear_4_proj_down_weight': Tensor(data=[[-0.04177669  0.01408239 -0.11380895 ... -0.03485847 -0.01919158\n",
       "    0.00830913]\n",
       "  [ 0.03753539 -0.02372166 -0.01999203 ... -0.01355532 -0.01205717\n",
       "   -0.03740107]\n",
       "  [ 0.04966727  0.10691928  0.00587263 ...  0.02682291 -0.06315269\n",
       "    0.07461361]\n",
       "  ...\n",
       "  [ 0.06904712  0.00062795  0.04481297 ... -0.0428426  -0.01266895\n",
       "    0.04130583]\n",
       "  [ 0.02145003 -0.02178963  0.01447664 ... -0.01067958  0.02280513\n",
       "    0.09285768]\n",
       "  [-0.07527015  0.03464802 -0.12963721 ...  0.00493565  0.06929956\n",
       "   -0.04326449]], shape=(4096, 1024), dtype=float32),\n",
       " 'transformer_2_linear_4_proj_down_bias': Tensor(data=[ 0.03486289 -0.01363766  0.0432366  ... -0.0086546   0.04539634\n",
       "   0.02956449], shape=(1024,), dtype=float32),\n",
       " 'transformer_2_layernorm_1_gamma': Tensor(data=[0.72580487 0.5613275  0.6000561  ... 0.62240154 0.57488775 0.63269055], shape=(1024,), dtype=float32),\n",
       " 'transformer_2_layernorm_1_beta': Tensor(data=[-0.03078512  0.0858308  -0.12552696 ... -0.04416483 -0.06676569\n",
       "  -0.04632083], shape=(1024,), dtype=float32),\n",
       " 'transformer_2_layernorm_2_gamma': Tensor(data=[0.93942046 1.0322198  1.0648384  ... 0.96799135 1.0637703  0.9799713 ], shape=(1024,), dtype=float32),\n",
       " 'transformer_2_layernorm_2_beta': Tensor(data=[-0.00954545 -0.08120032  0.04547548 ...  0.03276807  0.01294429\n",
       "   0.03988443], shape=(1024,), dtype=float32),\n",
       " 'transformer_3_linear_1_qkv_weight': Tensor(data=[[ 0.00752893 -0.07428405  0.08280898 ...  0.03397071 -0.05451166\n",
       "    0.01354362]\n",
       "  [-0.0575742  -0.03527263 -0.17396627 ... -0.01081565 -0.01487744\n",
       "    0.01970057]\n",
       "  [-0.04556214 -0.02355301  0.0302996  ...  0.00560425 -0.0645933\n",
       "    0.03068834]\n",
       "  ...\n",
       "  [-0.03970056  0.02508057 -0.04408915 ... -0.026116    0.00549808\n",
       "    0.03285711]\n",
       "  [-0.02452713 -0.05875803  0.03680402 ... -0.06880178  0.00355788\n",
       "   -0.0634277 ]\n",
       "  [-0.08921155 -0.01379902 -0.01133296 ... -0.02724513 -0.02158006\n",
       "   -0.03121799]], shape=(1024, 3072), dtype=float32),\n",
       " 'transformer_3_linear_1_qkv_bias': Tensor(data=[ 0.00404321 -0.16289116  0.03604937 ...  0.01249564  0.00129645\n",
       "   0.00633788], shape=(3072,), dtype=float32),\n",
       " 'transformer_3_linear_2_o_weight': Tensor(data=[[ 0.01430262 -0.01888602  0.05177491 ... -0.02908245  0.04178458\n",
       "   -0.00980359]\n",
       "  [ 0.01660055  0.04906814  0.003044   ...  0.04248355  0.01977151\n",
       "   -0.04490167]\n",
       "  [-0.00637275 -0.02153291 -0.02005785 ... -0.00658848 -0.02262994\n",
       "   -0.01164318]\n",
       "  ...\n",
       "  [ 0.02026196  0.0227733   0.00545761 ...  0.06553604 -0.02657453\n",
       "    0.0286037 ]\n",
       "  [ 0.09707622 -0.02539816  0.00576619 ... -0.05301268  0.0058254\n",
       "    0.00888825]\n",
       "  [ 0.04071966  0.04044346 -0.05101746 ...  0.04871904  0.01868271\n",
       "   -0.06723102]], shape=(1024, 1024), dtype=float32),\n",
       " 'transformer_3_linear_2_o_bias': Tensor(data=[ 0.03778297 -0.06678016  0.07264503 ... -0.00118111  0.06932449\n",
       "   0.06681676], shape=(1024,), dtype=float32),\n",
       " 'transformer_3_linear_3_proj_up_weight': Tensor(data=[[-0.01881712 -0.02444679 -0.0085632  ... -0.03442194  0.00938467\n",
       "   -0.00216298]\n",
       "  [ 0.04044592 -0.02731604  0.02709171 ...  0.04141279  0.01217927\n",
       "   -0.02858334]\n",
       "  [ 0.0310949  -0.00651457  0.0012276  ... -0.03344436  0.01025663\n",
       "    0.03586407]\n",
       "  ...\n",
       "  [-0.00918548  0.00564144  0.07449722 ... -0.01608959  0.01328515\n",
       "    0.07152381]\n",
       "  [-0.06113591 -0.02056569 -0.00010865 ... -0.01674733 -0.01488746\n",
       "   -0.05171769]\n",
       "  [-0.08841806  0.01660991  0.00795566 ...  0.00673854 -0.10795945\n",
       "    0.02363198]], shape=(1024, 4096), dtype=float32),\n",
       " 'transformer_3_linear_3_proj_up_bias': Tensor(data=[-0.03223651 -0.02625916 -0.02943092 ... -0.01033488 -0.00894442\n",
       "  -0.01892656], shape=(4096,), dtype=float32),\n",
       " 'transformer_3_linear_4_proj_down_weight': Tensor(data=[[-0.01725648 -0.02188142 -0.01596622 ...  0.04366497  0.00412354\n",
       "    0.05939599]\n",
       "  [-0.02447377  0.04203254  0.06578718 ... -0.00993142  0.00103165\n",
       "   -0.01226731]\n",
       "  [-0.01846862  0.00643737 -0.0061301  ... -0.0960094   0.03473965\n",
       "   -0.00581944]\n",
       "  ...\n",
       "  [ 0.05374036 -0.01381829 -0.05357757 ...  0.02934864  0.02576993\n",
       "    0.02426583]\n",
       "  [ 0.0891817  -0.08145804  0.01203945 ...  0.04816192 -0.07119\n",
       "   -0.04848651]\n",
       "  [-0.03219917 -0.00741656  0.00101825 ...  0.07860612 -0.02079735\n",
       "    0.01445475]], shape=(4096, 1024), dtype=float32),\n",
       " 'transformer_3_linear_4_proj_down_bias': Tensor(data=[ 0.00685284 -0.00304682 -0.0197613  ...  0.00217356 -0.00292899\n",
       "   0.00232885], shape=(1024,), dtype=float32),\n",
       " 'transformer_3_layernorm_1_gamma': Tensor(data=[0.50400287 0.5046209  0.4865559  ... 0.43008474 0.38661093 0.46510386], shape=(1024,), dtype=float32),\n",
       " 'transformer_3_layernorm_1_beta': Tensor(data=[ 0.00568392  0.07410698 -0.06847067 ... -0.01161651 -0.11855201\n",
       "  -0.05624976], shape=(1024,), dtype=float32),\n",
       " 'transformer_3_layernorm_2_gamma': Tensor(data=[1.0131204 1.0368388 1.0817807 ... 0.8909388 1.2349308 1.0822946], shape=(1024,), dtype=float32),\n",
       " 'transformer_3_layernorm_2_beta': Tensor(data=[ 0.07316583 -0.12994681  0.11470944 ...  0.00381354  0.12833667\n",
       "   0.11663297], shape=(1024,), dtype=float32),\n",
       " 'transformer_4_linear_1_qkv_weight': Tensor(data=[[ 0.02609648  0.04609617  0.0184406  ... -0.03641482  0.04142628\n",
       "   -0.03583736]\n",
       "  [-0.06675286 -0.0060661   0.13726303 ...  0.06431153 -0.0054094\n",
       "    0.0734484 ]\n",
       "  [-0.10329327  0.02747291  0.10623358 ... -0.08635464  0.05224148\n",
       "    0.01053072]\n",
       "  ...\n",
       "  [ 0.02451108  0.03640619 -0.00810144 ... -0.07408038 -0.01191819\n",
       "   -0.06888843]\n",
       "  [ 0.07018275 -0.02916713  0.10128449 ... -0.00412286 -0.00692926\n",
       "    0.02385379]\n",
       "  [-0.03363861  0.01529835 -0.02699208 ... -0.01785287  0.03160063\n",
       "    0.00813966]], shape=(1024, 3072), dtype=float32),\n",
       " 'transformer_4_linear_1_qkv_bias': Tensor(data=[-0.0184683   0.00189373  0.01842855 ...  0.00732484 -0.03102981\n",
       "  -0.00615436], shape=(3072,), dtype=float32),\n",
       " 'transformer_4_linear_2_o_weight': Tensor(data=[[ 0.0448775   0.03499355 -0.03716425 ...  0.0120696  -0.01107383\n",
       "    0.05771834]\n",
       "  [ 0.03406559  0.04028556 -0.07114576 ...  0.05726188 -0.00709276\n",
       "    0.00050976]\n",
       "  [ 0.01938618 -0.0112037   0.0401326  ...  0.0541415   0.01845769\n",
       "    0.06040578]\n",
       "  ...\n",
       "  [-0.02344784 -0.08669417  0.10479694 ...  0.04299618 -0.04872547\n",
       "   -0.01621011]\n",
       "  [-0.01702731 -0.05654263 -0.00703123 ...  0.02824584  0.03747129\n",
       "   -0.03097383]\n",
       "  [-0.01028372 -0.10901477 -0.02088159 ...  0.0592527   0.05806717\n",
       "   -0.03272034]], shape=(1024, 1024), dtype=float32),\n",
       " 'transformer_4_linear_2_o_bias': Tensor(data=[ 0.02902335 -0.04724545  0.0128741  ...  0.01795215  0.06736258\n",
       "   0.0347245 ], shape=(1024,), dtype=float32),\n",
       " 'transformer_4_linear_3_proj_up_weight': Tensor(data=[[-0.16478708 -0.08028947  0.06291295 ...  0.01582323  0.00623813\n",
       "    0.0298818 ]\n",
       "  [-0.04009093 -0.08957618 -0.00505543 ... -0.08378704 -0.00520188\n",
       "    0.07671009]\n",
       "  [ 0.03998479  0.03967113  0.0091815  ... -0.02764436  0.09814193\n",
       "   -0.04109376]\n",
       "  ...\n",
       "  [-0.01447322  0.04201668  0.05477639 ... -0.02149974  0.08305679\n",
       "   -0.07205536]\n",
       "  [-0.07822438 -0.01471238 -0.01499549 ...  0.00545923 -0.03610552\n",
       "   -0.0498119 ]\n",
       "  [ 0.03597821  0.07605078 -0.04245761 ...  0.09622125  0.02177619\n",
       "   -0.02369188]], shape=(1024, 4096), dtype=float32),\n",
       " 'transformer_4_linear_3_proj_up_bias': Tensor(data=[ 0.00789785 -0.0201818  -0.02266734 ... -0.01747374 -0.04017043\n",
       "  -0.01337162], shape=(4096,), dtype=float32),\n",
       " 'transformer_4_linear_4_proj_down_weight': Tensor(data=[[-0.13456942  0.03917071  0.01188667 ...  0.05878359 -0.08276237\n",
       "   -0.0179323 ]\n",
       "  [-0.00703552  0.01597556  0.070358   ...  0.02276535 -0.04238032\n",
       "    0.09207918]\n",
       "  [ 0.03084536  0.05812974  0.13462994 ...  0.02216941  0.01943448\n",
       "    0.03144907]\n",
       "  ...\n",
       "  [-0.02487329 -0.0028296   0.02316643 ...  0.03210932  0.05529084\n",
       "    0.07495759]\n",
       "  [ 0.09079127  0.00712291  0.00978939 ...  0.11107963 -0.00872492\n",
       "   -0.020653  ]\n",
       "  [ 0.00669064  0.0384278  -0.02922715 ...  0.09800341  0.03357127\n",
       "    0.03884766]], shape=(4096, 1024), dtype=float32),\n",
       " 'transformer_4_linear_4_proj_down_bias': Tensor(data=[ 0.00617873 -0.00586607 -0.00468756 ...  0.02968772  0.02995292\n",
       "   0.01477909], shape=(1024,), dtype=float32),\n",
       " 'transformer_4_layernorm_1_gamma': Tensor(data=[0.68234164 0.6476211  0.628977   ... 0.618194   0.5865893  0.6416909 ], shape=(1024,), dtype=float32),\n",
       " 'transformer_4_layernorm_1_beta': Tensor(data=[-0.04145449  0.05053996 -0.05242916 ...  0.01989135 -0.16974002\n",
       "  -0.04952161], shape=(1024,), dtype=float32),\n",
       " 'transformer_4_layernorm_2_gamma': Tensor(data=[0.9896178  0.92416966 1.0007378  ... 0.9276662  1.1268849  0.99957734], shape=(1024,), dtype=float32),\n",
       " 'transformer_4_layernorm_2_beta': Tensor(data=[ 0.05772926 -0.07716578  0.03120017 ...  0.01277938  0.11388868\n",
       "   0.04142267], shape=(1024,), dtype=float32),\n",
       " 'transformer_5_linear_1_qkv_weight': Tensor(data=[[-0.05421903 -0.06882406  0.01888363 ...  0.06268941 -0.04037001\n",
       "    0.01386276]\n",
       "  [-0.06329861 -0.0890165   0.01652889 ...  0.0357005  -0.01666014\n",
       "    0.02663225]\n",
       "  [ 0.01371597  0.04763301 -0.04493332 ...  0.03146058  0.03511253\n",
       "   -0.0003614 ]\n",
       "  ...\n",
       "  [ 0.00267081  0.00612979  0.03968025 ... -0.01899037 -0.06560431\n",
       "    0.069047  ]\n",
       "  [-0.01199731  0.03631698  0.10601065 ... -0.03188885 -0.06868351\n",
       "    0.02422922]\n",
       "  [ 0.03183002  0.04124675 -0.13820973 ...  0.07195727  0.10855834\n",
       "    0.08013814]], shape=(1024, 3072), dtype=float32),\n",
       " 'transformer_5_linear_1_qkv_bias': Tensor(data=[-0.03302146 -0.10330394 -0.04452388 ... -0.03123036 -0.02637623\n",
       "   0.00522377], shape=(3072,), dtype=float32),\n",
       " 'transformer_5_linear_2_o_weight': Tensor(data=[[-0.0488203  -0.08938219 -0.02336773 ... -0.0017153   0.04401112\n",
       "    0.02718686]\n",
       "  [-0.0290312   0.10739468  0.01948281 ... -0.06802416  0.08482939\n",
       "   -0.06814519]\n",
       "  [ 0.01142353 -0.03706225 -0.04316642 ... -0.01672487  0.03068673\n",
       "   -0.03995738]\n",
       "  ...\n",
       "  [ 0.04695692 -0.06883558 -0.02329743 ...  0.02852009 -0.11610472\n",
       "   -0.09215789]\n",
       "  [-0.00268978 -0.03205394  0.01268891 ... -0.00800142  0.08063153\n",
       "   -0.02350043]\n",
       "  [-0.0020662   0.02921673 -0.0643587  ...  0.05771333 -0.08497512\n",
       "    0.02477732]], shape=(1024, 1024), dtype=float32),\n",
       " 'transformer_5_linear_2_o_bias': Tensor(data=[-0.00612629 -0.01074187  0.01961082 ...  0.04778508  0.06597184\n",
       "   0.02530783], shape=(1024,), dtype=float32),\n",
       " 'transformer_5_linear_3_proj_up_weight': Tensor(data=[[ 0.02008739 -0.01786695 -0.03615718 ... -0.03885154 -0.00599148\n",
       "   -0.01124793]\n",
       "  [ 0.00179212 -0.00645549 -0.09620143 ...  0.05282899  0.01679832\n",
       "    0.0192484 ]\n",
       "  [-0.04395759 -0.00971176  0.03692628 ... -0.08308517 -0.04356688\n",
       "    0.10631555]\n",
       "  ...\n",
       "  [ 0.07577275  0.05132472 -0.02779531 ...  0.08611982 -0.04190557\n",
       "    0.01888071]\n",
       "  [-0.03030616 -0.01986423  0.0320668  ...  0.01333837 -0.05669119\n",
       "    0.05768755]\n",
       "  [ 0.06165477  0.00480718  0.03353164 ... -0.02197686 -0.02354923\n",
       "    0.02168422]], shape=(1024, 4096), dtype=float32),\n",
       " 'transformer_5_linear_3_proj_up_bias': Tensor(data=[-0.0321351  -0.0205663  -0.03587694 ... -0.02408094 -0.0221764\n",
       "  -0.03228924], shape=(4096,), dtype=float32),\n",
       " 'transformer_5_linear_4_proj_down_weight': Tensor(data=[[ 0.0305275  -0.0412075  -0.0740582  ...  0.0752919   0.07327072\n",
       "    0.0214689 ]\n",
       "  [-0.02172785  0.10377289 -0.02220727 ...  0.00524457 -0.02997377\n",
       "   -0.02672007]\n",
       "  [-0.02976873 -0.03098177 -0.04603102 ...  0.01207251  0.04884738\n",
       "   -0.05002576]\n",
       "  ...\n",
       "  [-0.07034061  0.04374788  0.06289843 ...  0.0211674  -0.03657584\n",
       "   -0.005092  ]\n",
       "  [ 0.0647733  -0.00433961  0.0334922  ...  0.09258923  0.01346013\n",
       "   -0.08997639]\n",
       "  [ 0.03941872 -0.0279511  -0.02367673 ... -0.02059545 -0.07162177\n",
       "   -0.01690156]], shape=(4096, 1024), dtype=float32),\n",
       " 'transformer_5_linear_4_proj_down_bias': Tensor(data=[-0.03571089 -0.00256362  0.00803166 ...  0.10020978 -0.02645535\n",
       "   0.01204569], shape=(1024,), dtype=float32),\n",
       " 'transformer_5_layernorm_1_gamma': Tensor(data=[0.7852036  0.69897836 0.6695884  ... 0.69958556 0.6632181  0.74141836], shape=(1024,), dtype=float32),\n",
       " 'transformer_5_layernorm_1_beta': Tensor(data=[ 0.02780237  0.00182121 -0.02665775 ... -0.01967961 -0.08250879\n",
       "   0.00441681], shape=(1024,), dtype=float32),\n",
       " 'transformer_5_layernorm_2_gamma': Tensor(data=[0.9851116  0.932082   0.98707795 ... 0.88074213 1.0756148  0.97043115], shape=(1024,), dtype=float32),\n",
       " 'transformer_5_layernorm_2_beta': Tensor(data=[ 0.00833154 -0.00132351  0.0270498  ...  0.0635369   0.08950005\n",
       "   0.02536442], shape=(1024,), dtype=float32),\n",
       " 'transformer_6_linear_1_qkv_weight': Tensor(data=[[-0.01039517  0.02876588 -0.01970778 ...  0.00087465  0.05800103\n",
       "   -0.02482408]\n",
       "  [ 0.00857945  0.0766236   0.04372195 ... -0.03311494 -0.00854692\n",
       "    0.0978673 ]\n",
       "  [-0.04358723  0.04422001  0.01721629 ... -0.04485232 -0.0114019\n",
       "   -0.00637108]\n",
       "  ...\n",
       "  [ 0.00867954 -0.02528898  0.07243495 ...  0.01590688  0.03314736\n",
       "   -0.07535022]\n",
       "  [-0.02451061 -0.01012993 -0.05603142 ...  0.05585393 -0.08249968\n",
       "    0.0221158 ]\n",
       "  [-0.10802306 -0.07720055 -0.08365837 ...  0.06488213 -0.02067481\n",
       "   -0.03733   ]], shape=(1024, 3072), dtype=float32),\n",
       " 'transformer_6_linear_1_qkv_bias': Tensor(data=[ 0.08300671  0.01078085 -0.00122057 ... -0.0002821  -0.01385332\n",
       "   0.03351273], shape=(3072,), dtype=float32),\n",
       " 'transformer_6_linear_2_o_weight': Tensor(data=[[-0.0036531   0.03081515 -0.02925749 ... -0.00269619 -0.00642896\n",
       "   -0.07722156]\n",
       "  [-0.02770765 -0.04365558 -0.00345814 ... -0.02511436  0.01571515\n",
       "    0.03646398]\n",
       "  [ 0.03434272 -0.00191093  0.03890887 ...  0.06457282  0.01528855\n",
       "   -0.0009646 ]\n",
       "  ...\n",
       "  [-0.04670892  0.02689028  0.01293129 ... -0.00896621 -0.02538271\n",
       "    0.02239042]\n",
       "  [-0.02791087  0.0854009   0.01429729 ...  0.01750226  0.0417572\n",
       "    0.02968344]\n",
       "  [-0.00013535 -0.02890323  0.01615923 ...  0.00133556 -0.02676946\n",
       "   -0.02801057]], shape=(1024, 1024), dtype=float32),\n",
       " 'transformer_6_linear_2_o_bias': Tensor(data=[-0.05297148  0.01000093  0.02857582 ...  0.11543464 -0.01385163\n",
       "   0.04000191], shape=(1024,), dtype=float32),\n",
       " 'transformer_6_linear_3_proj_up_weight': Tensor(data=[[ 6.51602075e-02  2.86039338e-02 -4.96369079e-02 ...  1.10354885e-01\n",
       "   -9.31789447e-03  4.81426269e-02]\n",
       "  [-4.54190001e-02 -3.19383591e-02  9.74775702e-02 ... -1.19370015e-04\n",
       "    6.06405959e-02 -1.97765455e-02]\n",
       "  [-2.07082536e-02 -1.64949689e-02 -3.74227576e-02 ...  4.99210460e-03\n",
       "   -1.56401411e-01  3.93490642e-02]\n",
       "  ...\n",
       "  [ 1.00449501e-02  5.46052214e-03 -5.89062311e-02 ... -7.43555129e-02\n",
       "   -5.35921380e-02 -3.55674662e-02]\n",
       "  [-7.73718134e-02 -1.16222836e-02 -3.02145351e-02 ... -2.51223128e-02\n",
       "   -5.63546456e-02  6.09692894e-02]\n",
       "  [-1.25609338e-02 -1.89265213e-03 -3.77103984e-02 ... -2.64100134e-02\n",
       "   -4.67338879e-03 -3.79897878e-02]], shape=(1024, 4096), dtype=float32),\n",
       " 'transformer_6_linear_3_proj_up_bias': Tensor(data=[-0.00855162  0.00247277 -0.04164016 ... -0.03191062 -0.01457505\n",
       "  -0.00360059], shape=(4096,), dtype=float32),\n",
       " 'transformer_6_linear_4_proj_down_weight': Tensor(data=[[-4.4280984e-02 -5.0988548e-02  1.2126097e-02 ... -3.6024649e-02\n",
       "   -7.9018012e-02  1.3661036e-01]\n",
       "  [-1.6578564e-02 -2.3146341e-02  3.5788737e-02 ...  9.1195203e-02\n",
       "    4.2665411e-02 -1.0104373e-04]\n",
       "  [ 8.9839665e-04  3.5524059e-02  3.8112137e-02 ... -1.1117558e-01\n",
       "   -4.2464592e-02  2.9439801e-02]\n",
       "  ...\n",
       "  [ 2.2521598e-02  3.8001426e-02 -6.8230569e-02 ...  1.3330644e-01\n",
       "   -1.1235133e-02  8.9269571e-02]\n",
       "  [ 3.0284137e-02 -4.3820878e-03  4.0528416e-03 ... -4.6501894e-02\n",
       "   -6.4628690e-02  8.4203137e-03]\n",
       "  [-2.3180131e-02  2.3693212e-03  3.7926532e-02 ... -9.0440363e-03\n",
       "   -3.9657079e-02 -7.4923530e-02]], shape=(4096, 1024), dtype=float32),\n",
       " 'transformer_6_linear_4_proj_down_bias': Tensor(data=[ 0.0196649   0.00443567  0.02861375 ...  0.10424818 -0.02426579\n",
       "   0.06139711], shape=(1024,), dtype=float32),\n",
       " 'transformer_6_layernorm_1_gamma': Tensor(data=[0.6628275  0.6785186  0.6282372  ... 0.622623   0.61389744 0.67453057], shape=(1024,), dtype=float32),\n",
       " 'transformer_6_layernorm_1_beta': Tensor(data=[ 0.02839023 -0.02227178 -0.0107552  ... -0.00985506 -0.06574629\n",
       "  -0.02620257], shape=(1024,), dtype=float32),\n",
       " 'transformer_6_layernorm_2_gamma': Tensor(data=[1.0088896  0.90020996 0.96843547 ... 0.9514244  1.0115296  0.960953  ], shape=(1024,), dtype=float32),\n",
       " 'transformer_6_layernorm_2_beta': Tensor(data=[-0.03846848  0.0085912   0.03709497 ...  0.1346416  -0.00188906\n",
       "   0.03708792], shape=(1024,), dtype=float32),\n",
       " 'transformer_7_linear_1_qkv_weight': Tensor(data=[[ 0.06309006 -0.02586839 -0.00268135 ...  0.05071604  0.000178\n",
       "    0.06126851]\n",
       "  [-0.07630821  0.03376791 -0.05215062 ...  0.05580351 -0.0108879\n",
       "   -0.03769008]\n",
       "  [ 0.00091999  0.07113881  0.05185437 ...  0.00392922  0.02725882\n",
       "   -0.05930443]\n",
       "  ...\n",
       "  [-0.03066697  0.06958813 -0.03858583 ...  0.00251266 -0.01020761\n",
       "   -0.01319843]\n",
       "  [-0.10628992 -0.02352859  0.04311313 ...  0.07937161  0.02153547\n",
       "   -0.04403805]\n",
       "  [ 0.00248585 -0.02390467 -0.00179038 ... -0.00143221 -0.03803774\n",
       "   -0.05874114]], shape=(1024, 3072), dtype=float32),\n",
       " 'transformer_7_linear_1_qkv_bias': Tensor(data=[-0.01339806  0.08378093  0.02612416 ...  0.00498469  0.00089872\n",
       "  -0.00130055], shape=(3072,), dtype=float32),\n",
       " 'transformer_7_linear_2_o_weight': Tensor(data=[[-0.00118751 -0.00378099 -0.00225674 ... -0.03447643  0.03802417\n",
       "   -0.09790169]\n",
       "  [ 0.09125257 -0.00269614  0.02725566 ...  0.01142705 -0.01751897\n",
       "   -0.04333367]\n",
       "  [ 0.0463784  -0.01260424 -0.06204037 ...  0.04003224 -0.04482179\n",
       "   -0.03135522]\n",
       "  ...\n",
       "  [-0.08962812 -0.06387427 -0.00263051 ... -0.03637683  0.03683833\n",
       "    0.0715933 ]\n",
       "  [ 0.00599654 -0.05143116 -0.0550129  ... -0.11037136 -0.0030622\n",
       "    0.01189882]\n",
       "  [-0.00260353  0.02765776  0.02290349 ... -0.02702724 -0.02830784\n",
       "   -0.06350833]], shape=(1024, 1024), dtype=float32),\n",
       " 'transformer_7_linear_2_o_bias': Tensor(data=[ 5.3555268e-05  1.0132136e-02  3.7418380e-02 ...  9.7106747e-02\n",
       "  -3.1084105e-02  4.3430474e-02], shape=(1024,), dtype=float32),\n",
       " 'transformer_7_linear_3_proj_up_weight': Tensor(data=[[-0.05252578  0.07273924  0.01346151 ...  0.03036481 -0.00805347\n",
       "   -0.0301172 ]\n",
       "  [-0.02983133  0.01819117 -0.04566664 ... -0.04605244  0.05486516\n",
       "   -0.03021038]\n",
       "  [-0.00557572 -0.01361598 -0.00587761 ...  0.03324975  0.00368908\n",
       "   -0.02975331]\n",
       "  ...\n",
       "  [ 0.01672667  0.00504868 -0.04148481 ... -0.01144763  0.03375946\n",
       "   -0.03893657]\n",
       "  [-0.00465918 -0.02614793  0.05348084 ...  0.01192684 -0.049206\n",
       "    0.05658992]\n",
       "  [-0.01989551 -0.07799006  0.05213719 ... -0.0100991  -0.05180009\n",
       "   -0.00839256]], shape=(1024, 4096), dtype=float32),\n",
       " 'transformer_7_linear_3_proj_up_bias': Tensor(data=[-0.00703905 -0.01417947 -0.02528243 ... -0.04339099 -0.03601324\n",
       "  -0.02385899], shape=(4096,), dtype=float32),\n",
       " 'transformer_7_linear_4_proj_down_weight': Tensor(data=[[ 0.00508601  0.04904451 -0.0303521  ... -0.00762924  0.00304844\n",
       "   -0.10226383]\n",
       "  [ 0.02867123 -0.13412838 -0.04144873 ... -0.02553283  0.02973634\n",
       "   -0.02298099]\n",
       "  [ 0.00665333  0.0166877   0.07676146 ... -0.10290264 -0.06129071\n",
       "    0.00049623]\n",
       "  ...\n",
       "  [-0.01624848 -0.04086282 -0.01211219 ...  0.00482718 -0.02868731\n",
       "   -0.05467393]\n",
       "  [-0.00710589 -0.02109881 -0.09382562 ...  0.01230428  0.08473049\n",
       "    0.02735206]\n",
       "  [-0.06599373  0.04653022 -0.05104478 ... -0.02103185  0.0702905\n",
       "   -0.02348896]], shape=(4096, 1024), dtype=float32),\n",
       " 'transformer_7_linear_4_proj_down_bias': Tensor(data=[ 0.02163084  0.00877051  0.02924295 ...  0.01120244 -0.027002\n",
       "  -0.01730864], shape=(1024,), dtype=float32),\n",
       " 'transformer_7_layernorm_1_gamma': Tensor(data=[0.58119196 0.5751594  0.6045164  ... 0.6227183  0.5572801  0.6013108 ], shape=(1024,), dtype=float32),\n",
       " 'transformer_7_layernorm_1_beta': Tensor(data=[ 0.04090063 -0.03224654 -0.00301391 ... -0.01053899  0.01310428\n",
       "   0.03297983], shape=(1024,), dtype=float32),\n",
       " 'transformer_7_layernorm_2_gamma': Tensor(data=[0.9603995  0.9126468  0.9441536  ... 0.94094896 0.9810965  0.95218045], shape=(1024,), dtype=float32),\n",
       " 'transformer_7_layernorm_2_beta': Tensor(data=[ 0.0189855   0.01866846  0.04080129 ...  0.11735518 -0.01954851\n",
       "   0.05386868], shape=(1024,), dtype=float32),\n",
       " 'transformer_8_linear_1_qkv_weight': Tensor(data=[[-0.04193223 -0.04535528  0.01044851 ...  0.01210189 -0.03938301\n",
       "   -0.02699088]\n",
       "  [-0.00843923  0.12176543 -0.07572642 ...  0.02922516 -0.05334439\n",
       "    0.04380558]\n",
       "  [ 0.04240707  0.07989618 -0.06036611 ...  0.06001277 -0.00082234\n",
       "   -0.02237537]\n",
       "  ...\n",
       "  [ 0.0374846   0.0123179  -0.02695313 ...  0.03342456  0.00712388\n",
       "   -0.04972975]\n",
       "  [ 0.02133661  0.01532535 -0.03373959 ... -0.00385567 -0.01622651\n",
       "    0.01225957]\n",
       "  [-0.01187554  0.06155313 -0.00320438 ... -0.03072761 -0.07316125\n",
       "    0.03465687]], shape=(1024, 3072), dtype=float32),\n",
       " 'transformer_8_linear_1_qkv_bias': Tensor(data=[ 0.01025013  0.02900107  0.00194637 ... -0.01017383 -0.03773427\n",
       "   0.00507921], shape=(3072,), dtype=float32),\n",
       " 'transformer_8_linear_2_o_weight': Tensor(data=[[ 0.07130615  0.07387201 -0.06353096 ... -0.01990908  0.01026221\n",
       "   -0.007319  ]\n",
       "  [ 0.0369837   0.05749109  0.11041036 ... -0.01479805  0.07517503\n",
       "   -0.03483327]\n",
       "  [ 0.01331994  0.03070093 -0.01844857 ... -0.0552028  -0.01899723\n",
       "   -0.01722835]\n",
       "  ...\n",
       "  [-0.07146357 -0.06477433  0.03174306 ...  0.01224614 -0.04829701\n",
       "   -0.09585836]\n",
       "  [-0.01298363 -0.0262055  -0.00449886 ...  0.03676176 -0.02605604\n",
       "   -0.04182306]\n",
       "  [-0.00978631  0.08211127 -0.00676777 ... -0.07669937  0.02878872\n",
       "    0.00309366]], shape=(1024, 1024), dtype=float32),\n",
       " 'transformer_8_linear_2_o_bias': Tensor(data=[ 0.01083716  0.01943072  0.02096443 ...  0.0277428  -0.02674558\n",
       "  -0.01913341], shape=(1024,), dtype=float32),\n",
       " 'transformer_8_linear_3_proj_up_weight': Tensor(data=[[-0.01041824 -0.02213098  0.01341714 ... -0.08474041 -0.03145583\n",
       "    0.03709979]\n",
       "  [-0.01276619 -0.03892294 -0.04939937 ... -0.09702053  0.04220522\n",
       "   -0.01322904]\n",
       "  [ 0.02505122 -0.02567272 -0.0177701  ...  0.06309699  0.07090385\n",
       "   -0.04312365]\n",
       "  ...\n",
       "  [ 0.02059534  0.00255794 -0.04256931 ... -0.00211335 -0.0286453\n",
       "   -0.01332886]\n",
       "  [-0.03167154  0.08305268  0.03316545 ... -0.00976088  0.06135618\n",
       "    0.02953061]\n",
       "  [ 0.02694545  0.02517164 -0.01341184 ...  0.04233791  0.06955016\n",
       "    0.03371062]], shape=(1024, 4096), dtype=float32),\n",
       " 'transformer_8_linear_3_proj_up_bias': Tensor(data=[ 0.01207502 -0.01445186 -0.03570434 ... -0.02555984 -0.00130717\n",
       "  -0.03038058], shape=(4096,), dtype=float32),\n",
       " 'transformer_8_linear_4_proj_down_weight': Tensor(data=[[-0.01775214 -0.02530277 -0.06132082 ...  0.01003592 -0.00162698\n",
       "    0.03630361]\n",
       "  [-0.05584795  0.05777092 -0.04298951 ... -0.05434482 -0.01145501\n",
       "    0.0538293 ]\n",
       "  [-0.01261606 -0.06909601  0.08069173 ... -0.00461944  0.01343128\n",
       "   -0.08292346]\n",
       "  ...\n",
       "  [ 0.0053132  -0.05176882  0.0296286  ... -0.12897317  0.07887591\n",
       "   -0.03198849]\n",
       "  [-0.06072833  0.10562359  0.01477838 ...  0.04459608  0.0458198\n",
       "    0.04439656]\n",
       "  [-0.02607159 -0.09112286  0.02688545 ...  0.01453563 -0.04474926\n",
       "    0.05471802]], shape=(4096, 1024), dtype=float32),\n",
       " 'transformer_8_linear_4_proj_down_bias': Tensor(data=[-0.00207143  0.00919915  0.00617009 ...  0.03539243 -0.04766288\n",
       "  -0.00591113], shape=(1024,), dtype=float32),\n",
       " 'transformer_8_layernorm_1_gamma': Tensor(data=[0.6177121  0.57454175 0.5573231  ... 0.6009693  0.6082366  0.5949513 ], shape=(1024,), dtype=float32),\n",
       " 'transformer_8_layernorm_1_beta': Tensor(data=[ 0.00811835 -0.02820332 -0.01384311 ... -0.08827994  0.01491952\n",
       "   0.00496711], shape=(1024,), dtype=float32),\n",
       " 'transformer_8_layernorm_2_gamma': Tensor(data=[0.96149063 0.94825935 0.9635396  ... 0.9512702  1.0056598  0.95181537], shape=(1024,), dtype=float32),\n",
       " 'transformer_8_layernorm_2_beta': Tensor(data=[ 0.04468742  0.02765285  0.02823452 ...  0.04972146 -0.03574559\n",
       "  -0.01514955], shape=(1024,), dtype=float32),\n",
       " 'transformer_9_linear_1_qkv_weight': Tensor(data=[[-0.00989602  0.05279307  0.01200898 ... -0.06094802 -0.05142808\n",
       "   -0.01962767]\n",
       "  [ 0.00851391 -0.13071902 -0.11928079 ... -0.05424006  0.0040013\n",
       "   -0.05116038]\n",
       "  [-0.01188407 -0.02093739  0.03313186 ...  0.00410714 -0.00485531\n",
       "    0.01083416]\n",
       "  ...\n",
       "  [-0.07918838 -0.03646618 -0.02659603 ... -0.02672155 -0.00049625\n",
       "    0.04896005]\n",
       "  [ 0.09948707  0.0083453  -0.04120014 ...  0.07711533  0.07916963\n",
       "    0.02655247]\n",
       "  [-0.02199532 -0.03397168  0.02062228 ...  0.06951237  0.04836391\n",
       "   -0.04607265]], shape=(1024, 3072), dtype=float32),\n",
       " 'transformer_9_linear_1_qkv_bias': Tensor(data=[ 0.14238474 -0.01384666  0.05040367 ... -0.00290733  0.01241887\n",
       "   0.01761651], shape=(3072,), dtype=float32),\n",
       " 'transformer_9_linear_2_o_weight': Tensor(data=[[ 0.06908505 -0.04593624 -0.00289518 ... -0.04924067  0.01316774\n",
       "    0.09392504]\n",
       "  [-0.00903972  0.00767954  0.03327191 ... -0.0946495  -0.01048737\n",
       "    0.01036031]\n",
       "  [-0.01895568  0.12665698  0.07301629 ...  0.08473866  0.02430159\n",
       "    0.0013715 ]\n",
       "  ...\n",
       "  [ 0.01826448  0.02126788 -0.02677197 ... -0.00901753  0.02639463\n",
       "   -0.11635977]\n",
       "  [ 0.0359526  -0.07872303 -0.01822663 ...  0.00086885 -0.08982716\n",
       "    0.05056695]\n",
       "  [ 0.03486712 -0.05619076  0.01853846 ... -0.0607529   0.0705725\n",
       "   -0.06566302]], shape=(1024, 1024), dtype=float32),\n",
       " 'transformer_9_linear_2_o_bias': Tensor(data=[-0.01345668  0.03214616 -0.01462717 ...  0.03365025 -0.02514457\n",
       "  -0.01628772], shape=(1024,), dtype=float32),\n",
       " 'transformer_9_linear_3_proj_up_weight': Tensor(data=[[ 0.0642907   0.01520458 -0.06479184 ... -0.02197239  0.07116441\n",
       "   -0.03026678]\n",
       "  [ 0.06065065 -0.04062232 -0.0429316  ... -0.02846263  0.07155009\n",
       "    0.04983756]\n",
       "  [-0.02133525 -0.08807442  0.01284921 ...  0.12389637  0.04853607\n",
       "   -0.04625433]\n",
       "  ...\n",
       "  [-0.02409282 -0.05310485 -0.02414098 ... -0.01901287 -0.06657277\n",
       "   -0.05466292]\n",
       "  [ 0.0071356   0.0028522  -0.01665188 ...  0.11500935  0.09201803\n",
       "    0.0409323 ]\n",
       "  [-0.02049236 -0.01520532  0.03519938 ...  0.02818931  0.03893395\n",
       "    0.00324501]], shape=(1024, 4096), dtype=float32),\n",
       " 'transformer_9_linear_3_proj_up_bias': Tensor(data=[-0.00707981 -0.02369141 -0.02077124 ... -0.04118186  0.00528076\n",
       "  -0.02454487], shape=(4096,), dtype=float32),\n",
       " 'transformer_9_linear_4_proj_down_weight': Tensor(data=[[ 0.03977539 -0.0591931   0.03221248 ... -0.01232731 -0.0025609\n",
       "    0.01024857]\n",
       "  [-0.0424292  -0.01164815  0.03353652 ... -0.02998926  0.02230498\n",
       "    0.04004052]\n",
       "  [ 0.03642873 -0.04816457  0.03773519 ...  0.01701223 -0.06803899\n",
       "    0.00326099]\n",
       "  ...\n",
       "  [ 0.01315379  0.04884096  0.03996909 ...  0.05600152 -0.0450527\n",
       "   -0.03981742]\n",
       "  [ 0.07822381 -0.08128865 -0.05397821 ...  0.01468854  0.07658333\n",
       "    0.03269688]\n",
       "  [-0.0015759  -0.00432215  0.02834953 ... -0.01017751 -0.01421047\n",
       "    0.06966804]], shape=(4096, 1024), dtype=float32),\n",
       " 'transformer_9_linear_4_proj_down_bias': Tensor(data=[-0.00421371  0.0360491  -0.0313792  ... -0.0457137  -0.03055719\n",
       "   0.00379461], shape=(1024,), dtype=float32),\n",
       " 'transformer_9_layernorm_1_gamma': Tensor(data=[0.7342956  0.64246947 0.6650128  ... 0.6836331  0.7287054  0.68115985], shape=(1024,), dtype=float32),\n",
       " 'transformer_9_layernorm_1_beta': Tensor(data=[ 0.00899229 -0.03457904  0.01475256 ... -0.00817154 -0.02645474\n",
       "   0.05365398], shape=(1024,), dtype=float32),\n",
       " 'transformer_9_layernorm_2_gamma': Tensor(data=[1.0252148  0.96630293 0.96260077 ... 0.98562026 1.0166914  0.976262  ], shape=(1024,), dtype=float32),\n",
       " 'transformer_9_layernorm_2_beta': Tensor(data=[ 0.00194566  0.05360188 -0.00127245 ...  0.0447977  -0.02608838\n",
       "  -0.01306876], shape=(1024,), dtype=float32),\n",
       " 'transformer_10_linear_1_qkv_weight': Tensor(data=[[-0.04448429 -0.05744357  0.06561248 ...  0.07324636 -0.0105716\n",
       "    0.04132855]\n",
       "  [-0.00896385 -0.06784122 -0.00498249 ... -0.00982954 -0.01713056\n",
       "    0.02619732]\n",
       "  [ 0.0158958   0.03221456  0.07180084 ... -0.01068425 -0.03176039\n",
       "    0.03729938]\n",
       "  ...\n",
       "  [ 0.05858793  0.00720251  0.01087151 ... -0.06808309  0.03177925\n",
       "    0.01286578]\n",
       "  [ 0.04794298  0.00148707  0.04859246 ...  0.0218335  -0.00840619\n",
       "    0.07562348]\n",
       "  [-0.00949681 -0.05179821 -0.04405968 ...  0.02451794  0.06673417\n",
       "    0.04049176]], shape=(1024, 3072), dtype=float32),\n",
       " 'transformer_10_linear_1_qkv_bias': Tensor(data=[ 0.04569415  0.05953404  0.04027736 ... -0.01736528  0.00108956\n",
       "  -0.0172471 ], shape=(3072,), dtype=float32),\n",
       " 'transformer_10_linear_2_o_weight': Tensor(data=[[ 0.05103151  0.00269199 -0.02336412 ... -0.00159955 -0.00195862\n",
       "    0.057416  ]\n",
       "  [-0.00932303 -0.01310406 -0.01425041 ...  0.03134026  0.00741971\n",
       "    0.02188168]\n",
       "  [-0.00803113  0.0076035   0.04793889 ... -0.07861778 -0.02728275\n",
       "   -0.01701862]\n",
       "  ...\n",
       "  [ 0.0710121  -0.05922839 -0.01021781 ... -0.00821293 -0.03260563\n",
       "   -0.05054685]\n",
       "  [ 0.02391462  0.02002454 -0.00348245 ... -0.02375982  0.06820983\n",
       "   -0.05265108]\n",
       "  [-0.06178747 -0.05620011 -0.0076532  ... -0.02660262  0.06703899\n",
       "   -0.00102212]], shape=(1024, 1024), dtype=float32),\n",
       " 'transformer_10_linear_2_o_bias': Tensor(data=[-0.01597743  0.03627407 -0.03388481 ... -0.01267982 -0.02725177\n",
       "  -0.00584718], shape=(1024,), dtype=float32),\n",
       " 'transformer_10_linear_3_proj_up_weight': Tensor(data=[[-0.04750977  0.00044323 -0.05828007 ... -0.09148361  0.01157315\n",
       "    0.03852371]\n",
       "  [-0.02826947 -0.0254677   0.03445967 ... -0.01760305 -0.03077769\n",
       "    0.04733597]\n",
       "  [-0.02652238 -0.00540162  0.06760913 ...  0.03829927 -0.00429458\n",
       "    0.04994618]\n",
       "  ...\n",
       "  [ 0.00819059  0.0263076   0.00517327 ...  0.04548226  0.01097848\n",
       "   -0.08994427]\n",
       "  [ 0.03861436 -0.00614367  0.00043597 ... -0.00376575  0.02514726\n",
       "   -0.02891083]\n",
       "  [ 0.02044872  0.02061388 -0.0095721  ...  0.00805238  0.00816091\n",
       "   -0.02920614]], shape=(1024, 4096), dtype=float32),\n",
       " 'transformer_10_linear_3_proj_up_bias': Tensor(data=[-0.03933812 -0.04164247 -0.03094032 ... -0.03011398 -0.03592239\n",
       "  -0.03059348], shape=(4096,), dtype=float32),\n",
       " 'transformer_10_linear_4_proj_down_weight': Tensor(data=[[-0.0833365  -0.0062739  -0.07286308 ...  0.0003836   0.04955401\n",
       "   -0.02239875]\n",
       "  [ 0.02838226  0.00107427  0.02255333 ... -0.01652835  0.0745057\n",
       "    0.08154655]\n",
       "  [-0.02281128  0.08046842 -0.03972464 ... -0.02624691  0.07281949\n",
       "   -0.00744881]\n",
       "  ...\n",
       "  [ 0.03435977  0.03032289 -0.02122231 ...  0.10291457  0.10051991\n",
       "   -0.00792482]\n",
       "  [-0.01148373 -0.01552331 -0.01019326 ... -0.03487976  0.07301042\n",
       "   -0.01458975]\n",
       "  [ 0.03580817  0.00893959 -0.09229823 ... -0.01509932  0.03197645\n",
       "   -0.00499609]], shape=(4096, 1024), dtype=float32),\n",
       " 'transformer_10_linear_4_proj_down_bias': Tensor(data=[ 0.00874574  0.01887227 -0.00245009 ...  0.0154605  -0.03547968\n",
       "  -0.01604944], shape=(1024,), dtype=float32),\n",
       " 'transformer_10_layernorm_1_gamma': Tensor(data=[0.64014363 0.6253275  0.6088011  ... 0.6344208  0.6510399  0.62288153], shape=(1024,), dtype=float32),\n",
       " 'transformer_10_layernorm_1_beta': Tensor(data=[ 0.033162    0.02966817  0.03156278 ... -0.09736888  0.04026897\n",
       "   0.04081465], shape=(1024,), dtype=float32),\n",
       " 'transformer_10_layernorm_2_gamma': Tensor(data=[0.9951931  0.97551686 0.9872654  ... 0.9510279  0.9906126  0.96852285], shape=(1024,), dtype=float32),\n",
       " 'transformer_10_layernorm_2_beta': Tensor(data=[-0.01870454  0.04264484 -0.04156772 ... -0.01322416 -0.01079137\n",
       "   0.01555293], shape=(1024,), dtype=float32),\n",
       " 'transformer_11_linear_1_qkv_weight': Tensor(data=[[ 0.02515745 -0.00640632  0.03533457 ...  0.03376834  0.07267865\n",
       "    0.00744986]\n",
       "  [-0.04980591  0.05110882 -0.08245712 ...  0.00269551  0.02080993\n",
       "    0.01607043]\n",
       "  [-0.070875    0.0239012   0.07915121 ... -0.02062701 -0.00243774\n",
       "    0.07298935]\n",
       "  ...\n",
       "  [ 0.03449941  0.09051145  0.01636665 ...  0.11794981  0.00976055\n",
       "   -0.07418028]\n",
       "  [-0.00855884 -0.03837813  0.03567181 ... -0.05076152  0.01776688\n",
       "   -0.06874182]\n",
       "  [-0.02571533 -0.02206979 -0.00411098 ...  0.03979613 -0.04675724\n",
       "    0.05730281]], shape=(1024, 3072), dtype=float32),\n",
       " 'transformer_11_linear_1_qkv_bias': Tensor(data=[-0.09001812 -0.01892509 -0.05338415 ... -0.00261127  0.01380885\n",
       "   0.00624785], shape=(3072,), dtype=float32),\n",
       " 'transformer_11_linear_2_o_weight': Tensor(data=[[-0.02836159  0.05006607 -0.01838418 ... -0.03266946 -0.08813493\n",
       "    0.01415682]\n",
       "  [-0.00525405  0.00703865 -0.00514576 ...  0.001115    0.03717632\n",
       "    0.04945702]\n",
       "  [-0.12441094  0.06564374 -0.0751121  ... -0.02799757  0.00812702\n",
       "   -0.09539512]\n",
       "  ...\n",
       "  [-0.07850546 -0.00528352 -0.00437989 ... -0.0265382   0.1139254\n",
       "    0.01367993]\n",
       "  [-0.03604231  0.09981348  0.00327845 ...  0.0254265  -0.02818618\n",
       "   -0.03510124]\n",
       "  [ 0.05917603 -0.00302156 -0.10128778 ...  0.0344791  -0.0220699\n",
       "   -0.02366463]], shape=(1024, 1024), dtype=float32),\n",
       " 'transformer_11_linear_2_o_bias': Tensor(data=[ 0.02193057  0.01890102 -0.02621073 ...  0.03652603 -0.03371337\n",
       "  -0.01660203], shape=(1024,), dtype=float32),\n",
       " 'transformer_11_linear_3_proj_up_weight': Tensor(data=[[ 0.06522102  0.06098207 -0.02747756 ... -0.00359753  0.01184494\n",
       "    0.02418279]\n",
       "  [-0.04416933  0.08391543  0.02576491 ... -0.04351965  0.06935183\n",
       "   -0.05005258]\n",
       "  [-0.00426729  0.03231409 -0.01948473 ...  0.10976066  0.05052746\n",
       "   -0.0607254 ]\n",
       "  ...\n",
       "  [-0.05426036 -0.01336329 -0.00076129 ... -0.03468657 -0.02099687\n",
       "    0.086337  ]\n",
       "  [-0.01568915 -0.06023733  0.00223477 ...  0.05480497  0.0203519\n",
       "    0.03044931]\n",
       "  [ 0.12265247  0.08075976 -0.0044532  ...  0.0414655   0.03791623\n",
       "   -0.03322074]], shape=(1024, 4096), dtype=float32),\n",
       " 'transformer_11_linear_3_proj_up_bias': Tensor(data=[-0.01547408 -0.02057402 -0.01590257 ... -0.01760871 -0.03652429\n",
       "  -0.00818435], shape=(4096,), dtype=float32),\n",
       " 'transformer_11_linear_4_proj_down_weight': Tensor(data=[[-0.04853534  0.05986889 -0.01471944 ... -0.02554804 -0.03397293\n",
       "    0.04132213]\n",
       "  [ 0.00993134  0.03827165 -0.03051005 ... -0.01696278 -0.05236514\n",
       "    0.00972168]\n",
       "  [ 0.04634365  0.05180518 -0.03281394 ...  0.02405021 -0.09129776\n",
       "    0.01810284]\n",
       "  ...\n",
       "  [-0.02791211 -0.09236449  0.00987739 ...  0.045899   -0.00544991\n",
       "   -0.09827992]\n",
       "  [ 0.07589693  0.12935296 -0.07643762 ...  0.04386902 -0.01076122\n",
       "    0.07302838]\n",
       "  [ 0.02212241 -0.00413989 -0.05297574 ... -0.00499934 -0.01917995\n",
       "    0.06329454]], shape=(4096, 1024), dtype=float32),\n",
       " 'transformer_11_linear_4_proj_down_bias': Tensor(data=[ 0.03104343  0.0333591  -0.01966018 ... -0.01215287  0.02003234\n",
       "   0.03777514], shape=(1024,), dtype=float32),\n",
       " 'transformer_11_layernorm_1_gamma': Tensor(data=[0.71339023 0.69553083 0.67068547 ... 0.6749897  0.7058204  0.68820864], shape=(1024,), dtype=float32),\n",
       " 'transformer_11_layernorm_1_beta': Tensor(data=[ 0.01658993 -0.00447804  0.08615761 ... -0.00289515  0.0371376\n",
       "  -0.00417629], shape=(1024,), dtype=float32),\n",
       " 'transformer_11_layernorm_2_gamma': Tensor(data=[0.96593034 0.97783124 0.97907805 ... 0.9629359  0.9516918  0.9601835 ], shape=(1024,), dtype=float32),\n",
       " 'transformer_11_layernorm_2_beta': Tensor(data=[ 0.00964115  0.01423528 -0.03441647 ...  0.04773929  0.015973\n",
       "  -0.00259498], shape=(1024,), dtype=float32),\n",
       " 'transformer_12_linear_1_qkv_weight': Tensor(data=[[-0.06214317 -0.10759814  0.00295571 ...  0.0052479  -0.01723316\n",
       "    0.04646613]\n",
       "  [ 0.00646907  0.08667487 -0.03710753 ... -0.02217532 -0.04430176\n",
       "    0.00788822]\n",
       "  [ 0.02379129 -0.01134937  0.03927522 ...  0.03392306 -0.07915922\n",
       "    0.01185013]\n",
       "  ...\n",
       "  [-0.01461897 -0.02829187  0.01746321 ... -0.07817838  0.06019326\n",
       "    0.05011209]\n",
       "  [-0.00037437  0.03099805  0.07267348 ...  0.06588915  0.02473048\n",
       "    0.01088899]\n",
       "  [-0.03061097 -0.10068917 -0.00435087 ...  0.01848218 -0.00525988\n",
       "   -0.03455009]], shape=(1024, 3072), dtype=float32),\n",
       " 'transformer_12_linear_1_qkv_bias': Tensor(data=[ 0.06237555  0.10640056  0.05691931 ... -0.00817544  0.00988761\n",
       "   0.02120238], shape=(3072,), dtype=float32),\n",
       " 'transformer_12_linear_2_o_weight': Tensor(data=[[ 0.05039648  0.03001039  0.02220755 ...  0.00634975  0.02467024\n",
       "    0.05087872]\n",
       "  [ 0.04173647 -0.0381053  -0.00702019 ...  0.00420705 -0.00244515\n",
       "    0.07690705]\n",
       "  [-0.02076038 -0.01520681 -0.02919374 ... -0.00021284 -0.12023303\n",
       "   -0.02168425]\n",
       "  ...\n",
       "  [ 0.06591353  0.02528119  0.03778516 ... -0.05864756 -0.05585896\n",
       "    0.02290091]\n",
       "  [-0.01641901 -0.04351133 -0.04631228 ... -0.01333698  0.0386311\n",
       "   -0.03197073]\n",
       "  [-0.06073694  0.07686511  0.00589065 ... -0.02678531 -0.01979914\n",
       "    0.01494321]], shape=(1024, 1024), dtype=float32),\n",
       " 'transformer_12_linear_2_o_bias': Tensor(data=[ 0.06159312  0.08150567 -0.01887552 ... -0.01905806  0.03605485\n",
       "   0.04199483], shape=(1024,), dtype=float32),\n",
       " 'transformer_12_linear_3_proj_up_weight': Tensor(data=[[ 0.08375253 -0.03613254 -0.00223258 ...  0.14256679  0.00803499\n",
       "    0.05665289]\n",
       "  [-0.00568469  0.03693518 -0.00927843 ...  0.00062108  0.00389894\n",
       "   -0.01019009]\n",
       "  [ 0.01180849 -0.01240166  0.03020379 ...  0.05558047 -0.03251633\n",
       "    0.04721634]\n",
       "  ...\n",
       "  [ 0.04183591  0.08950087  0.02135386 ...  0.05949507 -0.066417\n",
       "    0.00553072]\n",
       "  [-0.03781286 -0.02383296 -0.04899725 ... -0.01160716 -0.04788803\n",
       "   -0.09031053]\n",
       "  [ 0.04418496  0.0349487   0.04947354 ... -0.03290446 -0.02847865\n",
       "    0.01436281]], shape=(1024, 4096), dtype=float32),\n",
       " 'transformer_12_linear_3_proj_up_bias': Tensor(data=[-0.05229548 -0.01103857 -0.04588    ... -0.00660501 -0.00816977\n",
       "  -0.02021441], shape=(4096,), dtype=float32),\n",
       " 'transformer_12_linear_4_proj_down_weight': Tensor(data=[[-0.01897506 -0.07189173 -0.00885834 ... -0.03435403 -0.05251214\n",
       "    0.09182154]\n",
       "  [-0.03372692 -0.00743559 -0.00451812 ...  0.06098258  0.00404948\n",
       "   -0.04378405]\n",
       "  [ 0.01531889 -0.0008953   0.00465423 ...  0.06064285 -0.05854045\n",
       "    0.01441197]\n",
       "  ...\n",
       "  [-0.00125673 -0.04258346  0.04573603 ...  0.06544985 -0.01116708\n",
       "   -0.06796339]\n",
       "  [-0.0757128   0.00184587 -0.05552822 ... -0.02682383 -0.03480585\n",
       "   -0.03762523]\n",
       "  [ 0.07474253 -0.00273486  0.00737035 ... -0.03380122  0.02398122\n",
       "   -0.08381816]], shape=(4096, 1024), dtype=float32),\n",
       " 'transformer_12_linear_4_proj_down_bias': Tensor(data=[ 0.01299506  0.06673083 -0.06814854 ... -0.01406193  0.02080953\n",
       "   0.00371999], shape=(1024,), dtype=float32),\n",
       " 'transformer_12_layernorm_1_gamma': Tensor(data=[0.7473061  0.6883885  0.6696841  ... 0.67257965 0.7003957  0.70447993], shape=(1024,), dtype=float32),\n",
       " 'transformer_12_layernorm_1_beta': Tensor(data=[ 0.00477353  0.01471884  0.02807979 ... -0.00185174  0.00769524\n",
       "   0.04431359], shape=(1024,), dtype=float32),\n",
       " 'transformer_12_layernorm_2_gamma': Tensor(data=[1.2037777 1.2427108 1.2245973 ... 1.2050428 1.2029078 1.1945652], shape=(1024,), dtype=float32),\n",
       " 'transformer_12_layernorm_2_beta': Tensor(data=[ 0.02460841  0.11690721 -0.07295818 ...  0.0058705   0.09014397\n",
       "   0.08537806], shape=(1024,), dtype=float32),\n",
       " 'linear_1_linear_1_project_weight': Tensor(data=[[-0.00964709  0.00142069 -0.00879394 ...  0.03899081 -0.01037935\n",
       "   -0.00988851]\n",
       "  [-0.01722171  0.0051686  -0.01911877 ...  0.00258146 -0.01760416\n",
       "   -0.01739514]\n",
       "  [-0.01532871 -0.02346402 -0.01561601 ...  0.03316545 -0.01525184\n",
       "   -0.0168892 ]\n",
       "  ...\n",
       "  [-0.01872628 -0.00413136 -0.01869652 ...  0.01556605 -0.01770146\n",
       "   -0.01747467]\n",
       "  [-0.00037874  0.01190394 -0.00072949 ... -0.02839878 -0.00090232\n",
       "    0.00034159]\n",
       "  [-0.01910313 -0.03052066 -0.01867934 ... -0.00233947 -0.0192964\n",
       "   -0.01952385]], shape=(1024, 51682), dtype=float32),\n",
       " 'linear_1_linear_1_project_bias': Tensor(data=[-1.3892317 -0.7241985 -1.3841693 ... -0.7586388 -1.3921164 -1.3914891], shape=(51682,), dtype=float32)}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dlx_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "42ff193a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('pos_emb',\n",
       "              tensor([[ 0.0177,  0.1510, -0.0230,  ..., -0.0227,  0.0222,  0.0122],\n",
       "                      [ 0.0500,  0.0305, -0.0338,  ..., -0.0325,  0.0180,  0.0076],\n",
       "                      [ 0.0590, -0.0301, -0.0375,  ..., -0.0141,  0.0101, -0.0410],\n",
       "                      ...,\n",
       "                      [-0.0207, -0.0701,  0.0541,  ...,  0.0216, -0.0123,  0.0154],\n",
       "                      [ 0.0009, -0.0463,  0.0465,  ...,  0.0221, -0.0290, -0.0015],\n",
       "                      [-0.0164, -0.0146,  0.0394,  ..., -0.0120, -0.0002,  0.0408]])),\n",
       "             ('token_emb.weight',\n",
       "              tensor([[ 0.0179,  0.0125,  0.0206,  ..., -0.0223,  0.0185, -0.0053],\n",
       "                      [-0.0129, -0.0741, -0.0638,  ...,  0.0076,  0.0233, -0.0124],\n",
       "                      [ 0.0170, -0.0011,  0.0011,  ..., -0.0156, -0.0083,  0.0197],\n",
       "                      ...,\n",
       "                      [ 0.0313, -0.0198, -0.0743,  ..., -0.0144, -0.0156,  0.0185],\n",
       "                      [-0.0351,  0.0171, -0.0023,  ..., -0.0205, -0.0137, -0.0165],\n",
       "                      [-0.0374,  0.0177,  0.0614,  ...,  0.0054, -0.0020,  0.0090]])),\n",
       "             ('blocks.0.ln1.weight',\n",
       "              tensor([0.4664, 0.3463, 0.5085,  ..., 0.5331, 0.5480, 0.5667])),\n",
       "             ('blocks.0.ln1.bias',\n",
       "              tensor([-0.0008, -0.0002,  0.0633,  ..., -0.0362,  0.0538, -0.0110])),\n",
       "             ('blocks.0.attn.qkv.weight',\n",
       "              tensor([[ 0.0808, -0.0375, -0.0250,  ..., -0.0213,  0.0049, -0.0041],\n",
       "                      [-0.0007,  0.0025, -0.0417,  ...,  0.0265, -0.0140,  0.0048],\n",
       "                      [-0.0067, -0.0083, -0.0153,  ...,  0.0414,  0.0152, -0.0336],\n",
       "                      ...,\n",
       "                      [-0.0263,  0.0199,  0.0155,  ..., -0.0712, -0.0084,  0.0481],\n",
       "                      [ 0.0753, -0.0036, -0.0464,  ..., -0.1020, -0.0814,  0.0400],\n",
       "                      [ 0.0221,  0.0101, -0.0607,  ...,  0.0190,  0.0188,  0.0009]])),\n",
       "             ('blocks.0.attn.qkv.bias',\n",
       "              tensor([-0.0051,  0.0022,  0.0128,  ..., -0.0364, -0.0047, -0.1136])),\n",
       "             ('blocks.0.attn.out_proj.weight',\n",
       "              tensor([[ 0.0407, -0.1365, -0.0223,  ...,  0.0213,  0.0706,  0.0336],\n",
       "                      [ 0.0077,  0.0034,  0.0215,  ...,  0.0508,  0.0466,  0.0232],\n",
       "                      [ 0.0168,  0.0156,  0.0127,  ..., -0.0132, -0.0110,  0.0404],\n",
       "                      ...,\n",
       "                      [ 0.0622,  0.0300,  0.0580,  ..., -0.0556,  0.0152,  0.0686],\n",
       "                      [-0.0273, -0.0019, -0.0344,  ...,  0.0153, -0.0311,  0.0140],\n",
       "                      [-0.0067,  0.0454,  0.0619,  ...,  0.0049,  0.0583,  0.0544]])),\n",
       "             ('blocks.0.attn.out_proj.bias',\n",
       "              tensor([-0.0232,  0.0343,  0.1394,  ...,  0.0041, -0.0083,  0.1382])),\n",
       "             ('blocks.0.ln2.weight',\n",
       "              tensor([0.9189, 0.8048, 1.0429,  ..., 0.8937, 0.8946, 1.0118])),\n",
       "             ('blocks.0.ln2.bias',\n",
       "              tensor([-0.0237,  0.0053,  0.1682,  ...,  0.0519, -0.0162,  0.1472])),\n",
       "             ('blocks.0.mlp.0.weight',\n",
       "              tensor([[-0.1198,  0.0232,  0.0258,  ..., -0.0111, -0.0425,  0.0156],\n",
       "                      [ 0.0388, -0.0952,  0.0213,  ...,  0.0324,  0.0587, -0.0304],\n",
       "                      [-0.0581, -0.0430, -0.0315,  ..., -0.0061,  0.0028,  0.0185],\n",
       "                      ...,\n",
       "                      [-0.0867,  0.0093, -0.1199,  ...,  0.0268,  0.0221, -0.0226],\n",
       "                      [-0.0481, -0.0336, -0.0203,  ..., -0.0089, -0.0093,  0.0541],\n",
       "                      [ 0.0530, -0.0142, -0.0498,  ...,  0.0199,  0.0096, -0.0939]])),\n",
       "             ('blocks.0.mlp.0.bias',\n",
       "              tensor([-0.0660, -0.0654, -0.0631,  ..., -0.0479, -0.0086, -0.0673])),\n",
       "             ('blocks.0.mlp.2.weight',\n",
       "              tensor([[ 0.0512, -0.0322, -0.0027,  ...,  0.0322, -0.0148,  0.0205],\n",
       "                      [-0.0140, -0.0823, -0.0187,  ...,  0.0673, -0.0179, -0.0212],\n",
       "                      [ 0.0155, -0.0203,  0.0017,  ..., -0.0483, -0.0554,  0.0754],\n",
       "                      ...,\n",
       "                      [-0.0583, -0.0578,  0.0322,  ..., -0.0275,  0.0024,  0.0099],\n",
       "                      [ 0.0918, -0.0469,  0.0032,  ..., -0.0919,  0.0327,  0.0196],\n",
       "                      [-0.0131, -0.0172,  0.0019,  ...,  0.0075,  0.0185,  0.0357]])),\n",
       "             ('blocks.0.mlp.2.bias',\n",
       "              tensor([-5.4821e-05,  2.7712e-03,  1.0383e-02,  ...,  8.1858e-03,\n",
       "                      -4.2867e-04, -8.4697e-04])),\n",
       "             ('blocks.1.ln1.weight',\n",
       "              tensor([0.7258, 0.5613, 0.6001,  ..., 0.6224, 0.5749, 0.6327])),\n",
       "             ('blocks.1.ln1.bias',\n",
       "              tensor([-0.0308,  0.0858, -0.1255,  ..., -0.0442, -0.0668, -0.0463])),\n",
       "             ('blocks.1.attn.qkv.weight',\n",
       "              tensor([[ 0.0357,  0.0125,  0.0138,  ..., -0.0017, -0.0265,  0.0003],\n",
       "                      [ 0.0164,  0.0480, -0.0544,  ...,  0.0352, -0.0088,  0.0018],\n",
       "                      [ 0.0013,  0.0304, -0.0194,  ..., -0.0290, -0.0239,  0.0069],\n",
       "                      ...,\n",
       "                      [-0.0804,  0.0068, -0.0114,  ..., -0.0589,  0.0208, -0.0128],\n",
       "                      [-0.0251,  0.0293, -0.0411,  ...,  0.0005,  0.0870,  0.0128],\n",
       "                      [-0.0042,  0.0202, -0.0289,  ...,  0.0062,  0.0614, -0.0410]])),\n",
       "             ('blocks.1.attn.qkv.bias',\n",
       "              tensor([-0.0139, -0.0239,  0.0271,  ..., -0.0279, -0.0277,  0.0028])),\n",
       "             ('blocks.1.attn.out_proj.weight',\n",
       "              tensor([[-0.0304,  0.0335,  0.0219,  ...,  0.0151, -0.0462,  0.0494],\n",
       "                      [ 0.0432,  0.0201,  0.0060,  ..., -0.0041, -0.0383,  0.0192],\n",
       "                      [-0.0287,  0.0261,  0.0175,  ...,  0.0456,  0.0050,  0.0103],\n",
       "                      ...,\n",
       "                      [-0.0204, -0.0218, -0.0124,  ...,  0.0374,  0.0061,  0.0497],\n",
       "                      [-0.0043,  0.0271, -0.0277,  ..., -0.0322, -0.0272, -0.0249],\n",
       "                      [-0.0209, -0.0168, -0.0595,  ...,  0.0287,  0.0459,  0.0488]])),\n",
       "             ('blocks.1.attn.out_proj.bias',\n",
       "              tensor([-0.0260, -0.0214, -0.0017,  ..., -0.0069,  0.0088,  0.0193])),\n",
       "             ('blocks.1.ln2.weight',\n",
       "              tensor([0.9394, 1.0322, 1.0648,  ..., 0.9680, 1.0638, 0.9800])),\n",
       "             ('blocks.1.ln2.bias',\n",
       "              tensor([-0.0095, -0.0812,  0.0455,  ...,  0.0328,  0.0129,  0.0399])),\n",
       "             ('blocks.1.mlp.0.weight',\n",
       "              tensor([[ 0.0747,  0.0083,  0.0833,  ..., -0.0173, -0.0701, -0.1701],\n",
       "                      [-0.0298,  0.0624, -0.0243,  ..., -0.0070, -0.0176, -0.0513],\n",
       "                      [ 0.0272, -0.0513, -0.1026,  ..., -0.0556, -0.0650, -0.0053],\n",
       "                      ...,\n",
       "                      [-0.1325, -0.0409,  0.0148,  ...,  0.0582,  0.0173,  0.0269],\n",
       "                      [ 0.0272,  0.0512,  0.0273,  ..., -0.0270,  0.0429, -0.0041],\n",
       "                      [-0.0725, -0.0021, -0.0543,  ...,  0.0216, -0.0444,  0.0217]])),\n",
       "             ('blocks.1.mlp.0.bias',\n",
       "              tensor([-0.0325, -0.0411, -0.0262,  ..., -0.0244, -0.0195,  0.0120])),\n",
       "             ('blocks.1.mlp.2.weight',\n",
       "              tensor([[-0.0418,  0.0375,  0.0497,  ...,  0.0690,  0.0215, -0.0753],\n",
       "                      [ 0.0141, -0.0237,  0.1069,  ...,  0.0006, -0.0218,  0.0346],\n",
       "                      [-0.1138, -0.0200,  0.0059,  ...,  0.0448,  0.0145, -0.1296],\n",
       "                      ...,\n",
       "                      [-0.0349, -0.0136,  0.0268,  ..., -0.0428, -0.0107,  0.0049],\n",
       "                      [-0.0192, -0.0121, -0.0632,  ..., -0.0127,  0.0228,  0.0693],\n",
       "                      [ 0.0083, -0.0374,  0.0746,  ...,  0.0413,  0.0929, -0.0433]])),\n",
       "             ('blocks.1.mlp.2.bias',\n",
       "              tensor([ 0.0349, -0.0136,  0.0432,  ..., -0.0087,  0.0454,  0.0296])),\n",
       "             ('blocks.2.ln1.weight',\n",
       "              tensor([0.5040, 0.5046, 0.4866,  ..., 0.4301, 0.3866, 0.4651])),\n",
       "             ('blocks.2.ln1.bias',\n",
       "              tensor([ 0.0057,  0.0741, -0.0685,  ..., -0.0116, -0.1186, -0.0562])),\n",
       "             ('blocks.2.attn.qkv.weight',\n",
       "              tensor([[ 0.0075, -0.0576, -0.0456,  ..., -0.0397, -0.0245, -0.0892],\n",
       "                      [-0.0743, -0.0353, -0.0236,  ...,  0.0251, -0.0588, -0.0138],\n",
       "                      [ 0.0828, -0.1740,  0.0303,  ..., -0.0441,  0.0368, -0.0113],\n",
       "                      ...,\n",
       "                      [ 0.0340, -0.0108,  0.0056,  ..., -0.0261, -0.0688, -0.0272],\n",
       "                      [-0.0545, -0.0149, -0.0646,  ...,  0.0055,  0.0036, -0.0216],\n",
       "                      [ 0.0135,  0.0197,  0.0307,  ...,  0.0329, -0.0634, -0.0312]])),\n",
       "             ('blocks.2.attn.qkv.bias',\n",
       "              tensor([ 0.0040, -0.1629,  0.0360,  ...,  0.0125,  0.0013,  0.0063])),\n",
       "             ('blocks.2.attn.out_proj.weight',\n",
       "              tensor([[ 0.0143,  0.0166, -0.0064,  ...,  0.0203,  0.0971,  0.0407],\n",
       "                      [-0.0189,  0.0491, -0.0215,  ...,  0.0228, -0.0254,  0.0404],\n",
       "                      [ 0.0518,  0.0030, -0.0201,  ...,  0.0055,  0.0058, -0.0510],\n",
       "                      ...,\n",
       "                      [-0.0291,  0.0425, -0.0066,  ...,  0.0655, -0.0530,  0.0487],\n",
       "                      [ 0.0418,  0.0198, -0.0226,  ..., -0.0266,  0.0058,  0.0187],\n",
       "                      [-0.0098, -0.0449, -0.0116,  ...,  0.0286,  0.0089, -0.0672]])),\n",
       "             ('blocks.2.attn.out_proj.bias',\n",
       "              tensor([ 0.0378, -0.0668,  0.0726,  ..., -0.0012,  0.0693,  0.0668])),\n",
       "             ('blocks.2.ln2.weight',\n",
       "              tensor([1.0131, 1.0368, 1.0818,  ..., 0.8909, 1.2349, 1.0823])),\n",
       "             ('blocks.2.ln2.bias',\n",
       "              tensor([ 0.0732, -0.1299,  0.1147,  ...,  0.0038,  0.1283,  0.1166])),\n",
       "             ('blocks.2.mlp.0.weight',\n",
       "              tensor([[-0.0188,  0.0404,  0.0311,  ..., -0.0092, -0.0611, -0.0884],\n",
       "                      [-0.0244, -0.0273, -0.0065,  ...,  0.0056, -0.0206,  0.0166],\n",
       "                      [-0.0086,  0.0271,  0.0012,  ...,  0.0745, -0.0001,  0.0080],\n",
       "                      ...,\n",
       "                      [-0.0344,  0.0414, -0.0334,  ..., -0.0161, -0.0167,  0.0067],\n",
       "                      [ 0.0094,  0.0122,  0.0103,  ...,  0.0133, -0.0149, -0.1080],\n",
       "                      [-0.0022, -0.0286,  0.0359,  ...,  0.0715, -0.0517,  0.0236]])),\n",
       "             ('blocks.2.mlp.0.bias',\n",
       "              tensor([-0.0322, -0.0263, -0.0294,  ..., -0.0103, -0.0089, -0.0189])),\n",
       "             ('blocks.2.mlp.2.weight',\n",
       "              tensor([[-0.0173, -0.0245, -0.0185,  ...,  0.0537,  0.0892, -0.0322],\n",
       "                      [-0.0219,  0.0420,  0.0064,  ..., -0.0138, -0.0815, -0.0074],\n",
       "                      [-0.0160,  0.0658, -0.0061,  ..., -0.0536,  0.0120,  0.0010],\n",
       "                      ...,\n",
       "                      [ 0.0437, -0.0099, -0.0960,  ...,  0.0293,  0.0482,  0.0786],\n",
       "                      [ 0.0041,  0.0010,  0.0347,  ...,  0.0258, -0.0712, -0.0208],\n",
       "                      [ 0.0594, -0.0123, -0.0058,  ...,  0.0243, -0.0485,  0.0145]])),\n",
       "             ('blocks.2.mlp.2.bias',\n",
       "              tensor([ 0.0069, -0.0030, -0.0198,  ...,  0.0022, -0.0029,  0.0023])),\n",
       "             ('blocks.3.ln1.weight',\n",
       "              tensor([0.6823, 0.6476, 0.6290,  ..., 0.6182, 0.5866, 0.6417])),\n",
       "             ('blocks.3.ln1.bias',\n",
       "              tensor([-0.0415,  0.0505, -0.0524,  ...,  0.0199, -0.1697, -0.0495])),\n",
       "             ('blocks.3.attn.qkv.weight',\n",
       "              tensor([[ 0.0261, -0.0668, -0.1033,  ...,  0.0245,  0.0702, -0.0336],\n",
       "                      [ 0.0461, -0.0061,  0.0275,  ...,  0.0364, -0.0292,  0.0153],\n",
       "                      [ 0.0184,  0.1373,  0.1062,  ..., -0.0081,  0.1013, -0.0270],\n",
       "                      ...,\n",
       "                      [-0.0364,  0.0643, -0.0864,  ..., -0.0741, -0.0041, -0.0179],\n",
       "                      [ 0.0414, -0.0054,  0.0522,  ..., -0.0119, -0.0069,  0.0316],\n",
       "                      [-0.0358,  0.0734,  0.0105,  ..., -0.0689,  0.0239,  0.0081]])),\n",
       "             ('blocks.3.attn.qkv.bias',\n",
       "              tensor([-0.0185,  0.0019,  0.0184,  ...,  0.0073, -0.0310, -0.0062])),\n",
       "             ('blocks.3.attn.out_proj.weight',\n",
       "              tensor([[ 0.0449,  0.0341,  0.0194,  ..., -0.0234, -0.0170, -0.0103],\n",
       "                      [ 0.0350,  0.0403, -0.0112,  ..., -0.0867, -0.0565, -0.1090],\n",
       "                      [-0.0372, -0.0711,  0.0401,  ...,  0.1048, -0.0070, -0.0209],\n",
       "                      ...,\n",
       "                      [ 0.0121,  0.0573,  0.0541,  ...,  0.0430,  0.0282,  0.0593],\n",
       "                      [-0.0111, -0.0071,  0.0185,  ..., -0.0487,  0.0375,  0.0581],\n",
       "                      [ 0.0577,  0.0005,  0.0604,  ..., -0.0162, -0.0310, -0.0327]])),\n",
       "             ('blocks.3.attn.out_proj.bias',\n",
       "              tensor([ 0.0290, -0.0472,  0.0129,  ...,  0.0180,  0.0674,  0.0347])),\n",
       "             ('blocks.3.ln2.weight',\n",
       "              tensor([0.9896, 0.9242, 1.0007,  ..., 0.9277, 1.1269, 0.9996])),\n",
       "             ('blocks.3.ln2.bias',\n",
       "              tensor([ 0.0577, -0.0772,  0.0312,  ...,  0.0128,  0.1139,  0.0414])),\n",
       "             ('blocks.3.mlp.0.weight',\n",
       "              tensor([[-0.1648, -0.0401,  0.0400,  ..., -0.0145, -0.0782,  0.0360],\n",
       "                      [-0.0803, -0.0896,  0.0397,  ...,  0.0420, -0.0147,  0.0761],\n",
       "                      [ 0.0629, -0.0051,  0.0092,  ...,  0.0548, -0.0150, -0.0425],\n",
       "                      ...,\n",
       "                      [ 0.0158, -0.0838, -0.0276,  ..., -0.0215,  0.0055,  0.0962],\n",
       "                      [ 0.0062, -0.0052,  0.0981,  ...,  0.0831, -0.0361,  0.0218],\n",
       "                      [ 0.0299,  0.0767, -0.0411,  ..., -0.0721, -0.0498, -0.0237]])),\n",
       "             ('blocks.3.mlp.0.bias',\n",
       "              tensor([ 0.0079, -0.0202, -0.0227,  ..., -0.0175, -0.0402, -0.0134])),\n",
       "             ('blocks.3.mlp.2.weight',\n",
       "              tensor([[-0.1346, -0.0070,  0.0308,  ..., -0.0249,  0.0908,  0.0067],\n",
       "                      [ 0.0392,  0.0160,  0.0581,  ..., -0.0028,  0.0071,  0.0384],\n",
       "                      [ 0.0119,  0.0704,  0.1346,  ...,  0.0232,  0.0098, -0.0292],\n",
       "                      ...,\n",
       "                      [ 0.0588,  0.0228,  0.0222,  ...,  0.0321,  0.1111,  0.0980],\n",
       "                      [-0.0828, -0.0424,  0.0194,  ...,  0.0553, -0.0087,  0.0336],\n",
       "                      [-0.0179,  0.0921,  0.0314,  ...,  0.0750, -0.0207,  0.0388]])),\n",
       "             ('blocks.3.mlp.2.bias',\n",
       "              tensor([ 0.0062, -0.0059, -0.0047,  ...,  0.0297,  0.0300,  0.0148])),\n",
       "             ('blocks.4.ln1.weight',\n",
       "              tensor([0.7852, 0.6990, 0.6696,  ..., 0.6996, 0.6632, 0.7414])),\n",
       "             ('blocks.4.ln1.bias',\n",
       "              tensor([ 0.0278,  0.0018, -0.0267,  ..., -0.0197, -0.0825,  0.0044])),\n",
       "             ('blocks.4.attn.qkv.weight',\n",
       "              tensor([[-0.0542, -0.0633,  0.0137,  ...,  0.0027, -0.0120,  0.0318],\n",
       "                      [-0.0688, -0.0890,  0.0476,  ...,  0.0061,  0.0363,  0.0412],\n",
       "                      [ 0.0189,  0.0165, -0.0449,  ...,  0.0397,  0.1060, -0.1382],\n",
       "                      ...,\n",
       "                      [ 0.0627,  0.0357,  0.0315,  ..., -0.0190, -0.0319,  0.0720],\n",
       "                      [-0.0404, -0.0167,  0.0351,  ..., -0.0656, -0.0687,  0.1086],\n",
       "                      [ 0.0139,  0.0266, -0.0004,  ...,  0.0690,  0.0242,  0.0801]])),\n",
       "             ('blocks.4.attn.qkv.bias',\n",
       "              tensor([-0.0330, -0.1033, -0.0445,  ..., -0.0312, -0.0264,  0.0052])),\n",
       "             ('blocks.4.attn.out_proj.weight',\n",
       "              tensor([[-0.0488, -0.0290,  0.0114,  ...,  0.0470, -0.0027, -0.0021],\n",
       "                      [-0.0894,  0.1074, -0.0371,  ..., -0.0688, -0.0321,  0.0292],\n",
       "                      [-0.0234,  0.0195, -0.0432,  ..., -0.0233,  0.0127, -0.0644],\n",
       "                      ...,\n",
       "                      [-0.0017, -0.0680, -0.0167,  ...,  0.0285, -0.0080,  0.0577],\n",
       "                      [ 0.0440,  0.0848,  0.0307,  ..., -0.1161,  0.0806, -0.0850],\n",
       "                      [ 0.0272, -0.0681, -0.0400,  ..., -0.0922, -0.0235,  0.0248]])),\n",
       "             ('blocks.4.attn.out_proj.bias',\n",
       "              tensor([-0.0061, -0.0107,  0.0196,  ...,  0.0478,  0.0660,  0.0253])),\n",
       "             ('blocks.4.ln2.weight',\n",
       "              tensor([0.9851, 0.9321, 0.9871,  ..., 0.8807, 1.0756, 0.9704])),\n",
       "             ('blocks.4.ln2.bias',\n",
       "              tensor([ 0.0083, -0.0013,  0.0270,  ...,  0.0635,  0.0895,  0.0254])),\n",
       "             ('blocks.4.mlp.0.weight',\n",
       "              tensor([[ 0.0201,  0.0018, -0.0440,  ...,  0.0758, -0.0303,  0.0617],\n",
       "                      [-0.0179, -0.0065, -0.0097,  ...,  0.0513, -0.0199,  0.0048],\n",
       "                      [-0.0362, -0.0962,  0.0369,  ..., -0.0278,  0.0321,  0.0335],\n",
       "                      ...,\n",
       "                      [-0.0389,  0.0528, -0.0831,  ...,  0.0861,  0.0133, -0.0220],\n",
       "                      [-0.0060,  0.0168, -0.0436,  ..., -0.0419, -0.0567, -0.0235],\n",
       "                      [-0.0112,  0.0192,  0.1063,  ...,  0.0189,  0.0577,  0.0217]])),\n",
       "             ('blocks.4.mlp.0.bias',\n",
       "              tensor([-0.0321, -0.0206, -0.0359,  ..., -0.0241, -0.0222, -0.0323])),\n",
       "             ('blocks.4.mlp.2.weight',\n",
       "              tensor([[ 0.0305, -0.0217, -0.0298,  ..., -0.0703,  0.0648,  0.0394],\n",
       "                      [-0.0412,  0.1038, -0.0310,  ...,  0.0437, -0.0043, -0.0280],\n",
       "                      [-0.0741, -0.0222, -0.0460,  ...,  0.0629,  0.0335, -0.0237],\n",
       "                      ...,\n",
       "                      [ 0.0753,  0.0052,  0.0121,  ...,  0.0212,  0.0926, -0.0206],\n",
       "                      [ 0.0733, -0.0300,  0.0488,  ..., -0.0366,  0.0135, -0.0716],\n",
       "                      [ 0.0215, -0.0267, -0.0500,  ..., -0.0051, -0.0900, -0.0169]])),\n",
       "             ('blocks.4.mlp.2.bias',\n",
       "              tensor([-0.0357, -0.0026,  0.0080,  ...,  0.1002, -0.0265,  0.0120])),\n",
       "             ('blocks.5.ln1.weight',\n",
       "              tensor([0.6628, 0.6785, 0.6282,  ..., 0.6226, 0.6139, 0.6745])),\n",
       "             ('blocks.5.ln1.bias',\n",
       "              tensor([ 0.0284, -0.0223, -0.0108,  ..., -0.0099, -0.0657, -0.0262])),\n",
       "             ('blocks.5.attn.qkv.weight',\n",
       "              tensor([[-0.0104,  0.0086, -0.0436,  ...,  0.0087, -0.0245, -0.1080],\n",
       "                      [ 0.0288,  0.0766,  0.0442,  ..., -0.0253, -0.0101, -0.0772],\n",
       "                      [-0.0197,  0.0437,  0.0172,  ...,  0.0724, -0.0560, -0.0837],\n",
       "                      ...,\n",
       "                      [ 0.0009, -0.0331, -0.0449,  ...,  0.0159,  0.0559,  0.0649],\n",
       "                      [ 0.0580, -0.0085, -0.0114,  ...,  0.0331, -0.0825, -0.0207],\n",
       "                      [-0.0248,  0.0979, -0.0064,  ..., -0.0754,  0.0221, -0.0373]])),\n",
       "             ('blocks.5.attn.qkv.bias',\n",
       "              tensor([ 0.0830,  0.0108, -0.0012,  ..., -0.0003, -0.0139,  0.0335])),\n",
       "             ('blocks.5.attn.out_proj.weight',\n",
       "              tensor([[-0.0037, -0.0277,  0.0343,  ..., -0.0467, -0.0279, -0.0001],\n",
       "                      [ 0.0308, -0.0437, -0.0019,  ...,  0.0269,  0.0854, -0.0289],\n",
       "                      [-0.0293, -0.0035,  0.0389,  ...,  0.0129,  0.0143,  0.0162],\n",
       "                      ...,\n",
       "                      [-0.0027, -0.0251,  0.0646,  ..., -0.0090,  0.0175,  0.0013],\n",
       "                      [-0.0064,  0.0157,  0.0153,  ..., -0.0254,  0.0418, -0.0268],\n",
       "                      [-0.0772,  0.0365, -0.0010,  ...,  0.0224,  0.0297, -0.0280]])),\n",
       "             ('blocks.5.attn.out_proj.bias',\n",
       "              tensor([-0.0530,  0.0100,  0.0286,  ...,  0.1154, -0.0139,  0.0400])),\n",
       "             ('blocks.5.ln2.weight',\n",
       "              tensor([1.0089, 0.9002, 0.9684,  ..., 0.9514, 1.0115, 0.9610])),\n",
       "             ('blocks.5.ln2.bias',\n",
       "              tensor([-0.0385,  0.0086,  0.0371,  ...,  0.1346, -0.0019,  0.0371])),\n",
       "             ('blocks.5.mlp.0.weight',\n",
       "              tensor([[ 6.5160e-02, -4.5419e-02, -2.0708e-02,  ...,  1.0045e-02,\n",
       "                       -7.7372e-02, -1.2561e-02],\n",
       "                      [ 2.8604e-02, -3.1938e-02, -1.6495e-02,  ...,  5.4605e-03,\n",
       "                       -1.1622e-02, -1.8927e-03],\n",
       "                      [-4.9637e-02,  9.7478e-02, -3.7423e-02,  ..., -5.8906e-02,\n",
       "                       -3.0215e-02, -3.7710e-02],\n",
       "                      ...,\n",
       "                      [ 1.1035e-01, -1.1937e-04,  4.9921e-03,  ..., -7.4356e-02,\n",
       "                       -2.5122e-02, -2.6410e-02],\n",
       "                      [-9.3179e-03,  6.0641e-02, -1.5640e-01,  ..., -5.3592e-02,\n",
       "                       -5.6355e-02, -4.6734e-03],\n",
       "                      [ 4.8143e-02, -1.9777e-02,  3.9349e-02,  ..., -3.5567e-02,\n",
       "                        6.0969e-02, -3.7990e-02]])),\n",
       "             ('blocks.5.mlp.0.bias',\n",
       "              tensor([-0.0086,  0.0025, -0.0416,  ..., -0.0319, -0.0146, -0.0036])),\n",
       "             ('blocks.5.mlp.2.weight',\n",
       "              tensor([[-4.4281e-02, -1.6579e-02,  8.9840e-04,  ...,  2.2522e-02,\n",
       "                        3.0284e-02, -2.3180e-02],\n",
       "                      [-5.0989e-02, -2.3146e-02,  3.5524e-02,  ...,  3.8001e-02,\n",
       "                       -4.3821e-03,  2.3693e-03],\n",
       "                      [ 1.2126e-02,  3.5789e-02,  3.8112e-02,  ..., -6.8231e-02,\n",
       "                        4.0528e-03,  3.7927e-02],\n",
       "                      ...,\n",
       "                      [-3.6025e-02,  9.1195e-02, -1.1118e-01,  ...,  1.3331e-01,\n",
       "                       -4.6502e-02, -9.0440e-03],\n",
       "                      [-7.9018e-02,  4.2665e-02, -4.2465e-02,  ..., -1.1235e-02,\n",
       "                       -6.4629e-02, -3.9657e-02],\n",
       "                      [ 1.3661e-01, -1.0104e-04,  2.9440e-02,  ...,  8.9270e-02,\n",
       "                        8.4203e-03, -7.4924e-02]])),\n",
       "             ('blocks.5.mlp.2.bias',\n",
       "              tensor([ 0.0197,  0.0044,  0.0286,  ...,  0.1042, -0.0243,  0.0614])),\n",
       "             ('blocks.6.ln1.weight',\n",
       "              tensor([0.5812, 0.5752, 0.6045,  ..., 0.6227, 0.5573, 0.6013])),\n",
       "             ('blocks.6.ln1.bias',\n",
       "              tensor([ 0.0409, -0.0322, -0.0030,  ..., -0.0105,  0.0131,  0.0330])),\n",
       "             ('blocks.6.attn.qkv.weight',\n",
       "              tensor([[ 0.0631, -0.0763,  0.0009,  ..., -0.0307, -0.1063,  0.0025],\n",
       "                      [-0.0259,  0.0338,  0.0711,  ...,  0.0696, -0.0235, -0.0239],\n",
       "                      [-0.0027, -0.0522,  0.0519,  ..., -0.0386,  0.0431, -0.0018],\n",
       "                      ...,\n",
       "                      [ 0.0507,  0.0558,  0.0039,  ...,  0.0025,  0.0794, -0.0014],\n",
       "                      [ 0.0002, -0.0109,  0.0273,  ..., -0.0102,  0.0215, -0.0380],\n",
       "                      [ 0.0613, -0.0377, -0.0593,  ..., -0.0132, -0.0440, -0.0587]])),\n",
       "             ('blocks.6.attn.qkv.bias',\n",
       "              tensor([-0.0134,  0.0838,  0.0261,  ...,  0.0050,  0.0009, -0.0013])),\n",
       "             ('blocks.6.attn.out_proj.weight',\n",
       "              tensor([[-0.0012,  0.0913,  0.0464,  ..., -0.0896,  0.0060, -0.0026],\n",
       "                      [-0.0038, -0.0027, -0.0126,  ..., -0.0639, -0.0514,  0.0277],\n",
       "                      [-0.0023,  0.0273, -0.0620,  ..., -0.0026, -0.0550,  0.0229],\n",
       "                      ...,\n",
       "                      [-0.0345,  0.0114,  0.0400,  ..., -0.0364, -0.1104, -0.0270],\n",
       "                      [ 0.0380, -0.0175, -0.0448,  ...,  0.0368, -0.0031, -0.0283],\n",
       "                      [-0.0979, -0.0433, -0.0314,  ...,  0.0716,  0.0119, -0.0635]])),\n",
       "             ('blocks.6.attn.out_proj.bias',\n",
       "              tensor([ 5.3555e-05,  1.0132e-02,  3.7418e-02,  ...,  9.7107e-02,\n",
       "                      -3.1084e-02,  4.3430e-02])),\n",
       "             ('blocks.6.ln2.weight',\n",
       "              tensor([0.9604, 0.9126, 0.9442,  ..., 0.9409, 0.9811, 0.9522])),\n",
       "             ('blocks.6.ln2.bias',\n",
       "              tensor([ 0.0190,  0.0187,  0.0408,  ...,  0.1174, -0.0195,  0.0539])),\n",
       "             ('blocks.6.mlp.0.weight',\n",
       "              tensor([[-0.0525, -0.0298, -0.0056,  ...,  0.0167, -0.0047, -0.0199],\n",
       "                      [ 0.0727,  0.0182, -0.0136,  ...,  0.0050, -0.0261, -0.0780],\n",
       "                      [ 0.0135, -0.0457, -0.0059,  ..., -0.0415,  0.0535,  0.0521],\n",
       "                      ...,\n",
       "                      [ 0.0304, -0.0461,  0.0332,  ..., -0.0114,  0.0119, -0.0101],\n",
       "                      [-0.0081,  0.0549,  0.0037,  ...,  0.0338, -0.0492, -0.0518],\n",
       "                      [-0.0301, -0.0302, -0.0298,  ..., -0.0389,  0.0566, -0.0084]])),\n",
       "             ('blocks.6.mlp.0.bias',\n",
       "              tensor([-0.0070, -0.0142, -0.0253,  ..., -0.0434, -0.0360, -0.0239])),\n",
       "             ('blocks.6.mlp.2.weight',\n",
       "              tensor([[ 0.0051,  0.0287,  0.0067,  ..., -0.0162, -0.0071, -0.0660],\n",
       "                      [ 0.0490, -0.1341,  0.0167,  ..., -0.0409, -0.0211,  0.0465],\n",
       "                      [-0.0304, -0.0414,  0.0768,  ..., -0.0121, -0.0938, -0.0510],\n",
       "                      ...,\n",
       "                      [-0.0076, -0.0255, -0.1029,  ...,  0.0048,  0.0123, -0.0210],\n",
       "                      [ 0.0030,  0.0297, -0.0613,  ..., -0.0287,  0.0847,  0.0703],\n",
       "                      [-0.1023, -0.0230,  0.0005,  ..., -0.0547,  0.0274, -0.0235]])),\n",
       "             ('blocks.6.mlp.2.bias',\n",
       "              tensor([ 0.0216,  0.0088,  0.0292,  ...,  0.0112, -0.0270, -0.0173])),\n",
       "             ('blocks.7.ln1.weight',\n",
       "              tensor([0.6177, 0.5745, 0.5573,  ..., 0.6010, 0.6082, 0.5950])),\n",
       "             ('blocks.7.ln1.bias',\n",
       "              tensor([ 0.0081, -0.0282, -0.0138,  ..., -0.0883,  0.0149,  0.0050])),\n",
       "             ('blocks.7.attn.qkv.weight',\n",
       "              tensor([[-0.0419, -0.0084,  0.0424,  ...,  0.0375,  0.0213, -0.0119],\n",
       "                      [-0.0454,  0.1218,  0.0799,  ...,  0.0123,  0.0153,  0.0616],\n",
       "                      [ 0.0104, -0.0757, -0.0604,  ..., -0.0270, -0.0337, -0.0032],\n",
       "                      ...,\n",
       "                      [ 0.0121,  0.0292,  0.0600,  ...,  0.0334, -0.0039, -0.0307],\n",
       "                      [-0.0394, -0.0533, -0.0008,  ...,  0.0071, -0.0162, -0.0732],\n",
       "                      [-0.0270,  0.0438, -0.0224,  ..., -0.0497,  0.0123,  0.0347]])),\n",
       "             ('blocks.7.attn.qkv.bias',\n",
       "              tensor([ 0.0103,  0.0290,  0.0019,  ..., -0.0102, -0.0377,  0.0051])),\n",
       "             ('blocks.7.attn.out_proj.weight',\n",
       "              tensor([[ 0.0713,  0.0370,  0.0133,  ..., -0.0715, -0.0130, -0.0098],\n",
       "                      [ 0.0739,  0.0575,  0.0307,  ..., -0.0648, -0.0262,  0.0821],\n",
       "                      [-0.0635,  0.1104, -0.0184,  ...,  0.0317, -0.0045, -0.0068],\n",
       "                      ...,\n",
       "                      [-0.0199, -0.0148, -0.0552,  ...,  0.0122,  0.0368, -0.0767],\n",
       "                      [ 0.0103,  0.0752, -0.0190,  ..., -0.0483, -0.0261,  0.0288],\n",
       "                      [-0.0073, -0.0348, -0.0172,  ..., -0.0959, -0.0418,  0.0031]])),\n",
       "             ('blocks.7.attn.out_proj.bias',\n",
       "              tensor([ 0.0108,  0.0194,  0.0210,  ...,  0.0277, -0.0267, -0.0191])),\n",
       "             ('blocks.7.ln2.weight',\n",
       "              tensor([0.9615, 0.9483, 0.9635,  ..., 0.9513, 1.0057, 0.9518])),\n",
       "             ('blocks.7.ln2.bias',\n",
       "              tensor([ 0.0447,  0.0277,  0.0282,  ...,  0.0497, -0.0357, -0.0151])),\n",
       "             ('blocks.7.mlp.0.weight',\n",
       "              tensor([[-0.0104, -0.0128,  0.0251,  ...,  0.0206, -0.0317,  0.0269],\n",
       "                      [-0.0221, -0.0389, -0.0257,  ...,  0.0026,  0.0831,  0.0252],\n",
       "                      [ 0.0134, -0.0494, -0.0178,  ..., -0.0426,  0.0332, -0.0134],\n",
       "                      ...,\n",
       "                      [-0.0847, -0.0970,  0.0631,  ..., -0.0021, -0.0098,  0.0423],\n",
       "                      [-0.0315,  0.0422,  0.0709,  ..., -0.0286,  0.0614,  0.0696],\n",
       "                      [ 0.0371, -0.0132, -0.0431,  ..., -0.0133,  0.0295,  0.0337]])),\n",
       "             ('blocks.7.mlp.0.bias',\n",
       "              tensor([ 0.0121, -0.0145, -0.0357,  ..., -0.0256, -0.0013, -0.0304])),\n",
       "             ('blocks.7.mlp.2.weight',\n",
       "              tensor([[-0.0178, -0.0558, -0.0126,  ...,  0.0053, -0.0607, -0.0261],\n",
       "                      [-0.0253,  0.0578, -0.0691,  ..., -0.0518,  0.1056, -0.0911],\n",
       "                      [-0.0613, -0.0430,  0.0807,  ...,  0.0296,  0.0148,  0.0269],\n",
       "                      ...,\n",
       "                      [ 0.0100, -0.0543, -0.0046,  ..., -0.1290,  0.0446,  0.0145],\n",
       "                      [-0.0016, -0.0115,  0.0134,  ...,  0.0789,  0.0458, -0.0447],\n",
       "                      [ 0.0363,  0.0538, -0.0829,  ..., -0.0320,  0.0444,  0.0547]])),\n",
       "             ('blocks.7.mlp.2.bias',\n",
       "              tensor([-0.0021,  0.0092,  0.0062,  ...,  0.0354, -0.0477, -0.0059])),\n",
       "             ('blocks.8.ln1.weight',\n",
       "              tensor([0.7343, 0.6425, 0.6650,  ..., 0.6836, 0.7287, 0.6812])),\n",
       "             ('blocks.8.ln1.bias',\n",
       "              tensor([ 0.0090, -0.0346,  0.0148,  ..., -0.0082, -0.0265,  0.0537])),\n",
       "             ('blocks.8.attn.qkv.weight',\n",
       "              tensor([[-0.0099,  0.0085, -0.0119,  ..., -0.0792,  0.0995, -0.0220],\n",
       "                      [ 0.0528, -0.1307, -0.0209,  ..., -0.0365,  0.0083, -0.0340],\n",
       "                      [ 0.0120, -0.1193,  0.0331,  ..., -0.0266, -0.0412,  0.0206],\n",
       "                      ...,\n",
       "                      [-0.0609, -0.0542,  0.0041,  ..., -0.0267,  0.0771,  0.0695],\n",
       "                      [-0.0514,  0.0040, -0.0049,  ..., -0.0005,  0.0792,  0.0484],\n",
       "                      [-0.0196, -0.0512,  0.0108,  ...,  0.0490,  0.0266, -0.0461]])),\n",
       "             ('blocks.8.attn.qkv.bias',\n",
       "              tensor([ 0.1424, -0.0138,  0.0504,  ..., -0.0029,  0.0124,  0.0176])),\n",
       "             ('blocks.8.attn.out_proj.weight',\n",
       "              tensor([[ 0.0691, -0.0090, -0.0190,  ...,  0.0183,  0.0360,  0.0349],\n",
       "                      [-0.0459,  0.0077,  0.1267,  ...,  0.0213, -0.0787, -0.0562],\n",
       "                      [-0.0029,  0.0333,  0.0730,  ..., -0.0268, -0.0182,  0.0185],\n",
       "                      ...,\n",
       "                      [-0.0492, -0.0946,  0.0847,  ..., -0.0090,  0.0009, -0.0608],\n",
       "                      [ 0.0132, -0.0105,  0.0243,  ...,  0.0264, -0.0898,  0.0706],\n",
       "                      [ 0.0939,  0.0104,  0.0014,  ..., -0.1164,  0.0506, -0.0657]])),\n",
       "             ('blocks.8.attn.out_proj.bias',\n",
       "              tensor([-0.0135,  0.0321, -0.0146,  ...,  0.0337, -0.0251, -0.0163])),\n",
       "             ('blocks.8.ln2.weight',\n",
       "              tensor([1.0252, 0.9663, 0.9626,  ..., 0.9856, 1.0167, 0.9763])),\n",
       "             ('blocks.8.ln2.bias',\n",
       "              tensor([ 0.0019,  0.0536, -0.0013,  ...,  0.0448, -0.0261, -0.0131])),\n",
       "             ('blocks.8.mlp.0.weight',\n",
       "              tensor([[ 0.0643,  0.0607, -0.0213,  ..., -0.0241,  0.0071, -0.0205],\n",
       "                      [ 0.0152, -0.0406, -0.0881,  ..., -0.0531,  0.0029, -0.0152],\n",
       "                      [-0.0648, -0.0429,  0.0128,  ..., -0.0241, -0.0167,  0.0352],\n",
       "                      ...,\n",
       "                      [-0.0220, -0.0285,  0.1239,  ..., -0.0190,  0.1150,  0.0282],\n",
       "                      [ 0.0712,  0.0716,  0.0485,  ..., -0.0666,  0.0920,  0.0389],\n",
       "                      [-0.0303,  0.0498, -0.0463,  ..., -0.0547,  0.0409,  0.0032]])),\n",
       "             ('blocks.8.mlp.0.bias',\n",
       "              tensor([-0.0071, -0.0237, -0.0208,  ..., -0.0412,  0.0053, -0.0245])),\n",
       "             ('blocks.8.mlp.2.weight',\n",
       "              tensor([[ 0.0398, -0.0424,  0.0364,  ...,  0.0132,  0.0782, -0.0016],\n",
       "                      [-0.0592, -0.0116, -0.0482,  ...,  0.0488, -0.0813, -0.0043],\n",
       "                      [ 0.0322,  0.0335,  0.0377,  ...,  0.0400, -0.0540,  0.0283],\n",
       "                      ...,\n",
       "                      [-0.0123, -0.0300,  0.0170,  ...,  0.0560,  0.0147, -0.0102],\n",
       "                      [-0.0026,  0.0223, -0.0680,  ..., -0.0451,  0.0766, -0.0142],\n",
       "                      [ 0.0102,  0.0400,  0.0033,  ..., -0.0398,  0.0327,  0.0697]])),\n",
       "             ('blocks.8.mlp.2.bias',\n",
       "              tensor([-0.0042,  0.0360, -0.0314,  ..., -0.0457, -0.0306,  0.0038])),\n",
       "             ('blocks.9.ln1.weight',\n",
       "              tensor([0.6401, 0.6253, 0.6088,  ..., 0.6344, 0.6510, 0.6229])),\n",
       "             ('blocks.9.ln1.bias',\n",
       "              tensor([ 0.0332,  0.0297,  0.0316,  ..., -0.0974,  0.0403,  0.0408])),\n",
       "             ('blocks.9.attn.qkv.weight',\n",
       "              tensor([[-0.0445, -0.0090,  0.0159,  ...,  0.0586,  0.0479, -0.0095],\n",
       "                      [-0.0574, -0.0678,  0.0322,  ...,  0.0072,  0.0015, -0.0518],\n",
       "                      [ 0.0656, -0.0050,  0.0718,  ...,  0.0109,  0.0486, -0.0441],\n",
       "                      ...,\n",
       "                      [ 0.0732, -0.0098, -0.0107,  ..., -0.0681,  0.0218,  0.0245],\n",
       "                      [-0.0106, -0.0171, -0.0318,  ...,  0.0318, -0.0084,  0.0667],\n",
       "                      [ 0.0413,  0.0262,  0.0373,  ...,  0.0129,  0.0756,  0.0405]])),\n",
       "             ('blocks.9.attn.qkv.bias',\n",
       "              tensor([ 0.0457,  0.0595,  0.0403,  ..., -0.0174,  0.0011, -0.0172])),\n",
       "             ('blocks.9.attn.out_proj.weight',\n",
       "              tensor([[ 0.0510, -0.0093, -0.0080,  ...,  0.0710,  0.0239, -0.0618],\n",
       "                      [ 0.0027, -0.0131,  0.0076,  ..., -0.0592,  0.0200, -0.0562],\n",
       "                      [-0.0234, -0.0143,  0.0479,  ..., -0.0102, -0.0035, -0.0077],\n",
       "                      ...,\n",
       "                      [-0.0016,  0.0313, -0.0786,  ..., -0.0082, -0.0238, -0.0266],\n",
       "                      [-0.0020,  0.0074, -0.0273,  ..., -0.0326,  0.0682,  0.0670],\n",
       "                      [ 0.0574,  0.0219, -0.0170,  ..., -0.0505, -0.0527, -0.0010]])),\n",
       "             ('blocks.9.attn.out_proj.bias',\n",
       "              tensor([-0.0160,  0.0363, -0.0339,  ..., -0.0127, -0.0273, -0.0058])),\n",
       "             ('blocks.9.ln2.weight',\n",
       "              tensor([0.9952, 0.9755, 0.9873,  ..., 0.9510, 0.9906, 0.9685])),\n",
       "             ('blocks.9.ln2.bias',\n",
       "              tensor([-0.0187,  0.0426, -0.0416,  ..., -0.0132, -0.0108,  0.0156])),\n",
       "             ('blocks.9.mlp.0.weight',\n",
       "              tensor([[-0.0475, -0.0283, -0.0265,  ...,  0.0082,  0.0386,  0.0204],\n",
       "                      [ 0.0004, -0.0255, -0.0054,  ...,  0.0263, -0.0061,  0.0206],\n",
       "                      [-0.0583,  0.0345,  0.0676,  ...,  0.0052,  0.0004, -0.0096],\n",
       "                      ...,\n",
       "                      [-0.0915, -0.0176,  0.0383,  ...,  0.0455, -0.0038,  0.0081],\n",
       "                      [ 0.0116, -0.0308, -0.0043,  ...,  0.0110,  0.0251,  0.0082],\n",
       "                      [ 0.0385,  0.0473,  0.0499,  ..., -0.0899, -0.0289, -0.0292]])),\n",
       "             ('blocks.9.mlp.0.bias',\n",
       "              tensor([-0.0393, -0.0416, -0.0309,  ..., -0.0301, -0.0359, -0.0306])),\n",
       "             ('blocks.9.mlp.2.weight',\n",
       "              tensor([[-0.0833,  0.0284, -0.0228,  ...,  0.0344, -0.0115,  0.0358],\n",
       "                      [-0.0063,  0.0011,  0.0805,  ...,  0.0303, -0.0155,  0.0089],\n",
       "                      [-0.0729,  0.0226, -0.0397,  ..., -0.0212, -0.0102, -0.0923],\n",
       "                      ...,\n",
       "                      [ 0.0004, -0.0165, -0.0262,  ...,  0.1029, -0.0349, -0.0151],\n",
       "                      [ 0.0496,  0.0745,  0.0728,  ...,  0.1005,  0.0730,  0.0320],\n",
       "                      [-0.0224,  0.0815, -0.0074,  ..., -0.0079, -0.0146, -0.0050]])),\n",
       "             ('blocks.9.mlp.2.bias',\n",
       "              tensor([ 0.0087,  0.0189, -0.0025,  ...,  0.0155, -0.0355, -0.0160])),\n",
       "             ('blocks.10.ln1.weight',\n",
       "              tensor([0.7134, 0.6955, 0.6707,  ..., 0.6750, 0.7058, 0.6882])),\n",
       "             ('blocks.10.ln1.bias',\n",
       "              tensor([ 0.0166, -0.0045,  0.0862,  ..., -0.0029,  0.0371, -0.0042])),\n",
       "             ('blocks.10.attn.qkv.weight',\n",
       "              tensor([[ 0.0252, -0.0498, -0.0709,  ...,  0.0345, -0.0086, -0.0257],\n",
       "                      [-0.0064,  0.0511,  0.0239,  ...,  0.0905, -0.0384, -0.0221],\n",
       "                      [ 0.0353, -0.0825,  0.0792,  ...,  0.0164,  0.0357, -0.0041],\n",
       "                      ...,\n",
       "                      [ 0.0338,  0.0027, -0.0206,  ...,  0.1179, -0.0508,  0.0398],\n",
       "                      [ 0.0727,  0.0208, -0.0024,  ...,  0.0098,  0.0178, -0.0468],\n",
       "                      [ 0.0074,  0.0161,  0.0730,  ..., -0.0742, -0.0687,  0.0573]])),\n",
       "             ('blocks.10.attn.qkv.bias',\n",
       "              tensor([-0.0900, -0.0189, -0.0534,  ..., -0.0026,  0.0138,  0.0062])),\n",
       "             ('blocks.10.attn.out_proj.weight',\n",
       "              tensor([[-0.0284, -0.0053, -0.1244,  ..., -0.0785, -0.0360,  0.0592],\n",
       "                      [ 0.0501,  0.0070,  0.0656,  ..., -0.0053,  0.0998, -0.0030],\n",
       "                      [-0.0184, -0.0051, -0.0751,  ..., -0.0044,  0.0033, -0.1013],\n",
       "                      ...,\n",
       "                      [-0.0327,  0.0011, -0.0280,  ..., -0.0265,  0.0254,  0.0345],\n",
       "                      [-0.0881,  0.0372,  0.0081,  ...,  0.1139, -0.0282, -0.0221],\n",
       "                      [ 0.0142,  0.0495, -0.0954,  ...,  0.0137, -0.0351, -0.0237]])),\n",
       "             ('blocks.10.attn.out_proj.bias',\n",
       "              tensor([ 0.0219,  0.0189, -0.0262,  ...,  0.0365, -0.0337, -0.0166])),\n",
       "             ('blocks.10.ln2.weight',\n",
       "              tensor([0.9659, 0.9778, 0.9791,  ..., 0.9629, 0.9517, 0.9602])),\n",
       "             ('blocks.10.ln2.bias',\n",
       "              tensor([ 0.0096,  0.0142, -0.0344,  ...,  0.0477,  0.0160, -0.0026])),\n",
       "             ('blocks.10.mlp.0.weight',\n",
       "              tensor([[ 0.0652, -0.0442, -0.0043,  ..., -0.0543, -0.0157,  0.1227],\n",
       "                      [ 0.0610,  0.0839,  0.0323,  ..., -0.0134, -0.0602,  0.0808],\n",
       "                      [-0.0275,  0.0258, -0.0195,  ..., -0.0008,  0.0022, -0.0045],\n",
       "                      ...,\n",
       "                      [-0.0036, -0.0435,  0.1098,  ..., -0.0347,  0.0548,  0.0415],\n",
       "                      [ 0.0118,  0.0694,  0.0505,  ..., -0.0210,  0.0204,  0.0379],\n",
       "                      [ 0.0242, -0.0501, -0.0607,  ...,  0.0863,  0.0304, -0.0332]])),\n",
       "             ('blocks.10.mlp.0.bias',\n",
       "              tensor([-0.0155, -0.0206, -0.0159,  ..., -0.0176, -0.0365, -0.0082])),\n",
       "             ('blocks.10.mlp.2.weight',\n",
       "              tensor([[-0.0485,  0.0099,  0.0463,  ..., -0.0279,  0.0759,  0.0221],\n",
       "                      [ 0.0599,  0.0383,  0.0518,  ..., -0.0924,  0.1294, -0.0041],\n",
       "                      [-0.0147, -0.0305, -0.0328,  ...,  0.0099, -0.0764, -0.0530],\n",
       "                      ...,\n",
       "                      [-0.0255, -0.0170,  0.0241,  ...,  0.0459,  0.0439, -0.0050],\n",
       "                      [-0.0340, -0.0524, -0.0913,  ..., -0.0054, -0.0108, -0.0192],\n",
       "                      [ 0.0413,  0.0097,  0.0181,  ..., -0.0983,  0.0730,  0.0633]])),\n",
       "             ('blocks.10.mlp.2.bias',\n",
       "              tensor([ 0.0310,  0.0334, -0.0197,  ..., -0.0122,  0.0200,  0.0378])),\n",
       "             ('blocks.11.ln1.weight',\n",
       "              tensor([0.7473, 0.6884, 0.6697,  ..., 0.6726, 0.7004, 0.7045])),\n",
       "             ('blocks.11.ln1.bias',\n",
       "              tensor([ 0.0048,  0.0147,  0.0281,  ..., -0.0019,  0.0077,  0.0443])),\n",
       "             ('blocks.11.attn.qkv.weight',\n",
       "              tensor([[-0.0621,  0.0065,  0.0238,  ..., -0.0146, -0.0004, -0.0306],\n",
       "                      [-0.1076,  0.0867, -0.0113,  ..., -0.0283,  0.0310, -0.1007],\n",
       "                      [ 0.0030, -0.0371,  0.0393,  ...,  0.0175,  0.0727, -0.0044],\n",
       "                      ...,\n",
       "                      [ 0.0052, -0.0222,  0.0339,  ..., -0.0782,  0.0659,  0.0185],\n",
       "                      [-0.0172, -0.0443, -0.0792,  ...,  0.0602,  0.0247, -0.0053],\n",
       "                      [ 0.0465,  0.0079,  0.0119,  ...,  0.0501,  0.0109, -0.0346]])),\n",
       "             ('blocks.11.attn.qkv.bias',\n",
       "              tensor([ 0.0624,  0.1064,  0.0569,  ..., -0.0082,  0.0099,  0.0212])),\n",
       "             ('blocks.11.attn.out_proj.weight',\n",
       "              tensor([[ 0.0504,  0.0417, -0.0208,  ...,  0.0659, -0.0164, -0.0607],\n",
       "                      [ 0.0300, -0.0381, -0.0152,  ...,  0.0253, -0.0435,  0.0769],\n",
       "                      [ 0.0222, -0.0070, -0.0292,  ...,  0.0378, -0.0463,  0.0059],\n",
       "                      ...,\n",
       "                      [ 0.0063,  0.0042, -0.0002,  ..., -0.0586, -0.0133, -0.0268],\n",
       "                      [ 0.0247, -0.0024, -0.1202,  ..., -0.0559,  0.0386, -0.0198],\n",
       "                      [ 0.0509,  0.0769, -0.0217,  ...,  0.0229, -0.0320,  0.0149]])),\n",
       "             ('blocks.11.attn.out_proj.bias',\n",
       "              tensor([ 0.0616,  0.0815, -0.0189,  ..., -0.0191,  0.0361,  0.0420])),\n",
       "             ('blocks.11.ln2.weight',\n",
       "              tensor([1.2038, 1.2427, 1.2246,  ..., 1.2050, 1.2029, 1.1946])),\n",
       "             ('blocks.11.ln2.bias',\n",
       "              tensor([ 0.0246,  0.1169, -0.0730,  ...,  0.0059,  0.0901,  0.0854])),\n",
       "             ('blocks.11.mlp.0.weight',\n",
       "              tensor([[ 0.0838, -0.0057,  0.0118,  ...,  0.0418, -0.0378,  0.0442],\n",
       "                      [-0.0361,  0.0369, -0.0124,  ...,  0.0895, -0.0238,  0.0349],\n",
       "                      [-0.0022, -0.0093,  0.0302,  ...,  0.0214, -0.0490,  0.0495],\n",
       "                      ...,\n",
       "                      [ 0.1426,  0.0006,  0.0556,  ...,  0.0595, -0.0116, -0.0329],\n",
       "                      [ 0.0080,  0.0039, -0.0325,  ..., -0.0664, -0.0479, -0.0285],\n",
       "                      [ 0.0567, -0.0102,  0.0472,  ...,  0.0055, -0.0903,  0.0144]])),\n",
       "             ('blocks.11.mlp.0.bias',\n",
       "              tensor([-0.0523, -0.0110, -0.0459,  ..., -0.0066, -0.0082, -0.0202])),\n",
       "             ('blocks.11.mlp.2.weight',\n",
       "              tensor([[-0.0190, -0.0337,  0.0153,  ..., -0.0013, -0.0757,  0.0747],\n",
       "                      [-0.0719, -0.0074, -0.0009,  ..., -0.0426,  0.0018, -0.0027],\n",
       "                      [-0.0089, -0.0045,  0.0047,  ...,  0.0457, -0.0555,  0.0074],\n",
       "                      ...,\n",
       "                      [-0.0344,  0.0610,  0.0606,  ...,  0.0654, -0.0268, -0.0338],\n",
       "                      [-0.0525,  0.0040, -0.0585,  ..., -0.0112, -0.0348,  0.0240],\n",
       "                      [ 0.0918, -0.0438,  0.0144,  ..., -0.0680, -0.0376, -0.0838]])),\n",
       "             ('blocks.11.mlp.2.bias',\n",
       "              tensor([ 0.0130,  0.0667, -0.0681,  ..., -0.0141,  0.0208,  0.0037])),\n",
       "             ('lm_head.weight',\n",
       "              tensor([[-0.0096, -0.0172, -0.0153,  ..., -0.0187, -0.0004, -0.0191],\n",
       "                      [ 0.0014,  0.0052, -0.0235,  ..., -0.0041,  0.0119, -0.0305],\n",
       "                      [-0.0088, -0.0191, -0.0156,  ..., -0.0187, -0.0007, -0.0187],\n",
       "                      ...,\n",
       "                      [ 0.0390,  0.0026,  0.0332,  ...,  0.0156, -0.0284, -0.0023],\n",
       "                      [-0.0104, -0.0176, -0.0153,  ..., -0.0177, -0.0009, -0.0193],\n",
       "                      [-0.0099, -0.0174, -0.0169,  ..., -0.0175,  0.0003, -0.0195]])),\n",
       "             ('lm_head.bias',\n",
       "              tensor([-1.3892, -0.7242, -1.3842,  ..., -0.7586, -1.3921, -1.3915]))])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b80cde18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_diff(a, b):\n",
    "    return np.max(np.abs(a - b))\n",
    "\n",
    "def mean_diff(a, b):\n",
    "    return np.mean(np.abs(a - b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "56b443e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch: (51682, 1024) dlx: (51682, 1024)\n",
      "Mean abs diff: 0.0\n"
     ]
    }
   ],
   "source": [
    "# get torch tensor and numpy array\n",
    "torch_w = params_np[\"token_emb.weight\"]\n",
    "dlx_w = dlx_params[\"0_1_embedding_1_embed\"].data  # assuming it's a NumPy array\n",
    "\n",
    "# make sure shapes match\n",
    "print(\"torch:\", torch_w.shape, \"dlx:\", dlx_w.shape)\n",
    "\n",
    "# if transposed, you can also test torch_w.T later\n",
    "\n",
    "# compute mean absolute difference\n",
    "mean_diff = np.mean(np.abs(torch_w - dlx_w))\n",
    "print(\"Mean abs diff:\", mean_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3b23212d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dlx_params = dlx_model.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d87c0b7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token_emb.weight -> 0_1_embedding_1_embed\n",
      "(51682, 1024) (51682, 1024)\n",
      "pos_emb -> 0_1_embedding_1_pe\n",
      "(512, 1024) (512, 1024)\n",
      "lm_head.weight -> linear_1_linear_1_project_weight\n",
      "(1024, 51682) (1024, 51682)\n",
      "lm_head.bias -> linear_1_linear_1_project_bias\n",
      "(51682,) (51682,)\n",
      "blocks.0.ln1.weight -> transformer_1_layernorm_1_gamma\n",
      "(1024,) (1024,)\n",
      "blocks.0.ln1.bias -> transformer_1_layernorm_1_beta\n",
      "(1024,) (1024,)\n",
      "blocks.0.ln2.weight -> transformer_1_layernorm_2_gamma\n",
      "(1024,) (1024,)\n",
      "blocks.0.ln2.bias -> transformer_1_layernorm_2_beta\n",
      "(1024,) (1024,)\n",
      "blocks.0.attn.qkv.weight -> transformer_1_linear_1_qkv_weight\n",
      "(1024, 3072) (1024, 3072)\n",
      "blocks.0.attn.qkv.bias -> transformer_1_linear_1_qkv_bias\n",
      "(3072,) (3072,)\n",
      "blocks.0.attn.out_proj.weight -> transformer_1_linear_2_o_weight\n",
      "(1024, 1024) (1024, 1024)\n",
      "blocks.0.attn.out_proj.bias -> transformer_1_linear_2_o_bias\n",
      "(1024,) (1024,)\n",
      "blocks.0.attn.q_lora_A.weight -> transformer_1_linear_3_q_lora_A_weight\n",
      "(1024, 8) (1024, 8)\n",
      "blocks.0.attn.k_lora_A.weight -> transformer_1_linear_4_k_lora_A_weight\n",
      "(1024, 8) (1024, 8)\n",
      "blocks.0.attn.v_lora_A.weight -> transformer_1_linear_5_v_lora_A_weight\n",
      "(1024, 8) (1024, 8)\n",
      "blocks.0.attn.q_lora_B.weight -> transformer_1_linear_6_q_lora_B_weight\n",
      "(8, 1024) (8, 1024)\n",
      "blocks.0.attn.k_lora_B.weight -> transformer_1_linear_7_k_lora_B_weight\n",
      "(8, 1024) (8, 1024)\n",
      "blocks.0.attn.v_lora_B.weight -> transformer_1_linear_8_v_lora_B_weight\n",
      "(8, 1024) (8, 1024)\n",
      "blocks.0.attn.o_lora_A.weight -> transformer_1_linear_9_o_lora_A_weight\n",
      "(1024, 8) (1024, 8)\n",
      "blocks.0.attn.o_lora_B.weight -> transformer_1_linear_10_o_lora_B_weight\n",
      "(8, 1024) (8, 1024)\n",
      "blocks.0.proj_up_lora_A.weight -> transformer_1_linear_11_proj_up_lora_A_weight\n",
      "(1024, 8) (1024, 8)\n",
      "blocks.0.proj_up_lora_B.weight -> transformer_1_linear_12_proj_up_lora_B_weight\n",
      "(8, 4096) (8, 4096)\n",
      "blocks.0.proj_down_lora_A.weight -> transformer_1_linear_13_proj_down_lora_A_weight\n",
      "(4096, 8) (4096, 8)\n",
      "blocks.0.proj_down_lora_B.weight -> transformer_1_linear_14_proj_down_lora_B_weight\n",
      "(8, 1024) (8, 1024)\n",
      "blocks.0.mlp.0.weight -> transformer_1_linear_15_proj_up_weight\n",
      "(1024, 4096) (1024, 4096)\n",
      "blocks.0.mlp.0.bias -> transformer_1_linear_15_proj_up_bias\n",
      "(4096,) (4096,)\n",
      "blocks.0.mlp.2.weight -> transformer_1_linear_16_proj_down_weight\n",
      "(4096, 1024) (4096, 1024)\n",
      "blocks.0.mlp.2.bias -> transformer_1_linear_16_proj_down_bias\n",
      "(1024,) (1024,)\n",
      "blocks.1.ln1.weight -> transformer_2_layernorm_1_gamma\n",
      "(1024,) (1024,)\n",
      "blocks.1.ln1.bias -> transformer_2_layernorm_1_beta\n",
      "(1024,) (1024,)\n",
      "blocks.1.ln2.weight -> transformer_2_layernorm_2_gamma\n",
      "(1024,) (1024,)\n",
      "blocks.1.ln2.bias -> transformer_2_layernorm_2_beta\n",
      "(1024,) (1024,)\n",
      "blocks.1.attn.qkv.weight -> transformer_2_linear_1_qkv_weight\n",
      "(1024, 3072) (1024, 3072)\n",
      "blocks.1.attn.qkv.bias -> transformer_2_linear_1_qkv_bias\n",
      "(3072,) (3072,)\n",
      "blocks.1.attn.out_proj.weight -> transformer_2_linear_2_o_weight\n",
      "(1024, 1024) (1024, 1024)\n",
      "blocks.1.attn.out_proj.bias -> transformer_2_linear_2_o_bias\n",
      "(1024,) (1024,)\n",
      "blocks.1.attn.q_lora_A.weight -> transformer_2_linear_3_q_lora_A_weight\n",
      "(1024, 8) (1024, 8)\n",
      "blocks.1.attn.k_lora_A.weight -> transformer_2_linear_4_k_lora_A_weight\n",
      "(1024, 8) (1024, 8)\n",
      "blocks.1.attn.v_lora_A.weight -> transformer_2_linear_5_v_lora_A_weight\n",
      "(1024, 8) (1024, 8)\n",
      "blocks.1.attn.q_lora_B.weight -> transformer_2_linear_6_q_lora_B_weight\n",
      "(8, 1024) (8, 1024)\n",
      "blocks.1.attn.k_lora_B.weight -> transformer_2_linear_7_k_lora_B_weight\n",
      "(8, 1024) (8, 1024)\n",
      "blocks.1.attn.v_lora_B.weight -> transformer_2_linear_8_v_lora_B_weight\n",
      "(8, 1024) (8, 1024)\n",
      "blocks.1.attn.o_lora_A.weight -> transformer_2_linear_9_o_lora_A_weight\n",
      "(1024, 8) (1024, 8)\n",
      "blocks.1.attn.o_lora_B.weight -> transformer_2_linear_10_o_lora_B_weight\n",
      "(8, 1024) (8, 1024)\n",
      "blocks.1.proj_up_lora_A.weight -> transformer_2_linear_11_proj_up_lora_A_weight\n",
      "(1024, 8) (1024, 8)\n",
      "blocks.1.proj_up_lora_B.weight -> transformer_2_linear_12_proj_up_lora_B_weight\n",
      "(8, 4096) (8, 4096)\n",
      "blocks.1.proj_down_lora_A.weight -> transformer_2_linear_13_proj_down_lora_A_weight\n",
      "(4096, 8) (4096, 8)\n",
      "blocks.1.proj_down_lora_B.weight -> transformer_2_linear_14_proj_down_lora_B_weight\n",
      "(8, 1024) (8, 1024)\n",
      "blocks.1.mlp.0.weight -> transformer_2_linear_15_proj_up_weight\n",
      "(1024, 4096) (1024, 4096)\n",
      "blocks.1.mlp.0.bias -> transformer_2_linear_15_proj_up_bias\n",
      "(4096,) (4096,)\n",
      "blocks.1.mlp.2.weight -> transformer_2_linear_16_proj_down_weight\n",
      "(4096, 1024) (4096, 1024)\n",
      "blocks.1.mlp.2.bias -> transformer_2_linear_16_proj_down_bias\n",
      "(1024,) (1024,)\n",
      "blocks.2.ln1.weight -> transformer_3_layernorm_1_gamma\n",
      "(1024,) (1024,)\n",
      "blocks.2.ln1.bias -> transformer_3_layernorm_1_beta\n",
      "(1024,) (1024,)\n",
      "blocks.2.ln2.weight -> transformer_3_layernorm_2_gamma\n",
      "(1024,) (1024,)\n",
      "blocks.2.ln2.bias -> transformer_3_layernorm_2_beta\n",
      "(1024,) (1024,)\n",
      "blocks.2.attn.qkv.weight -> transformer_3_linear_1_qkv_weight\n",
      "(1024, 3072) (1024, 3072)\n",
      "blocks.2.attn.qkv.bias -> transformer_3_linear_1_qkv_bias\n",
      "(3072,) (3072,)\n",
      "blocks.2.attn.out_proj.weight -> transformer_3_linear_2_o_weight\n",
      "(1024, 1024) (1024, 1024)\n",
      "blocks.2.attn.out_proj.bias -> transformer_3_linear_2_o_bias\n",
      "(1024,) (1024,)\n",
      "blocks.2.attn.q_lora_A.weight -> transformer_3_linear_3_q_lora_A_weight\n",
      "(1024, 8) (1024, 8)\n",
      "blocks.2.attn.k_lora_A.weight -> transformer_3_linear_4_k_lora_A_weight\n",
      "(1024, 8) (1024, 8)\n",
      "blocks.2.attn.v_lora_A.weight -> transformer_3_linear_5_v_lora_A_weight\n",
      "(1024, 8) (1024, 8)\n",
      "blocks.2.attn.q_lora_B.weight -> transformer_3_linear_6_q_lora_B_weight\n",
      "(8, 1024) (8, 1024)\n",
      "blocks.2.attn.k_lora_B.weight -> transformer_3_linear_7_k_lora_B_weight\n",
      "(8, 1024) (8, 1024)\n",
      "blocks.2.attn.v_lora_B.weight -> transformer_3_linear_8_v_lora_B_weight\n",
      "(8, 1024) (8, 1024)\n",
      "blocks.2.attn.o_lora_A.weight -> transformer_3_linear_9_o_lora_A_weight\n",
      "(1024, 8) (1024, 8)\n",
      "blocks.2.attn.o_lora_B.weight -> transformer_3_linear_10_o_lora_B_weight\n",
      "(8, 1024) (8, 1024)\n",
      "blocks.2.proj_up_lora_A.weight -> transformer_3_linear_11_proj_up_lora_A_weight\n",
      "(1024, 8) (1024, 8)\n",
      "blocks.2.proj_up_lora_B.weight -> transformer_3_linear_12_proj_up_lora_B_weight\n",
      "(8, 4096) (8, 4096)\n",
      "blocks.2.proj_down_lora_A.weight -> transformer_3_linear_13_proj_down_lora_A_weight\n",
      "(4096, 8) (4096, 8)\n",
      "blocks.2.proj_down_lora_B.weight -> transformer_3_linear_14_proj_down_lora_B_weight\n",
      "(8, 1024) (8, 1024)\n",
      "blocks.2.mlp.0.weight -> transformer_3_linear_15_proj_up_weight\n",
      "(1024, 4096) (1024, 4096)\n",
      "blocks.2.mlp.0.bias -> transformer_3_linear_15_proj_up_bias\n",
      "(4096,) (4096,)\n",
      "blocks.2.mlp.2.weight -> transformer_3_linear_16_proj_down_weight\n",
      "(4096, 1024) (4096, 1024)\n",
      "blocks.2.mlp.2.bias -> transformer_3_linear_16_proj_down_bias\n",
      "(1024,) (1024,)\n",
      "blocks.3.ln1.weight -> transformer_4_layernorm_1_gamma\n",
      "(1024,) (1024,)\n",
      "blocks.3.ln1.bias -> transformer_4_layernorm_1_beta\n",
      "(1024,) (1024,)\n",
      "blocks.3.ln2.weight -> transformer_4_layernorm_2_gamma\n",
      "(1024,) (1024,)\n",
      "blocks.3.ln2.bias -> transformer_4_layernorm_2_beta\n",
      "(1024,) (1024,)\n",
      "blocks.3.attn.qkv.weight -> transformer_4_linear_1_qkv_weight\n",
      "(1024, 3072) (1024, 3072)\n",
      "blocks.3.attn.qkv.bias -> transformer_4_linear_1_qkv_bias\n",
      "(3072,) (3072,)\n",
      "blocks.3.attn.out_proj.weight -> transformer_4_linear_2_o_weight\n",
      "(1024, 1024) (1024, 1024)\n",
      "blocks.3.attn.out_proj.bias -> transformer_4_linear_2_o_bias\n",
      "(1024,) (1024,)\n",
      "blocks.3.attn.q_lora_A.weight -> transformer_4_linear_3_q_lora_A_weight\n",
      "(1024, 8) (1024, 8)\n",
      "blocks.3.attn.k_lora_A.weight -> transformer_4_linear_4_k_lora_A_weight\n",
      "(1024, 8) (1024, 8)\n",
      "blocks.3.attn.v_lora_A.weight -> transformer_4_linear_5_v_lora_A_weight\n",
      "(1024, 8) (1024, 8)\n",
      "blocks.3.attn.q_lora_B.weight -> transformer_4_linear_6_q_lora_B_weight\n",
      "(8, 1024) (8, 1024)\n",
      "blocks.3.attn.k_lora_B.weight -> transformer_4_linear_7_k_lora_B_weight\n",
      "(8, 1024) (8, 1024)\n",
      "blocks.3.attn.v_lora_B.weight -> transformer_4_linear_8_v_lora_B_weight\n",
      "(8, 1024) (8, 1024)\n",
      "blocks.3.attn.o_lora_A.weight -> transformer_4_linear_9_o_lora_A_weight\n",
      "(1024, 8) (1024, 8)\n",
      "blocks.3.attn.o_lora_B.weight -> transformer_4_linear_10_o_lora_B_weight\n",
      "(8, 1024) (8, 1024)\n",
      "blocks.3.proj_up_lora_A.weight -> transformer_4_linear_11_proj_up_lora_A_weight\n",
      "(1024, 8) (1024, 8)\n",
      "blocks.3.proj_up_lora_B.weight -> transformer_4_linear_12_proj_up_lora_B_weight\n",
      "(8, 4096) (8, 4096)\n",
      "blocks.3.proj_down_lora_A.weight -> transformer_4_linear_13_proj_down_lora_A_weight\n",
      "(4096, 8) (4096, 8)\n",
      "blocks.3.proj_down_lora_B.weight -> transformer_4_linear_14_proj_down_lora_B_weight\n",
      "(8, 1024) (8, 1024)\n",
      "blocks.3.mlp.0.weight -> transformer_4_linear_15_proj_up_weight\n",
      "(1024, 4096) (1024, 4096)\n",
      "blocks.3.mlp.0.bias -> transformer_4_linear_15_proj_up_bias\n",
      "(4096,) (4096,)\n",
      "blocks.3.mlp.2.weight -> transformer_4_linear_16_proj_down_weight\n",
      "(4096, 1024) (4096, 1024)\n",
      "blocks.3.mlp.2.bias -> transformer_4_linear_16_proj_down_bias\n",
      "(1024,) (1024,)\n",
      "blocks.4.ln1.weight -> transformer_5_layernorm_1_gamma\n",
      "(1024,) (1024,)\n",
      "blocks.4.ln1.bias -> transformer_5_layernorm_1_beta\n",
      "(1024,) (1024,)\n",
      "blocks.4.ln2.weight -> transformer_5_layernorm_2_gamma\n",
      "(1024,) (1024,)\n",
      "blocks.4.ln2.bias -> transformer_5_layernorm_2_beta\n",
      "(1024,) (1024,)\n",
      "blocks.4.attn.qkv.weight -> transformer_5_linear_1_qkv_weight\n",
      "(1024, 3072) (1024, 3072)\n",
      "blocks.4.attn.qkv.bias -> transformer_5_linear_1_qkv_bias\n",
      "(3072,) (3072,)\n",
      "blocks.4.attn.out_proj.weight -> transformer_5_linear_2_o_weight\n",
      "(1024, 1024) (1024, 1024)\n",
      "blocks.4.attn.out_proj.bias -> transformer_5_linear_2_o_bias\n",
      "(1024,) (1024,)\n",
      "blocks.4.attn.q_lora_A.weight -> transformer_5_linear_3_q_lora_A_weight\n",
      "(1024, 8) (1024, 8)\n",
      "blocks.4.attn.k_lora_A.weight -> transformer_5_linear_4_k_lora_A_weight\n",
      "(1024, 8) (1024, 8)\n",
      "blocks.4.attn.v_lora_A.weight -> transformer_5_linear_5_v_lora_A_weight\n",
      "(1024, 8) (1024, 8)\n",
      "blocks.4.attn.q_lora_B.weight -> transformer_5_linear_6_q_lora_B_weight\n",
      "(8, 1024) (8, 1024)\n",
      "blocks.4.attn.k_lora_B.weight -> transformer_5_linear_7_k_lora_B_weight\n",
      "(8, 1024) (8, 1024)\n",
      "blocks.4.attn.v_lora_B.weight -> transformer_5_linear_8_v_lora_B_weight\n",
      "(8, 1024) (8, 1024)\n",
      "blocks.4.attn.o_lora_A.weight -> transformer_5_linear_9_o_lora_A_weight\n",
      "(1024, 8) (1024, 8)\n",
      "blocks.4.attn.o_lora_B.weight -> transformer_5_linear_10_o_lora_B_weight\n",
      "(8, 1024) (8, 1024)\n",
      "blocks.4.proj_up_lora_A.weight -> transformer_5_linear_11_proj_up_lora_A_weight\n",
      "(1024, 8) (1024, 8)\n",
      "blocks.4.proj_up_lora_B.weight -> transformer_5_linear_12_proj_up_lora_B_weight\n",
      "(8, 4096) (8, 4096)\n",
      "blocks.4.proj_down_lora_A.weight -> transformer_5_linear_13_proj_down_lora_A_weight\n",
      "(4096, 8) (4096, 8)\n",
      "blocks.4.proj_down_lora_B.weight -> transformer_5_linear_14_proj_down_lora_B_weight\n",
      "(8, 1024) (8, 1024)\n",
      "blocks.4.mlp.0.weight -> transformer_5_linear_15_proj_up_weight\n",
      "(1024, 4096) (1024, 4096)\n",
      "blocks.4.mlp.0.bias -> transformer_5_linear_15_proj_up_bias\n",
      "(4096,) (4096,)\n",
      "blocks.4.mlp.2.weight -> transformer_5_linear_16_proj_down_weight\n",
      "(4096, 1024) (4096, 1024)\n",
      "blocks.4.mlp.2.bias -> transformer_5_linear_16_proj_down_bias\n",
      "(1024,) (1024,)\n",
      "blocks.5.ln1.weight -> transformer_6_layernorm_1_gamma\n",
      "(1024,) (1024,)\n",
      "blocks.5.ln1.bias -> transformer_6_layernorm_1_beta\n",
      "(1024,) (1024,)\n",
      "blocks.5.ln2.weight -> transformer_6_layernorm_2_gamma\n",
      "(1024,) (1024,)\n",
      "blocks.5.ln2.bias -> transformer_6_layernorm_2_beta\n",
      "(1024,) (1024,)\n",
      "blocks.5.attn.qkv.weight -> transformer_6_linear_1_qkv_weight\n",
      "(1024, 3072) (1024, 3072)\n",
      "blocks.5.attn.qkv.bias -> transformer_6_linear_1_qkv_bias\n",
      "(3072,) (3072,)\n",
      "blocks.5.attn.out_proj.weight -> transformer_6_linear_2_o_weight\n",
      "(1024, 1024) (1024, 1024)\n",
      "blocks.5.attn.out_proj.bias -> transformer_6_linear_2_o_bias\n",
      "(1024,) (1024,)\n",
      "blocks.5.attn.q_lora_A.weight -> transformer_6_linear_3_q_lora_A_weight\n",
      "(1024, 8) (1024, 8)\n",
      "blocks.5.attn.k_lora_A.weight -> transformer_6_linear_4_k_lora_A_weight\n",
      "(1024, 8) (1024, 8)\n",
      "blocks.5.attn.v_lora_A.weight -> transformer_6_linear_5_v_lora_A_weight\n",
      "(1024, 8) (1024, 8)\n",
      "blocks.5.attn.q_lora_B.weight -> transformer_6_linear_6_q_lora_B_weight\n",
      "(8, 1024) (8, 1024)\n",
      "blocks.5.attn.k_lora_B.weight -> transformer_6_linear_7_k_lora_B_weight\n",
      "(8, 1024) (8, 1024)\n",
      "blocks.5.attn.v_lora_B.weight -> transformer_6_linear_8_v_lora_B_weight\n",
      "(8, 1024) (8, 1024)\n",
      "blocks.5.attn.o_lora_A.weight -> transformer_6_linear_9_o_lora_A_weight\n",
      "(1024, 8) (1024, 8)\n",
      "blocks.5.attn.o_lora_B.weight -> transformer_6_linear_10_o_lora_B_weight\n",
      "(8, 1024) (8, 1024)\n",
      "blocks.5.proj_up_lora_A.weight -> transformer_6_linear_11_proj_up_lora_A_weight\n",
      "(1024, 8) (1024, 8)\n",
      "blocks.5.proj_up_lora_B.weight -> transformer_6_linear_12_proj_up_lora_B_weight\n",
      "(8, 4096) (8, 4096)\n",
      "blocks.5.proj_down_lora_A.weight -> transformer_6_linear_13_proj_down_lora_A_weight\n",
      "(4096, 8) (4096, 8)\n",
      "blocks.5.proj_down_lora_B.weight -> transformer_6_linear_14_proj_down_lora_B_weight\n",
      "(8, 1024) (8, 1024)\n",
      "blocks.5.mlp.0.weight -> transformer_6_linear_15_proj_up_weight\n",
      "(1024, 4096) (1024, 4096)\n",
      "blocks.5.mlp.0.bias -> transformer_6_linear_15_proj_up_bias\n",
      "(4096,) (4096,)\n",
      "blocks.5.mlp.2.weight -> transformer_6_linear_16_proj_down_weight\n",
      "(4096, 1024) (4096, 1024)\n",
      "blocks.5.mlp.2.bias -> transformer_6_linear_16_proj_down_bias\n",
      "(1024,) (1024,)\n",
      "blocks.6.ln1.weight -> transformer_7_layernorm_1_gamma\n",
      "(1024,) (1024,)\n",
      "blocks.6.ln1.bias -> transformer_7_layernorm_1_beta\n",
      "(1024,) (1024,)\n",
      "blocks.6.ln2.weight -> transformer_7_layernorm_2_gamma\n",
      "(1024,) (1024,)\n",
      "blocks.6.ln2.bias -> transformer_7_layernorm_2_beta\n",
      "(1024,) (1024,)\n",
      "blocks.6.attn.qkv.weight -> transformer_7_linear_1_qkv_weight\n",
      "(1024, 3072) (1024, 3072)\n",
      "blocks.6.attn.qkv.bias -> transformer_7_linear_1_qkv_bias\n",
      "(3072,) (3072,)\n",
      "blocks.6.attn.out_proj.weight -> transformer_7_linear_2_o_weight\n",
      "(1024, 1024) (1024, 1024)\n",
      "blocks.6.attn.out_proj.bias -> transformer_7_linear_2_o_bias\n",
      "(1024,) (1024,)\n",
      "blocks.6.attn.q_lora_A.weight -> transformer_7_linear_3_q_lora_A_weight\n",
      "(1024, 8) (1024, 8)\n",
      "blocks.6.attn.k_lora_A.weight -> transformer_7_linear_4_k_lora_A_weight\n",
      "(1024, 8) (1024, 8)\n",
      "blocks.6.attn.v_lora_A.weight -> transformer_7_linear_5_v_lora_A_weight\n",
      "(1024, 8) (1024, 8)\n",
      "blocks.6.attn.q_lora_B.weight -> transformer_7_linear_6_q_lora_B_weight\n",
      "(8, 1024) (8, 1024)\n",
      "blocks.6.attn.k_lora_B.weight -> transformer_7_linear_7_k_lora_B_weight\n",
      "(8, 1024) (8, 1024)\n",
      "blocks.6.attn.v_lora_B.weight -> transformer_7_linear_8_v_lora_B_weight\n",
      "(8, 1024) (8, 1024)\n",
      "blocks.6.attn.o_lora_A.weight -> transformer_7_linear_9_o_lora_A_weight\n",
      "(1024, 8) (1024, 8)\n",
      "blocks.6.attn.o_lora_B.weight -> transformer_7_linear_10_o_lora_B_weight\n",
      "(8, 1024) (8, 1024)\n",
      "blocks.6.proj_up_lora_A.weight -> transformer_7_linear_11_proj_up_lora_A_weight\n",
      "(1024, 8) (1024, 8)\n",
      "blocks.6.proj_up_lora_B.weight -> transformer_7_linear_12_proj_up_lora_B_weight\n",
      "(8, 4096) (8, 4096)\n",
      "blocks.6.proj_down_lora_A.weight -> transformer_7_linear_13_proj_down_lora_A_weight\n",
      "(4096, 8) (4096, 8)\n",
      "blocks.6.proj_down_lora_B.weight -> transformer_7_linear_14_proj_down_lora_B_weight\n",
      "(8, 1024) (8, 1024)\n",
      "blocks.6.mlp.0.weight -> transformer_7_linear_15_proj_up_weight\n",
      "(1024, 4096) (1024, 4096)\n",
      "blocks.6.mlp.0.bias -> transformer_7_linear_15_proj_up_bias\n",
      "(4096,) (4096,)\n",
      "blocks.6.mlp.2.weight -> transformer_7_linear_16_proj_down_weight\n",
      "(4096, 1024) (4096, 1024)\n",
      "blocks.6.mlp.2.bias -> transformer_7_linear_16_proj_down_bias\n",
      "(1024,) (1024,)\n",
      "blocks.7.ln1.weight -> transformer_8_layernorm_1_gamma\n",
      "(1024,) (1024,)\n",
      "blocks.7.ln1.bias -> transformer_8_layernorm_1_beta\n",
      "(1024,) (1024,)\n",
      "blocks.7.ln2.weight -> transformer_8_layernorm_2_gamma\n",
      "(1024,) (1024,)\n",
      "blocks.7.ln2.bias -> transformer_8_layernorm_2_beta\n",
      "(1024,) (1024,)\n",
      "blocks.7.attn.qkv.weight -> transformer_8_linear_1_qkv_weight\n",
      "(1024, 3072) (1024, 3072)\n",
      "blocks.7.attn.qkv.bias -> transformer_8_linear_1_qkv_bias\n",
      "(3072,) (3072,)\n",
      "blocks.7.attn.out_proj.weight -> transformer_8_linear_2_o_weight\n",
      "(1024, 1024) (1024, 1024)\n",
      "blocks.7.attn.out_proj.bias -> transformer_8_linear_2_o_bias\n",
      "(1024,) (1024,)\n",
      "blocks.7.attn.q_lora_A.weight -> transformer_8_linear_3_q_lora_A_weight\n",
      "(1024, 8) (1024, 8)\n",
      "blocks.7.attn.k_lora_A.weight -> transformer_8_linear_4_k_lora_A_weight\n",
      "(1024, 8) (1024, 8)\n",
      "blocks.7.attn.v_lora_A.weight -> transformer_8_linear_5_v_lora_A_weight\n",
      "(1024, 8) (1024, 8)\n",
      "blocks.7.attn.q_lora_B.weight -> transformer_8_linear_6_q_lora_B_weight\n",
      "(8, 1024) (8, 1024)\n",
      "blocks.7.attn.k_lora_B.weight -> transformer_8_linear_7_k_lora_B_weight\n",
      "(8, 1024) (8, 1024)\n",
      "blocks.7.attn.v_lora_B.weight -> transformer_8_linear_8_v_lora_B_weight\n",
      "(8, 1024) (8, 1024)\n",
      "blocks.7.attn.o_lora_A.weight -> transformer_8_linear_9_o_lora_A_weight\n",
      "(1024, 8) (1024, 8)\n",
      "blocks.7.attn.o_lora_B.weight -> transformer_8_linear_10_o_lora_B_weight\n",
      "(8, 1024) (8, 1024)\n",
      "blocks.7.proj_up_lora_A.weight -> transformer_8_linear_11_proj_up_lora_A_weight\n",
      "(1024, 8) (1024, 8)\n",
      "blocks.7.proj_up_lora_B.weight -> transformer_8_linear_12_proj_up_lora_B_weight\n",
      "(8, 4096) (8, 4096)\n",
      "blocks.7.proj_down_lora_A.weight -> transformer_8_linear_13_proj_down_lora_A_weight\n",
      "(4096, 8) (4096, 8)\n",
      "blocks.7.proj_down_lora_B.weight -> transformer_8_linear_14_proj_down_lora_B_weight\n",
      "(8, 1024) (8, 1024)\n",
      "blocks.7.mlp.0.weight -> transformer_8_linear_15_proj_up_weight\n",
      "(1024, 4096) (1024, 4096)\n",
      "blocks.7.mlp.0.bias -> transformer_8_linear_15_proj_up_bias\n",
      "(4096,) (4096,)\n",
      "blocks.7.mlp.2.weight -> transformer_8_linear_16_proj_down_weight\n",
      "(4096, 1024) (4096, 1024)\n",
      "blocks.7.mlp.2.bias -> transformer_8_linear_16_proj_down_bias\n",
      "(1024,) (1024,)\n",
      "blocks.8.ln1.weight -> transformer_9_layernorm_1_gamma\n",
      "(1024,) (1024,)\n",
      "blocks.8.ln1.bias -> transformer_9_layernorm_1_beta\n",
      "(1024,) (1024,)\n",
      "blocks.8.ln2.weight -> transformer_9_layernorm_2_gamma\n",
      "(1024,) (1024,)\n",
      "blocks.8.ln2.bias -> transformer_9_layernorm_2_beta\n",
      "(1024,) (1024,)\n",
      "blocks.8.attn.qkv.weight -> transformer_9_linear_1_qkv_weight\n",
      "(1024, 3072) (1024, 3072)\n",
      "blocks.8.attn.qkv.bias -> transformer_9_linear_1_qkv_bias\n",
      "(3072,) (3072,)\n",
      "blocks.8.attn.out_proj.weight -> transformer_9_linear_2_o_weight\n",
      "(1024, 1024) (1024, 1024)\n",
      "blocks.8.attn.out_proj.bias -> transformer_9_linear_2_o_bias\n",
      "(1024,) (1024,)\n",
      "blocks.8.attn.q_lora_A.weight -> transformer_9_linear_3_q_lora_A_weight\n",
      "(1024, 8) (1024, 8)\n",
      "blocks.8.attn.k_lora_A.weight -> transformer_9_linear_4_k_lora_A_weight\n",
      "(1024, 8) (1024, 8)\n",
      "blocks.8.attn.v_lora_A.weight -> transformer_9_linear_5_v_lora_A_weight\n",
      "(1024, 8) (1024, 8)\n",
      "blocks.8.attn.q_lora_B.weight -> transformer_9_linear_6_q_lora_B_weight\n",
      "(8, 1024) (8, 1024)\n",
      "blocks.8.attn.k_lora_B.weight -> transformer_9_linear_7_k_lora_B_weight\n",
      "(8, 1024) (8, 1024)\n",
      "blocks.8.attn.v_lora_B.weight -> transformer_9_linear_8_v_lora_B_weight\n",
      "(8, 1024) (8, 1024)\n",
      "blocks.8.attn.o_lora_A.weight -> transformer_9_linear_9_o_lora_A_weight\n",
      "(1024, 8) (1024, 8)\n",
      "blocks.8.attn.o_lora_B.weight -> transformer_9_linear_10_o_lora_B_weight\n",
      "(8, 1024) (8, 1024)\n",
      "blocks.8.proj_up_lora_A.weight -> transformer_9_linear_11_proj_up_lora_A_weight\n",
      "(1024, 8) (1024, 8)\n",
      "blocks.8.proj_up_lora_B.weight -> transformer_9_linear_12_proj_up_lora_B_weight\n",
      "(8, 4096) (8, 4096)\n",
      "blocks.8.proj_down_lora_A.weight -> transformer_9_linear_13_proj_down_lora_A_weight\n",
      "(4096, 8) (4096, 8)\n",
      "blocks.8.proj_down_lora_B.weight -> transformer_9_linear_14_proj_down_lora_B_weight\n",
      "(8, 1024) (8, 1024)\n",
      "blocks.8.mlp.0.weight -> transformer_9_linear_15_proj_up_weight\n",
      "(1024, 4096) (1024, 4096)\n",
      "blocks.8.mlp.0.bias -> transformer_9_linear_15_proj_up_bias\n",
      "(4096,) (4096,)\n",
      "blocks.8.mlp.2.weight -> transformer_9_linear_16_proj_down_weight\n",
      "(4096, 1024) (4096, 1024)\n",
      "blocks.8.mlp.2.bias -> transformer_9_linear_16_proj_down_bias\n",
      "(1024,) (1024,)\n",
      "blocks.9.ln1.weight -> transformer_10_layernorm_1_gamma\n",
      "(1024,) (1024,)\n",
      "blocks.9.ln1.bias -> transformer_10_layernorm_1_beta\n",
      "(1024,) (1024,)\n",
      "blocks.9.ln2.weight -> transformer_10_layernorm_2_gamma\n",
      "(1024,) (1024,)\n",
      "blocks.9.ln2.bias -> transformer_10_layernorm_2_beta\n",
      "(1024,) (1024,)\n",
      "blocks.9.attn.qkv.weight -> transformer_10_linear_1_qkv_weight\n",
      "(1024, 3072) (1024, 3072)\n",
      "blocks.9.attn.qkv.bias -> transformer_10_linear_1_qkv_bias\n",
      "(3072,) (3072,)\n",
      "blocks.9.attn.out_proj.weight -> transformer_10_linear_2_o_weight\n",
      "(1024, 1024) (1024, 1024)\n",
      "blocks.9.attn.out_proj.bias -> transformer_10_linear_2_o_bias\n",
      "(1024,) (1024,)\n",
      "blocks.9.attn.q_lora_A.weight -> transformer_10_linear_3_q_lora_A_weight\n",
      "(1024, 8) (1024, 8)\n",
      "blocks.9.attn.k_lora_A.weight -> transformer_10_linear_4_k_lora_A_weight\n",
      "(1024, 8) (1024, 8)\n",
      "blocks.9.attn.v_lora_A.weight -> transformer_10_linear_5_v_lora_A_weight\n",
      "(1024, 8) (1024, 8)\n",
      "blocks.9.attn.q_lora_B.weight -> transformer_10_linear_6_q_lora_B_weight\n",
      "(8, 1024) (8, 1024)\n",
      "blocks.9.attn.k_lora_B.weight -> transformer_10_linear_7_k_lora_B_weight\n",
      "(8, 1024) (8, 1024)\n",
      "blocks.9.attn.v_lora_B.weight -> transformer_10_linear_8_v_lora_B_weight\n",
      "(8, 1024) (8, 1024)\n",
      "blocks.9.attn.o_lora_A.weight -> transformer_10_linear_9_o_lora_A_weight\n",
      "(1024, 8) (1024, 8)\n",
      "blocks.9.attn.o_lora_B.weight -> transformer_10_linear_10_o_lora_B_weight\n",
      "(8, 1024) (8, 1024)\n",
      "blocks.9.proj_up_lora_A.weight -> transformer_10_linear_11_proj_up_lora_A_weight\n",
      "(1024, 8) (1024, 8)\n",
      "blocks.9.proj_up_lora_B.weight -> transformer_10_linear_12_proj_up_lora_B_weight\n",
      "(8, 4096) (8, 4096)\n",
      "blocks.9.proj_down_lora_A.weight -> transformer_10_linear_13_proj_down_lora_A_weight\n",
      "(4096, 8) (4096, 8)\n",
      "blocks.9.proj_down_lora_B.weight -> transformer_10_linear_14_proj_down_lora_B_weight\n",
      "(8, 1024) (8, 1024)\n",
      "blocks.9.mlp.0.weight -> transformer_10_linear_15_proj_up_weight\n",
      "(1024, 4096) (1024, 4096)\n",
      "blocks.9.mlp.0.bias -> transformer_10_linear_15_proj_up_bias\n",
      "(4096,) (4096,)\n",
      "blocks.9.mlp.2.weight -> transformer_10_linear_16_proj_down_weight\n",
      "(4096, 1024) (4096, 1024)\n",
      "blocks.9.mlp.2.bias -> transformer_10_linear_16_proj_down_bias\n",
      "(1024,) (1024,)\n",
      "blocks.10.ln1.weight -> transformer_11_layernorm_1_gamma\n",
      "(1024,) (1024,)\n",
      "blocks.10.ln1.bias -> transformer_11_layernorm_1_beta\n",
      "(1024,) (1024,)\n",
      "blocks.10.ln2.weight -> transformer_11_layernorm_2_gamma\n",
      "(1024,) (1024,)\n",
      "blocks.10.ln2.bias -> transformer_11_layernorm_2_beta\n",
      "(1024,) (1024,)\n",
      "blocks.10.attn.qkv.weight -> transformer_11_linear_1_qkv_weight\n",
      "(1024, 3072) (1024, 3072)\n",
      "blocks.10.attn.qkv.bias -> transformer_11_linear_1_qkv_bias\n",
      "(3072,) (3072,)\n",
      "blocks.10.attn.out_proj.weight -> transformer_11_linear_2_o_weight\n",
      "(1024, 1024) (1024, 1024)\n",
      "blocks.10.attn.out_proj.bias -> transformer_11_linear_2_o_bias\n",
      "(1024,) (1024,)\n",
      "blocks.10.attn.q_lora_A.weight -> transformer_11_linear_3_q_lora_A_weight\n",
      "(1024, 8) (1024, 8)\n",
      "blocks.10.attn.k_lora_A.weight -> transformer_11_linear_4_k_lora_A_weight\n",
      "(1024, 8) (1024, 8)\n",
      "blocks.10.attn.v_lora_A.weight -> transformer_11_linear_5_v_lora_A_weight\n",
      "(1024, 8) (1024, 8)\n",
      "blocks.10.attn.q_lora_B.weight -> transformer_11_linear_6_q_lora_B_weight\n",
      "(8, 1024) (8, 1024)\n",
      "blocks.10.attn.k_lora_B.weight -> transformer_11_linear_7_k_lora_B_weight\n",
      "(8, 1024) (8, 1024)\n",
      "blocks.10.attn.v_lora_B.weight -> transformer_11_linear_8_v_lora_B_weight\n",
      "(8, 1024) (8, 1024)\n",
      "blocks.10.attn.o_lora_A.weight -> transformer_11_linear_9_o_lora_A_weight\n",
      "(1024, 8) (1024, 8)\n",
      "blocks.10.attn.o_lora_B.weight -> transformer_11_linear_10_o_lora_B_weight\n",
      "(8, 1024) (8, 1024)\n",
      "blocks.10.proj_up_lora_A.weight -> transformer_11_linear_11_proj_up_lora_A_weight\n",
      "(1024, 8) (1024, 8)\n",
      "blocks.10.proj_up_lora_B.weight -> transformer_11_linear_12_proj_up_lora_B_weight\n",
      "(8, 4096) (8, 4096)\n",
      "blocks.10.proj_down_lora_A.weight -> transformer_11_linear_13_proj_down_lora_A_weight\n",
      "(4096, 8) (4096, 8)\n",
      "blocks.10.proj_down_lora_B.weight -> transformer_11_linear_14_proj_down_lora_B_weight\n",
      "(8, 1024) (8, 1024)\n",
      "blocks.10.mlp.0.weight -> transformer_11_linear_15_proj_up_weight\n",
      "(1024, 4096) (1024, 4096)\n",
      "blocks.10.mlp.0.bias -> transformer_11_linear_15_proj_up_bias\n",
      "(4096,) (4096,)\n",
      "blocks.10.mlp.2.weight -> transformer_11_linear_16_proj_down_weight\n",
      "(4096, 1024) (4096, 1024)\n",
      "blocks.10.mlp.2.bias -> transformer_11_linear_16_proj_down_bias\n",
      "(1024,) (1024,)\n",
      "blocks.11.ln1.weight -> transformer_12_layernorm_1_gamma\n",
      "(1024,) (1024,)\n",
      "blocks.11.ln1.bias -> transformer_12_layernorm_1_beta\n",
      "(1024,) (1024,)\n",
      "blocks.11.ln2.weight -> transformer_12_layernorm_2_gamma\n",
      "(1024,) (1024,)\n",
      "blocks.11.ln2.bias -> transformer_12_layernorm_2_beta\n",
      "(1024,) (1024,)\n",
      "blocks.11.attn.qkv.weight -> transformer_12_linear_1_qkv_weight\n",
      "(1024, 3072) (1024, 3072)\n",
      "blocks.11.attn.qkv.bias -> transformer_12_linear_1_qkv_bias\n",
      "(3072,) (3072,)\n",
      "blocks.11.attn.out_proj.weight -> transformer_12_linear_2_o_weight\n",
      "(1024, 1024) (1024, 1024)\n",
      "blocks.11.attn.out_proj.bias -> transformer_12_linear_2_o_bias\n",
      "(1024,) (1024,)\n",
      "blocks.11.attn.q_lora_A.weight -> transformer_12_linear_3_q_lora_A_weight\n",
      "(1024, 8) (1024, 8)\n",
      "blocks.11.attn.k_lora_A.weight -> transformer_12_linear_4_k_lora_A_weight\n",
      "(1024, 8) (1024, 8)\n",
      "blocks.11.attn.v_lora_A.weight -> transformer_12_linear_5_v_lora_A_weight\n",
      "(1024, 8) (1024, 8)\n",
      "blocks.11.attn.q_lora_B.weight -> transformer_12_linear_6_q_lora_B_weight\n",
      "(8, 1024) (8, 1024)\n",
      "blocks.11.attn.k_lora_B.weight -> transformer_12_linear_7_k_lora_B_weight\n",
      "(8, 1024) (8, 1024)\n",
      "blocks.11.attn.v_lora_B.weight -> transformer_12_linear_8_v_lora_B_weight\n",
      "(8, 1024) (8, 1024)\n",
      "blocks.11.attn.o_lora_A.weight -> transformer_12_linear_9_o_lora_A_weight\n",
      "(1024, 8) (1024, 8)\n",
      "blocks.11.attn.o_lora_B.weight -> transformer_12_linear_10_o_lora_B_weight\n",
      "(8, 1024) (8, 1024)\n",
      "blocks.11.proj_up_lora_A.weight -> transformer_12_linear_11_proj_up_lora_A_weight\n",
      "(1024, 8) (1024, 8)\n",
      "blocks.11.proj_up_lora_B.weight -> transformer_12_linear_12_proj_up_lora_B_weight\n",
      "(8, 4096) (8, 4096)\n",
      "blocks.11.proj_down_lora_A.weight -> transformer_12_linear_13_proj_down_lora_A_weight\n",
      "(4096, 8) (4096, 8)\n",
      "blocks.11.proj_down_lora_B.weight -> transformer_12_linear_14_proj_down_lora_B_weight\n",
      "(8, 1024) (8, 1024)\n",
      "blocks.11.mlp.0.weight -> transformer_12_linear_15_proj_up_weight\n",
      "(1024, 4096) (1024, 4096)\n",
      "blocks.11.mlp.0.bias -> transformer_12_linear_15_proj_up_bias\n",
      "(4096,) (4096,)\n",
      "blocks.11.mlp.2.weight -> transformer_12_linear_16_proj_down_weight\n",
      "(4096, 1024) (4096, 1024)\n",
      "blocks.11.mlp.2.bias -> transformer_12_linear_16_proj_down_bias\n",
      "(1024,) (1024,)\n"
     ]
    }
   ],
   "source": [
    "for pt_k, dlx_k in pt_to_my_lib.items():\n",
    "\n",
    "    if \"weight\" in pt_k and \"layernorm\" not in pt_k:\n",
    "        if \"emb\" in pt_k:\n",
    "            pt_param = params_np[pt_k]\n",
    "        else:\n",
    "            pt_param = params_np[pt_k].T\n",
    "    else:\n",
    "        pt_param = params_np[pt_k]\n",
    "\n",
    "\n",
    "    print(f\"{pt_k} -> {dlx_k}\")\n",
    "    print(pt_param.shape, dlx_params[dlx_k].shape)\n",
    "\n",
    "    if pt_param.shape != dlx_params[dlx_k].shape:\n",
    "        raise ValueError(f\"Shape mismatch: {pt_k} -> {dlx_k}\")\n",
    "\n",
    "    dlx_params[dlx_k].data = pt_param\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "14279402",
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = AdamW(dlx_params, precision=(np.float32, np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b5e8ca03",
   "metadata": {},
   "outputs": [],
   "source": [
    "optim.save_state(\"checkpoints/finetuned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842710b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
